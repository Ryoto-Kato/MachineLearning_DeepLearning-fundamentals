{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction 1\n",
    "## PCA and SVD\n",
    "    Linear dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "- Linear dimensionality reduction method\n",
    "- Find optimal orthogonal transformation such that covariance between the new dimensions is 0\n",
    "    - Exploits eigen decomposition\n",
    "    - By the orthogonal transformation, we could ignore the low variance direction to reduce the dimensionality of the features.\n",
    "- Transformations\n",
    "    - Find the basis (eigen vector of covariance matrix) such that independent each other\n",
    "    - Diagonalization by changing the basis (inner-product of the original matrix and basis (new coordinate system)) \n",
    "    $\n",
    "    \\mathbf{X} \\text{: Data original coordinates}\n",
    "    \\\\\n",
    "    \\hat{\\mathbf{X}} \\text{: Centralized dataset}\n",
    "    \\\\\n",
    "    \\mathbf{\\Gamma} \\text{: New coordinate system (Eigen vectors matrix)}\n",
    "    \\\\\n",
    "    \\mathbf{\\Lambda} \\text{: Variance of respective direction (Eigen values matrix)}\n",
    "    \\\\\n",
    "    $$\n",
    "    Eigen Decomposition of covariance matrix of original data\n",
    "    $$\n",
    "    \\Sigma_{\\hat{\\mathbf{X}}} =  \\mathbf{\\Gamma} \\mathbf{\\Lambda} \\mathbf{\\Gamma}^T\n",
    "    $\n",
    "    \n",
    "        New coordinate system\n",
    "    $$\\mathbf{\\Gamma}$$\n",
    "\n",
    "        Variance of each direction\n",
    "        Covariance matrix on the new coordinate\n",
    "    $$\\mathbf{\\Lambda}$$\n",
    "    $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "### Process of principal component analysis\n",
    "Given input\n",
    "Args\n",
    "$\\mathbf{X} \\text{: array, shape[N, D]}$\n",
    "- N : #samples, D : features dimensions\n",
    "\n",
    "1. Calculate the mean over samples\n",
    "$$\\mathbf{x_m} \\text{: array, shape [D, 1]}$$\n",
    "\n",
    "2. Centralized (Standardized) the given data matrix around the mean $\\mathbf{x_m}$\n",
    "$$\\hat{\\mathbf{X}} = X - x_m[\\text{none}, :]$$\n",
    "$$\\text{or}$$\n",
    "$$\\hat{\\mathbf{X}} = X -  1_N@x_m^T$$\n",
    "\n",
    "3. Calculate the covariance matrix of the centralized data matrix \n",
    "$$\\Sigma_{\\hat{\\mathbf{X}}} = \\frac{1}{N}\\hat{\\mathbf{X}}^T\\hat{\\mathbf{X}}$$\n",
    "\n",
    "4. Eigendecomposition\n",
    "   - Derived the eigen values and eigenvectos of $\\Sigma_{\\hat{\\mathbf{X}}}$ \n",
    "$$\n",
    "\\Sigma_{\\hat{\\mathbf{X}}} = \\mathbf{\\Gamma}\\mathbf{\\Lambda}\\mathbf{\\Gamma^T}\n",
    "$$\n",
    "\n",
    "   - $\\mathbf{\\Gamma}$ : Eigenvector matrix, shape[D, D]\n",
    "     - Orthonormal matrix\n",
    "       - All rows are independent with each other.\n",
    "   - $\\mathbf{\\Lambda}$ : Eigenvalue matrix, dig(1, .... , D)\n",
    "     - variance of each direction\n",
    "     - none-covariance between the direction\n",
    "     - Directions (new coordinate systems) are independent with each other\n",
    "\n",
    "5. Plot the original data $\\mathbf{X}$ and the eigenvectors to a single diagram\n",
    "   - To obtain optimal diagonal transformation system onto M-dim space\n",
    "   - We need to prune the eigenvectors' matrix leaving out only corresponding M the largest eigenvalues\n",
    "   - We could obtain the \n",
    "   - $\\mathbf{\\Gamma_{prune}}$: some columns are zero\n",
    "\n",
    "\n",
    "6. Transform all vectors in X in this new subspace by expressing all vectors in X in this new basis (project the vectors in $\\mathbf{X}$ onto the M-dim subspace)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Transformed dataset\n",
    "$$\\mathbf{Y}=\\mathbf{X}\\mathbf{\\Gamma}$$\n",
    "    Covariance matrix of transformed dataset\n",
    "$$\\mathbf{\\Sigma_\\mathbf{Y}}=\\mathbf{\\Lambda}=\\mathbf{\\Gamma}^T{\\mathbf{\\Sigma_{\\hat{\\mathbf{X}}}}}\\mathbf{\\Gamma}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given data X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (17, 2)\n",
      "sample: 17\n",
      "features: 2\n",
      "    [-3 -2]\n",
      "    [-2 -1]\n",
      "    [-1  0]\n",
      "    [0 1]\n",
      "    [1 2]\n",
      "    [2 3]\n",
      "    [-2 -2]\n",
      "    [-1 -1]\n",
      "    [0 0]\n",
      "    [1 1]\n",
      "    [2 2]\n",
      "    [-2 -3]\n",
      "    [-1 -2]\n",
      "    [ 0 -1]\n",
      "    [1 0]\n",
      "    [2 1]\n",
      "    [3 2]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([(-3,-2),(-2,-1),(-1,0),(0,1),\n",
    "              (1,2),(2,3),(-2,-2),(-1,-1),\n",
    "              (0,0),(1,1),(2,2), (-2,-3),\n",
    "              (-1,-2),(0,-1),(1,0), (2,1),(3,2)])\n",
    "\n",
    "N, D = X.shape[0], X.shape[1]\n",
    "\n",
    "print(f\"shape: {X.shape}\")\n",
    "print(f\"sample: {N}\")\n",
    "print(f\"features: {D}\")\n",
    "\n",
    "for idx, x in enumerate(X):\n",
    "    print(f'    {x}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the mean over samples\n",
    "$$\\mathbf{x_m} \\text{: array, shape [D, 1]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "# Axis = over which axis we take mean\n",
    "# Keepdims(False) for the subsequent operation\n",
    "# To use augumentation in the following operation, \n",
    "# we should squeeze the dimension of the derived mean of data vectors over samples\n",
    "x_m = X.mean(0, keepdims=False)\n",
    "print(x_m)\n",
    "print(f\"shape: {x_m.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Centralized (Standardized) the given data matrix around the mean $\\mathbf{x_m}$\n",
    "$$\\hat{\\mathbf{X}} = X - x_m[None, :]$$\n",
    "$$\\text{or}$$\n",
    "$$\\hat{\\mathbf{X}} = X - x_m^T 1_N$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3. -2.]\n",
      " [-2. -1.]\n",
      " [-1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  2.]\n",
      " [ 2.  3.]\n",
      " [-2. -2.]\n",
      " [-1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 2.  2.]\n",
      " [-2. -3.]\n",
      " [-1. -2.]\n",
      " [ 0. -1.]\n",
      " [ 1.  0.]\n",
      " [ 2.  1.]\n",
      " [ 3.  2.]]\n",
      "shape: (17, 2)\n"
     ]
    }
   ],
   "source": [
    "X_hat = X - x_m[None, :]\n",
    "## alternative\n",
    "# rx_m = X.mean(0, keepdims=True)\n",
    "# one_n = np.ones((N, 1))\n",
    "# print(f\"1_N shape: {one_n.shape}\")\n",
    "# X_hat = X - one_n @ rx_m\n",
    "print(X_hat)\n",
    "print(f\"shape: {X_hat.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate the covariance matrix of the centralized data matrix \n",
    "$$\\Sigma_{\\hat{\\mathbf{X}}} = \\frac{1}{N}\\hat{\\mathbf{X^T}}\\hat{\\mathbf{X}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "C_X = X_hat.transpose() @ X_hat\n",
    "C_X = C_X * (1/N)\n",
    "# print(C_X)\n",
    "print(f\"shape: {C_X.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance(X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: array[N, D]\n",
    "        data matrix including each data vectors on original space (Usually, Cartesian Space)\n",
    "    Return:\n",
    "        C_X: array[N, N]\n",
    "        covariance matrix of the centralized data of given data matrix X\n",
    "    \"\"\"\n",
    "\n",
    "    # taking mean over samples\n",
    "    # x_m: array(D)\n",
    "\n",
    "    x_m = X.mean(0, keepdims=False)\n",
    "    \n",
    "    # Centralized X\n",
    "    # X_hat: array[N, D], dtype=Float\n",
    "\n",
    "    X_hat = X - x_m[None, :]\n",
    "\n",
    "    # Calculating the covariance matrix of the centralized data X_hat\n",
    "    # C_X: array[N, N], dtype=Float\n",
    "\n",
    "    C_X = X_hat.transpose() @ X_hat\n",
    "    # Normalization\n",
    "    C_X *= (1/N)\n",
    "\n",
    "    return C_X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Eigendecomposition\n",
    "   - Derived the eigen values and eigenvectos of $\\Sigma_{\\hat{\\mathbf{X}}}$ \n",
    "$$\n",
    "\\Sigma_{\\hat{\\mathbf{X}}} = \\mathbf{\\Gamma}\\mathbf{\\Lambda}\\mathbf{\\Gamma^T}\n",
    "$$\n",
    "\n",
    "   - $\\mathbf{\\Gamma}$ : Eigenvector matrix, shape[D, D]\n",
    "     - Orthonormal matrix\n",
    "       - All rows are independent with each other.\n",
    "   - $\\mathbf{\\Lambda}$ : Eigenvalue matrix, dig(1, .... , D)\n",
    "     - variance of each direction\n",
    "     - none-covariance between the direction\n",
    "     - Directions (new coordinate systems) are independent with each other"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Numpy eigendecomposition function\n",
    "- numpy.linalg.eig function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:(2, 2)\n",
      " [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "Lammda:(2,)\n",
      " [5.29411765 0.35294118]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "eig_values, eig_vectors = np.linalg.eig(C_X)\n",
    "print(f\"Gamma:{eig_vectors.shape}\\n {eig_vectors}\")\n",
    "print(f\"Lammda:{eig_values.shape}\\n {eig_values}\")\n",
    "max_dim = np.argmax(eig_values)\n",
    "print(max_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "diagonal mat\n",
      " [[5.29411765 0.        ]\n",
      " [0.         0.35294118]]\n"
     ]
    }
   ],
   "source": [
    "I = np.eye(len(eig_values))\n",
    "print(I.shape)\n",
    "\n",
    "_dig_eig_values = I * eig_values\n",
    "print(f\"diagonal mat\\n {_dig_eig_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eigen(Cov_x):\n",
    "    \"\"\"\n",
    "    Args (1):\n",
    "        Cov_x : array[D, D]\n",
    "            Covariance matrix of the standardized given data matrix X\n",
    "    \n",
    "    Returns (2):\n",
    "        g : array[D, D]\n",
    "            eigen_vectors matrix\n",
    "        \n",
    "        l : array[D, D]\n",
    "            eigen_values diagonal matrix\n",
    "    \"\"\"\n",
    "\n",
    "    D = Cov_x.shape[0]\n",
    "\n",
    "    l, g = np.linalg.eig(Cov_x)\n",
    "    I = np.eye(D)\n",
    "    l = I * l[None, :]\n",
    "\n",
    "    return g, l\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the original data $\\mathbf{X}$ and the eigenvectors to a single diagram\n",
    "   - To obtain optimal diagonal transformation system onto M-dim space\n",
    "   - We need to prune the eigenvectors' matrix leaving out only corresponding M the largest eigenvalues\n",
    "   - We could obtain the \n",
    "   - $\\mathbf{\\Gamma_{prune}}$: some columns are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.3, 3.3, -3.3, 3.3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot given data\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff85b2f4190>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the mean of the data\n",
    "mean = X.mean(0)\n",
    "plt.plot(mean[0], mean[1], 'o', markersize=10, color='red', alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot summery\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal')\n",
    "\n",
    "mean = X.mean(0)\n",
    "plt.plot(mean[0], mean[1], 'o', markersize=10, color='red', alpha=0.5)\n",
    "\n",
    "Sigma = get_covariance(X)\n",
    "g, l = get_eigen(Sigma)\n",
    "\n",
    "max_dim = np.argmax(l)\n",
    "\n",
    "for i in range(l.shape[0]):\n",
    "    variance = l[i][i]\n",
    "    value = str(np.round(variance, 2))\n",
    "    if i == max_dim:\n",
    "        plt.arrow(mean[0], mean[1], g[0][i], g[1][i], width=0.05, color='red', alpha=0.5)\n",
    "        plt.annotate(f'PC\\n{value}' , [g[0][i], g[1][i]])\n",
    "    else:\n",
    "        plt.arrow(mean[0], mean[1], g[0][i], g[1][i], width=0.05, color='blue', alpha=0.5)\n",
    "        plt.annotate(f'{value}', [g[0][i], g[1][i]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "$$\n",
    "\\mathbf{M} = \\mathbf{U} \\cdot \\mathbf{\\Sigma} \\cdot \\mathbf{V}\n",
    "$$\n",
    "\n",
    "- $\\mathbf{U}$: array[N, D]\n",
    "    - Left Singular Matrix\n",
    "- $\\mathbf{\\Sigma}$: array [D, D]\n",
    "    - Singular Matrix\n",
    "- $\\mathbf{V}$: array[D, D]\n",
    "    - Right Singular Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and SVD\n",
    "- Relationship\n",
    "$$\\lambda_i = \\frac{s_i^2}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 2)\n",
      "sample: 17\n",
      "features: 2\n",
      "    [1 2]\n",
      "    [6 3]\n",
      "    [0 2]\n"
     ]
    }
   ],
   "source": [
    "# Given data\n",
    "M = np.array([[1, 2], [6, 3], [0, 2]])\n",
    "\n",
    "N, D = X.shape[0], X.shape[1]\n",
    "\n",
    "print(f\"shape: {M.shape}\")\n",
    "print(f\"sample: {N}\")\n",
    "print(f\"features: {D}\")\n",
    "\n",
    "for _, m in enumerate(M):\n",
    "    print(f'    {m}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using numpy linalg library\n",
    "- np.linalg.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 17)\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n",
      "[9.48683298 2.44948974]\n",
      "(2, 2)\n",
      "[[ 0.70710678  0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "u, s, v = np.linalg.svd(X)\n",
    "print(u.shape)\n",
    "print(s.shape)\n",
    "print(type(s))\n",
    "print(s)\n",
    "print(v.shape)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal')\n",
    "\n",
    "mean = X.mean(0)\n",
    "print(mean)\n",
    "plt.plot(mean[0], mean[1], 'o', markersize = 10, color='red', alpha=0.5)\n",
    "\n",
    "max_dim = np.argmax(s)\n",
    "print(max_dim)\n",
    "\n",
    "for i in range(len(s)):\n",
    "    variance = s[i]**2 / (N)\n",
    "    value = str(np.round(variance, 2))\n",
    "    if i == max_dim:\n",
    "        plt.arrow(mean[0], mean[1], v[i][0], v[i][1], width=0.05, color='red', alpha=0.5)\n",
    "        plt.annotate(f'PC\\n{value}' , [v[i][0], v[i][1]])\n",
    "    else:\n",
    "        plt.arrow(mean[0], mean[1], v[i][0], v[i][1], width=0.05, color='blue', alpha=0.5)\n",
    "        plt.annotate(f'{value}', [v[i][0], v[i][1]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction 2\n",
    "## Matrix Factorization\n",
    "    Non-linear dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system\n",
    "- Restaurant recommendation system\n",
    "- Primary optimization problem\n",
    "  - [Goal] minimize the reconstruction error\n",
    "    - Matrix $\\mathbf{R}$ completion task\n",
    "  - Predict the ratings a user will give to a restaurant they have not yet rated based on a latent factor model\n",
    "  - We are going to factorize the rating matrix by $\\mathbf{Q}$ and $\\mathbf{P}$ given $\\mathbf{R}$\n",
    "  - Given $\\mathbf{R}$\n",
    "    - $\\mathbf{r_{ui}}$ : ratings to item i by user u\n",
    "- Args\n",
    "  - $\\mathbf{R}$: array[N, D]\n",
    "    - rating matrix, sparse\n",
    "  - S={(u, i)|$r_{ui}$ != None} \n",
    "- Returns (optimum solution)\n",
    "  - $\\mathbf{Q}$: array[N, K]\n",
    "    - $\\mathbf{q}_u$ : Latent factor for user $u$\n",
    "  - $\\mathbf{P}$: array[K, D]\n",
    "    - $\\mathbf{p}_i$ : Latent factor for item $i$\n",
    "\n",
    "- Objective function (minimization of reconstruction error)\n",
    "  - including the regularization term\n",
    "    - If R is too sparse to reconstruct, \n",
    "      - Regularization term becomes __Dominant__\n",
    "        - push latent factors to the undetermined area (minimize the length)\n",
    "    - Else,\n",
    "      - Sum of Squared loss becomes __Dominant__\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\min{P, Q}_{\\sum{(u, i) \\in S}} (R_{ui} - \\mathbf{q}_u\\mathbf{p}_i^T)^2 + \\lambda [\\sum_i{\\left\\lVert \\mathbf{p}_i \\right\\rVert}^2 + \\sum_u{\\left\\lVert \\mathbf{q}_u \\right\\rVert}^2]\n",
    "$$ \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to solve this minimization problem\n",
    "- Problem to solve\n",
    "  - We need to optimize __TWO VARIABLES__ {P and Q} at the same time\n",
    "$$\n",
    "\\mathcal{L} = \\min{P, Q}_{\\sum{(u, i) \\in S}} (R_{ui} - \\mathbf{q}_u\\mathbf{p}_i^T)^2 + \\lambda [\\sum_i{\\left\\lVert \\mathbf{p}_i \\right\\rVert}^2 + \\sum_u{\\left\\lVert \\mathbf{q}_u \\right\\rVert}^2]\n",
    "$$ \n",
    "\n",
    "### Methods\n",
    "1. Alternating optimization\n",
    "   - We assume that one of the free parameters is given\n",
    "   - Optimize the two parameters in turns\n",
    "2. Stochastic gradient descent (SGD)\n",
    "   - Sample $\\mathbf{r}_{ui}$ (mini-batch)\n",
    "   - Optimize the parameters by approximating the loss of all data samples with sampled dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre-process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.load(\"yelp-dataset.npy\")\n",
    "\n",
    "## given data interior\n",
    "## [user_id, restaurant, ratings]\n",
    "#   [[101968   1880      1]\n",
    "#   [101968    284      5]\n",
    "#   [101968   1378      2]\n",
    "#   ...\n",
    "#   [ 72452   2100      4]\n",
    "#   [ 72452   2050      5]\n",
    "#   [ 74861   3979      5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#users: 337867\n",
      "#items (restaurants): 5899\n",
      "data type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# shape of the rating matrix\n",
    "# Given matrix is containing the ratings by each user\n",
    "\n",
    "n_users, n_rests = np.max(ratings[:,0]), np.max(ratings[:, 1])\n",
    "print(f'#users: {n_users + 1}')\n",
    "print(f'#items (restaurants): {n_rests + 1}')\n",
    "print(f'data type: {type(ratings)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to store this matrix as a sparse matrix to avoid out-of-memory issues\n",
    "R = sp.coo_matrix((ratings[:, 2], (ratings[:, 0], ratings[:, 1])), shape = (n_users+1, n_rests+1)).tocsr()\n",
    "# R interior\n",
    "#  (User_id, restaurant), rating\n",
    "#     (0, 2050)\t        5\n",
    "#     (1, 36)\t        1\n",
    "#     (1, 580)\t        5\n",
    "#     (1, 628)\t        5\n",
    "#     (1, 703)\t        1\n",
    "#     (1, 774)\t        5\n",
    "#     (1, 1303)\t        4\n",
    "#     (1, 2345)\t        4\n",
    "#     (1, 2809)\t        5\n",
    "#     (1, 3870)\t        4\n",
    "#     (1, 4193)\t        5\n",
    "#     (1, 5256)\t        5\n",
    "#     (1, 5344)\t        4\n",
    "#     (1, 5703)\t        4\n",
    "#     (1, 5890)\t        5\n",
    "#     (2, 3694)\t        5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution for the cold start problem\n",
    "- Cold start problem\n",
    "  - When a new user is coming into the recommendation system, we can not predict about his/her future rating because there is no history\n",
    "- In preprocessing step\n",
    "  - We recursively remove all users and restaurants with 10 or less ratings\n",
    "  - Then, we randomly select 200 data points for the validation and tests sets, respectively\n",
    "  - After this, we subtract the mean rating for each user to account for this global effects (standardize)\n",
    "\n",
    "\n",
    "__NOTE__:\n",
    "  Zero in R is the rating with 0 not the 'unknown' zeros in the matrix. We store the indices for which we are rating data available in a separate variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_start_preprocessing(matrix, min_entries):\n",
    "    \"\"\"\n",
    "    Recursively removes rows and columns from the input matrix\n",
    "    which have less ratings than min_entries\n",
    "\n",
    "    Args:\n",
    "        matrix: array[n_users, n_items]\n",
    "            rating data matrix R\n",
    "        min_entries: int\n",
    "            minimum entries to be arrowed to exist in the matrix\n",
    "    \n",
    "    Returns:\n",
    "        matrix: sp.spmatrix, shape[N', D']\n",
    "        The pre-processed matrix -> where N' <= N, D' <= D\n",
    "    \"\"\"\n",
    "    print(\"shape before: {}\".format(matrix.shape))\n",
    "    print(\"----------- V -----------\")\n",
    "\n",
    "    shape = (-1, -1)\n",
    "    while matrix.shape != shape:\n",
    "        shape = matrix.shape\n",
    "        # Make stencil buffer (mask) masking more than 0 entries\n",
    "        nnz = matrix > 0\n",
    "        # Make stencil buffer masking the row which has less than minimum entries\n",
    "        # .A1 returns flatten matrix\n",
    "        row_ixs = nnz.sum(1).A1 > min_entries\n",
    "        # Only leave out the rows having more than minimum entries\n",
    "        matrix = matrix[row_ixs]\n",
    "\n",
    "        # Make stencil buffer masking more than 0 entries\n",
    "        nnz = matrix > 0\n",
    "        # Make stencil buffer masking the column which has less than minimum entries\n",
    "        # .A1 returns flatten matrix\n",
    "        col_ixs = nnz.sum(0).A1 > min_entries\n",
    "        # Only leave out the columns having more than minimum entries\n",
    "        matrix = matrix[:, col_ixs]\n",
    "    print(\"shape after: {}\".format(matrix.shape))\n",
    "    nnz = matrix > 0\n",
    "\n",
    "    assert (nnz.sum(0).A1 > min_entries).all()\n",
    "    assert (nnz.sum(1).A1 > min_entries).all()\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before: (337867, 5899)\n",
      "----------- V -----------\n",
      "shape after: (11275, 3531)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11275x3531 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 285343 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cold_start_preprocessing(R, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtraction the mean user rating from the sparse rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before: (337867, 5899)\n",
      "----------- V -----------\n",
      "shape after: (11275, 3531)\n"
     ]
    }
   ],
   "source": [
    "dev_mat = cold_start_preprocessing(R, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11275, 3531\n"
     ]
    }
   ],
   "source": [
    "n_users, n_rests = dev_mat.shape\n",
    "print(f'{n_users}, {n_rests}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nnz: 13\n",
      "sum_ratings: 52\n",
      "mean ratings over a user\n",
      "[4.         4.4        3.72727273 ... 3.5        4.11764706 2.1       ]\n",
      "shape: (11275,)\n",
      "\n",
      "mean ratings over an item\n",
      "[[0.01800443 0.00381375 0.04399113 ... 0.00629712 0.00700665 0.01649667]]\n",
      "shape: (1, 3531)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean ratings over a user\n",
    "row = dev_mat.getrow(0)\n",
    "sum_ratings = row.sum()\n",
    "num_nnz = row.count_nonzero()\n",
    "\n",
    "print(f'num_nnz: {num_nnz}')\n",
    "print(f'sum_ratings: {sum_ratings}')\n",
    "\n",
    "\n",
    "mean_over_u = np.zeros(n_users)\n",
    "\n",
    "for u in range(n_users):\n",
    "    row = dev_mat.getrow(u)\n",
    "    sum_ratings = row.sum()\n",
    "    num_nnz = row.count_nonzero()\n",
    "    mean_user = sum_ratings/num_nnz\n",
    "    mean_over_u[u] = mean_user\n",
    "\n",
    "print('mean ratings over a user')\n",
    "print(mean_over_u)\n",
    "print(f'shape: {mean_over_u.shape}\\n')\n",
    "\n",
    "\n",
    "# mean ratings over an item\\\n",
    "print('mean ratings over an item')\n",
    "mean_over_i = dev_mat.mean(0)\n",
    "print(mean_over_i)\n",
    "print(f'shape: {mean_over_i.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11275,)\n"
     ]
    }
   ],
   "source": [
    "# flatten the mean ratings over a user\n",
    "f_mou = mean_over_u.flatten()\n",
    "print(f_mou.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.98527329]\n",
      " [-4.38130841]\n",
      " [-3.71566129]\n",
      " ...\n",
      " [-3.48810535]\n",
      " [-4.09782265]\n",
      " [-2.07621071]]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "std_mat = dev_mat - f_mou[:, None]\n",
    "# mean ratings over a user should be close to zero\n",
    "eps = 1e-10\n",
    "print(std_mat.mean(1))\n",
    "assert (std_mat.mean(1).A1 < eps).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dev_mat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 1: Centralization\n",
    "    for subtraction the man rating per user from the non-zero elements in the input matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralization(matrix):\n",
    "    \"\"\"\n",
    "    Subtract the mean rating per user from the non-zero elements\n",
    "\n",
    "    Args:\n",
    "    matrix: sp.spmatrix, shape [N, D]\n",
    "            Input sparse matrix\n",
    "\n",
    "    Retuns:\n",
    "    matrix: sp.spmatrix, shape[N, D]\n",
    "            centralized input matrix at 0 (the mean-shifted ones)\n",
    "    user_means: np.array, shape[N, 1]\n",
    "                The mean rating per user that can be used to recover the absolute ratings from the mean-shifted ones.\n",
    "    \"\"\"\n",
    "    n_users, n_items = matrix.shape\n",
    "    print(n_users)\n",
    "    print(n_items)\n",
    "\n",
    "    # Create mask of non_zero entries\n",
    "    nnz_mask = matrix > 0\n",
    "\n",
    "    # Take mean per user\n",
    "    # uer_means: matrix [n_users, 1]\n",
    "    user_means = matrix.sum(1) / nnz_mask.sum(1)\n",
    "    print(type(user_means))\n",
    "    print()\n",
    "    print(user_means.shape)\n",
    "\n",
    "    # cerate a compressed sparse row matrix(csr_matrix, same type with input matrix\n",
    "    #  \n",
    "    subtract_mask = sp.csr_matrix(user_means).multiply(nnz_mask)\n",
    "\n",
    "    cent_mat = matrix - subtract_mask\n",
    "\n",
    "#     assert np.all(np.isclose(matrix.mean(1), 0))\n",
    "    return cent_mat, user_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11275\n",
      "3531\n",
      "<class 'numpy.matrix'>\n",
      "\n",
      "(11275, 1)\n"
     ]
    }
   ],
   "source": [
    "cent_mat, user_means = centralization(dev_mat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into a train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 9166,   157,  7275,  9043,   287,  8294, 10665, 11037,  7116,\n",
      "        4360,  4229,  1400,  7209,  6906,  7559,  1891, 10354,  3651,\n",
      "       10631,  8484,  4994,  9913,  2171, 11218,  7926,  7192,  2373,\n",
      "       10634, 10600,  7616,  5930, 10550,  4418,    69,  3245, 10533,\n",
      "        9181,  8294,  8806,  6509,   476,  5210,  4697,  7542,   758,\n",
      "        9502,  2521,  8703,  6742,  4716,  3109,  5971,  2478,  9391,\n",
      "        2790,  7807,  3670,  2987,  6697, 10015,  7938,  4222,  6971,\n",
      "       10663,  1082,  1111,  1995,  6543,  4019,  6147,  6029,  5941,\n",
      "         156,  3113,  8706,  2269,  4144,  7923,  2234,  8026,  1764,\n",
      "        9859,  1221,  6059,  1426, 10751,  6784,  8615,  9592,  5378,\n",
      "        7287,  5469,  5678,  5029,  8443,   543,  3246,  7784,  4239,\n",
      "       10013], dtype=int32), array([1562, 3342, 2977,  430,  528,  459, 1967, 1428,   91, 3200, 2221,\n",
      "        775, 3168, 1442, 1005, 2672, 3447,  224, 1753, 1154, 1303, 2414,\n",
      "       1311, 3029, 3069, 2545, 2012, 2020, 1039, 1340, 2238, 3452, 2888,\n",
      "       1311, 3100,  545, 1851, 1519, 1910, 3465,  847,  872, 2344, 2934,\n",
      "       2247, 2351, 2229, 3487, 1345, 3300,  465, 3424, 1728, 2044, 2652,\n",
      "       1731,  715, 1493, 2445, 1859, 2550, 1799, 1264, 3128, 2633, 3120,\n",
      "       1203,  749, 3468, 1580, 2976, 2962,   36,  840, 2763, 2085,  771,\n",
      "       2660, 3325, 1112, 1519, 1133, 1645, 2293, 1021,  160, 3482, 1548,\n",
      "       2975,   62, 1813, 2129, 3326, 1238, 2397, 2030, 2398,  528,  387,\n",
      "        425], dtype=int32))\n",
      "(array([ 1951,  4951,  8617,  7334,  9495,  7836,  5481,  4221,  7859,\n",
      "        9997,  3527, 10837,  4384, 10766,  8709, 11239,  8989,  8371,\n",
      "        7793,  8204,  6028,  7829,  8197,  6691,  7899,  6053, 10216,\n",
      "        1187,  5530,  3003,  2809,  6900,  4433, 10395,  8290,  8847,\n",
      "        7797,  5405,  6079,  9723,  3942,  4669,  2697,  9285,  4948,\n",
      "        1372,  8495,  4606, 10832,  5836, 10032,  9290,  7492,  5501,\n",
      "        4871,  9096,  5445,  4513,  5686,  1916,  6722,  1302,  5685,\n",
      "        9622,  1118,  5840,  5014,  3523,  4307,  1369, 10476,  7252,\n",
      "        3417,  9123,  9417,  6692,  2626,  4490,  2570,  6929,  1902,\n",
      "        6739, 10630,  5749,  2382,  2724,  6301, 10533,  4992,  3637,\n",
      "         607,  7797,  9949,  1778,  6603,  9342, 10024,  6365,  7243,\n",
      "        2386], dtype=int32), array([ 540, 2942, 1291, 3027, 1785, 3212,  372, 1753, 3324, 2843, 2666,\n",
      "       2745, 1955, 2350, 1569, 2136, 1036, 3368, 1088,   58, 2838, 1832,\n",
      "        907, 2242, 3029, 1079, 1808, 1397,  104, 1618,  822, 2760, 2385,\n",
      "       1507, 1932, 3520, 2187, 2371, 1913, 3377, 2406, 1245, 2287,  683,\n",
      "        261, 2860,  783, 3311, 1812, 3285,  461,  319, 2457, 1006, 3128,\n",
      "       3029, 2839, 2753, 2130,  709, 1187, 1850, 3157, 2216, 2100, 3407,\n",
      "       2503,  224,  117, 1316, 1587, 2246, 3475, 2175, 1912, 1922, 3407,\n",
      "         62, 2733, 1813, 3127, 1085, 1764, 1750, 2441,  942, 3377,  798,\n",
      "       1750, 2053, 2058, 3281, 1813, 2331, 1817, 3217, 2044, 2277,  357,\n",
      "       3185], dtype=int32))\n",
      "(11275, 3531)\n"
     ]
    }
   ],
   "source": [
    "# create train, valid, and test dataset\n",
    "# create dataloader\n",
    "\n",
    "# Here we are using sparse matrix, then we need to sample train, valid, and test data samples randomly.\n",
    "\n",
    "# Configuration\n",
    "# The number of the valid and test samples\n",
    "\n",
    "n_validation = 100\n",
    "n_test = 100\n",
    "\n",
    "# copy centralized matrix\n",
    "matrix_cp = cent_mat.copy()\n",
    "# obtain the index lists which are containing non-zero variable\n",
    "non_zero_idx = np.argwhere(matrix_cp)\n",
    "\n",
    "# sample indices randomly (user u, item i)\n",
    "ixs = np.random.permutation(non_zero_idx)\n",
    "\n",
    "# obtain u-index list and i-index list as tuple (u, i)\n",
    "val_idx = tuple(ixs[:n_validation].T)\n",
    "test_idx = tuple(ixs[n_validation:n_validation + n_test].T)\n",
    "\n",
    "print(val_idx)\n",
    "print(test_idx)\n",
    "\n",
    "# obtain the array (flatten) \n",
    "val_values = matrix_cp[val_idx].A1\n",
    "test_values = matrix_cp[test_idx].A1\n",
    "\n",
    "# Eliminate valid and test samples and obtain train data.\n",
    "matrix_cp[val_idx] = matrix_cp[test_idx] = 0\n",
    "print(matrix_cp.shape)\n",
    "matrix_cp.eliminate_zeros()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample\n",
      "  (0, 3526)\t1.0\n",
      "  (0, 3120)\t1.0\n",
      "  (0, 2508)\t1.0\n",
      "  (0, 1694)\t1.0\n",
      "  (0, 461)\t1.0\n",
      "  (0, 416)\t-3.0\n",
      "  (0, 368)\t1.0\n",
      "  (0, 22)\t-3.0\n",
      "valid sample\n",
      "(9166, 1562):0.3125\n",
      "test sample\n",
      "(1951, 540):0.02499999999999991\n"
     ]
    }
   ],
   "source": [
    "for t, train in enumerate(matrix_cp):\n",
    "    if t > 0:\n",
    "        break\n",
    "    print('train sample')\n",
    "    print(train)\n",
    "    print('valid sample')\n",
    "    print(f'({val_idx[0][t]}, {val_idx[1][t]}):{val_values[t]}')\n",
    "    print('test sample')\n",
    "    print(f'({test_idx[0][t]}, {test_idx[1][t]}):{test_values[t]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2: split the centralized data into train, valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matrix, n_val, n_test):\n",
    "    \"\"\"\n",
    "    Extract validation and test entries from the input matrix\n",
    "\n",
    "    Args:\n",
    "        matrix: sp.spmatrix, shape [N, D]\n",
    "                The input data matrix\n",
    "        n_val:  int\n",
    "                The number of validation entries to extract\n",
    "        n_test: int\n",
    "                The number fo test entries to extract\n",
    "        \n",
    "    Returns:\n",
    "        matrix_split:   sp.spmatrix, shape [N, D]\n",
    "                        a copy of the input matrix in which the validation and \n",
    "\n",
    "        val_idx:        tuple, shape [2, n_val]\n",
    "                        The indices of the validation entries\n",
    "\n",
    "        test_idx:       tuple, shape [2, n_test]\n",
    "                        The indices of the test entries\n",
    "\n",
    "        val_values:     np.array, shape [n_val]\n",
    "                        The values of the input matrix at the validation indices\n",
    "\n",
    "        test_values:    np.array, shape [n_train]\n",
    "                        The values of the input matrix at the test indices\n",
    "    \"\"\"\n",
    "\n",
    "    # copy the input matrix\n",
    "    matrix_cp = matrix.copy()\n",
    "\n",
    "    # obtain indices pair (User u, Item i)\n",
    "    non_zero_idx = np.argwhere(matrix_cp)\n",
    "\n",
    "    # random permutation of the list of indices pair\n",
    "    ixs = np.random.permutation(non_zero_idx)\n",
    "\n",
    "    # obtain a tuple [n_val, 2] for u and i, respectively\n",
    "    val_idx =tuple(ixs[:n_val].T)\n",
    "    test_idx = tuple(ixs[n_val:n_val + n_test].T)\n",
    "\n",
    "    # obtain the ratings for the validation data\n",
    "    val_values = matrix_cp[val_idx].A1\n",
    "\n",
    "    # obtain the ratings for the test data\n",
    "    test_values = matrix_cp[test_idx].A1\n",
    "\n",
    "    # Set zero to entries which are assigned as valid or test data\n",
    "    matrix_cp[val_idx] = matrix_cp[test_idx] = 0\n",
    "\n",
    "    # Eliminate zero entries\n",
    "    matrix_cp.eliminate_zeros()\n",
    "\n",
    "    return matrix_cp, val_idx, test_idx, val_values, test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before: (337867, 5899)\n",
      "----------- V -----------\n",
      "shape after: (3529, 2072)\n"
     ]
    }
   ],
   "source": [
    "dev_mat = cold_start_preprocessing(R, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = 200\n",
    "n_test = 200\n",
    "# split data\n",
    "R_train, val_idx, test_idx, val_values, test_values = split_data(dev_mat, n_val, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3529\n",
      "2072\n",
      "<class 'numpy.matrix'>\n",
      "\n",
      "(3529, 1)\n"
     ]
    }
   ],
   "source": [
    "# Centralization\n",
    "non_zero_indices = np.argwhere(R_train)\n",
    "R_shifted, user_means = centralization(R_train)\n",
    "\n",
    "# Apply the same shift to the validation and test data\n",
    "val_values_shifted = val_values - np.ravel(user_means[np.array(val_idx).T[:, 0]])\n",
    "test_values_shifted = test_values - np.ravel(user_means[np.array(test_idx).T[:, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(values, ixs, Q, P, reg_lambda):\n",
    "    \"\"\"\n",
    "    Compute the loss of the latent factor model (at indices ixs)\n",
    "\n",
    "    Args:\n",
    "        values (list of R_ui): np.array, shape[n_ixs, ]\n",
    "                The array with the ground-truth values\n",
    "        ixs (list of ui itself):    tuple, shape[2, n_ixs]\n",
    "                The indices at which we want to evaluate the loss(usually the nonzero indices of the unshifted data matrix)\n",
    "        Q:  np.array, shape [N. k]\n",
    "            The matrix Q of a latent factor model\n",
    "        P:  np.array, shape [k, D]\n",
    "            The matrix P of a latent factor model\n",
    "        reg_lambda: float\n",
    "            The regulation strength\n",
    "    \n",
    "    Returns:\n",
    "        loss:   float\n",
    "                The loss of the latent factor model\n",
    "    \"\"\"\n",
    "\n",
    "    # mean of sum of squared error\n",
    "    sse_loss = np.sum((values - Q.dot(P)[ixs])**2)\n",
    "    # regularization term \n",
    "    regularization_loss = reg_lambda * (np.sum(np.linalg.norm(P, axis = 0)**2) + np,sum(np.linalg.norm(Q, axis = 0)**2))\n",
    "\n",
    "    return sse_loss + regularization_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of the Q and P for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529, 2072)\n",
      "<class 'numpy.ndarray'>\n",
      "(3529, 2072)\n",
      "<class 'numpy.ndarray'>\n",
      "(2072, 2072)\n"
     ]
    }
   ],
   "source": [
    "print(R_train.shape)\n",
    "N, D = R_train.shape\n",
    "k = D\n",
    "Q = np.random.rand(N, k)\n",
    "print(type(Q))\n",
    "print(Q.shape)\n",
    "P = np.random.rand(k, D)\n",
    "print(type(P))\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529, 2072)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "  (0, 24)\t3.0\n",
      "  (0, 219)\t4.0\n",
      "  (0, 333)\t2.0\n",
      "  (0, 344)\t3.0\n",
      "  (0, 393)\t5.0\n",
      "  (0, 470)\t4.0\n",
      "  (0, 530)\t5.0\n",
      "  (0, 570)\t3.0\n",
      "  (0, 585)\t3.0\n",
      "  (0, 657)\t4.0\n",
      "  (0, 664)\t4.0\n",
      "  (0, 711)\t5.0\n",
      "  (0, 799)\t4.0\n",
      "  (0, 825)\t3.0\n",
      "  (0, 872)\t4.0\n",
      "  (0, 1069)\t4.0\n",
      "  (0, 1120)\t5.0\n",
      "  (0, 1188)\t5.0\n",
      "  (0, 1323)\t4.0\n",
      "  (0, 1627)\t2.0\n",
      "  (0, 1648)\t4.0\n",
      "  (0, 1768)\t4.0\n",
      "  (0, 1865)\t4.0\n",
      "  (0, 1946)\t2.0\n",
      "  (1, 280)\t5.0\n",
      "  :\t:\n",
      "  (3528, 735)\t2.0\n",
      "  (3528, 795)\t1.0\n",
      "  (3528, 818)\t1.0\n",
      "  (3528, 839)\t4.0\n",
      "  (3528, 899)\t4.0\n",
      "  (3528, 936)\t1.0\n",
      "  (3528, 1001)\t2.0\n",
      "  (3528, 1005)\t1.0\n",
      "  (3528, 1070)\t4.0\n",
      "  (3528, 1130)\t2.0\n",
      "  (3528, 1144)\t2.0\n",
      "  (3528, 1170)\t2.0\n",
      "  (3528, 1175)\t2.0\n",
      "  (3528, 1215)\t2.0\n",
      "  (3528, 1252)\t2.0\n",
      "  (3528, 1332)\t3.0\n",
      "  (3528, 1363)\t1.0\n",
      "  (3528, 1395)\t3.0\n",
      "  (3528, 1682)\t3.0\n",
      "  (3528, 1685)\t4.0\n",
      "  (3528, 1689)\t2.0\n",
      "  (3528, 1798)\t1.0\n",
      "  (3528, 1945)\t5.0\n",
      "  (3528, 1954)\t4.0\n",
      "  (3528, 1998)\t1.0\n",
      "2072\n"
     ]
    }
   ],
   "source": [
    "f_R_train = R_train.astype(float)\n",
    "print(f_R_train.shape)\n",
    "print(type(f_R_train))\n",
    "print(f_R_train)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we use svds function from the scipy.sparse.linalg, we need to set k as following\n",
    "# `k` must be an integer satisfying `0 < k < min(A.shape)`.\n",
    "U, s, V = svds(f_R_train, k=100)\n",
    "S = np.diag(s)\n",
    "Q = U.dot(S)\n",
    "P = V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529, 100)\n",
      "(100,)\n",
      "(100, 2072)\n"
     ]
    }
   ],
   "source": [
    "print(U.shape)\n",
    "print(s.shape)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529, 100)\n",
      "(100, 2072)\n"
     ]
    }
   ],
   "source": [
    "S = np.diag(s)\n",
    "Q = U.dot(S)\n",
    "print(Q.shape)\n",
    "P = V\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that initialize the latent factors Q and P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q_P(matrix, k, init = 'random'):\n",
    "    \"\"\"\n",
    "    Initialize the matrices Q and P for a latent factor model\n",
    "    Initialize them by using SVD or random\n",
    "\n",
    "    Args:\n",
    "        matrix: sp.spmatrix, shape [N, D]\n",
    "                The matrix to be factorized\n",
    "        k:      int\n",
    "                The number of latent dimension\n",
    "        init:   str in ['svd', 'random'], default:'random'\n",
    "                The initialization strategy. 'svd' means that we use SVD to initialize P and Q\n",
    "                'random' means we initialize the entries in P and Q randomly in the interval [0. 1)\n",
    "                -> numpy.random.rand(shape)\n",
    "    Returns:\n",
    "        Q:  np.print(U.shape)\n",
    "            The initialized matrix Q of a latent factor model\n",
    "        P:  np.array, shape[k, D]\n",
    "            The initialized matrix P of a latent factor model\n",
    "    \"\"\" \n",
    "\n",
    "    N, D = matrix.shape\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    if matrix.dtype != float:\n",
    "        matrix = matrix.astype(float)\n",
    "\n",
    "    if init=='random':\n",
    "        Q = np.random.rand((N, k))\n",
    "        P = np.random.rand((k, D))\n",
    "    elif init=='svd':\n",
    "        U, s, V = svds(matrix, k=k)\n",
    "        S = np.diag(s)\n",
    "        Q = U.dot(S)\n",
    "        P = V\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    assert Q.shape == (N, k)\n",
    "    assert P.shape == (k, D)\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, P = initialize_Q_P(R_train, k = 100, init='svd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529, 100)\n",
      "(100, 2072)\n"
     ]
    }
   ],
   "source": [
    "print(f'{Q.shape}')\n",
    "print(f'{P.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- Alternating optimization\n",
    "    - We need to optimize Q and P simultaneously in the primary optimization problem\n",
    "    - But this is really difficult to implement\n",
    "    - We are going to pretend knowing either Q or P at a moment and optimize the other variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]]\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (3, 3)\t1\n",
      "=list=\n",
      "[list([0, 2]) list([1]) list([]) list([3])]\n",
      "---------------------------\n",
      "[[1 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]]\n",
      "  (0, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (3, 3)\t1\n",
      "=list=\n",
      "[list([0, 2]) list([1]) list([]) list([3])]\n"
     ]
    }
   ],
   "source": [
    "row  = np.array([0, 0, 1, 3])\n",
    "col  = np.array([0, 2, 1, 3])\n",
    "data = np.array([1, 1, 1, 1])\n",
    "\n",
    "A = sp.coo_matrix((data, (row, col)), shape=(4, 4)).tocsr()\n",
    "print(A.toarray())\n",
    "print(A)\n",
    "list_rows = A.tolil().rows\n",
    "print('=list=')\n",
    "print(list_rows)\n",
    "\n",
    "print('---------------------------')\n",
    "\n",
    "A = A.tocsc()\n",
    "print(A.toarray())\n",
    "print(A)\n",
    "list_colums = A.tolil().rows\n",
    "print('=list=')\n",
    "print(list_colums)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy sparse \n",
    "    csc vs csr\n",
    "- CSC: Compressed sparse column\n",
    "  - Sorted in the column indices \n",
    "- CSR: Compressed sparse row\n",
    "  - Sorted in the row indices \n",
    "They are used for write-once-read-many-tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_alternating_optimization(R, non_zero_idx, k, val_idx, reg_lambda, max_steps = 100, init = 'random', log_every = 1, patience = 5, eval_every = 1, optimizer = 'sgd', lr = 1e-2):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using alternating optimization. \n",
    "    Training is done via patience.\n",
    "    i.e. we stop training after we observe no improvement \n",
    "    on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values \n",
    "    for Q and P observed during training.\n",
    "    \n",
    "    Args:\n",
    "        R:              sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized (train_matrix)\n",
    "                        It has to be centralized by mean.\n",
    "        \n",
    "        non_zero_idx:   np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the **un-shifted** matrix to be factorized.\n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different \n",
    "                        from the number of non-zero entries in the input matrix(training matrix) since this indices refers\n",
    "                        original data matrix\n",
    "        \n",
    "        k:              int\n",
    "                        The latent factor dimension\n",
    "        \n",
    "        val_idx:        tuple, shape[2, n_val]\n",
    "                        [u1, u2, ,,,,, u_n_val]\n",
    "                        [i1, i2, ,,,,, i_n_val]\n",
    "                        Tuple pf the validation set indices\n",
    "                        n_val refers to the size of the validation set\n",
    "        \n",
    "        val_values:     np.array, shape [n_val, ]\n",
    "                        The values in the validation set\n",
    "\n",
    "        reg_lambda:     float\n",
    "                        The regularization strength\n",
    "        \n",
    "        max_steps:      int, optional, default = 100\n",
    "                        Maximum number of training interactions (steps, 1 steps, one optimization of two matrix factor Q and P),\n",
    "                        Note that we will step early if we observe\n",
    "                        no improvement on the validation with the step to be patient\n",
    "\n",
    "        init:           str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q.\n",
    "        \n",
    "        log_every:      int, optional, default: 1\n",
    "                        Log the training status every X iterations\n",
    "        \n",
    "        patience:       int, optional, default: 5\n",
    "                        Stop training after we observe no improvement of the valid loss for X evaluation\n",
    "                        iterations. After we stop training, we restore the best observed values for Q and P\n",
    "\n",
    "        eval_every:     int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every x steps\n",
    "                        If we observe no improvement of the validation error, we decrease out patience by 1, else we reset it to *patience*\n",
    "        \n",
    "        optimizer:      str, optional, default: sgd\n",
    "                        If 'sgd; stochastic gradient descent shall be used, otherwise, use alternating least squares.\n",
    "        \n",
    "    Returns:\n",
    "\n",
    "        best_Q:             np.array, shape [N, k]\n",
    "                            Best value for Q (based on the validation loss) observed during training\n",
    "        \n",
    "        best_P:             np.array, shape[k, D]\n",
    "                            Best value for P (based on validation loss) observed during training\n",
    "\n",
    "        validation_losses:  list of floats\n",
    "                            Validation loss for every evaluation iteration, can be used for plotting the validation loss\n",
    "                            over time\n",
    "        \n",
    "        train_losses:       list of floats\n",
    "                            Training loss for every evaluation iteration, can be used for plotting the training loss over time\n",
    "\n",
    "        converged_after:    int\n",
    "                            it - patience * eval_every, where it is the iteration in which patience hits 0,\n",
    "                            or -1 if we hit max_steps before converging.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    nnz_mask = sp.coo_matrix((np.ones(len(non_zero_idx)), (non_zero_idx[:, 0], non_zero_idx[:, 1])), shape=R.shape, dtype = \"unit8\").tocsr()\n",
    "\n",
    "    nnz_mask_col = nnz_mask.tocsc()\n",
    "\n",
    "    cols = nnz_mask.T.tolil().rows\n",
    "    rows = nnz_mask.tolil().rows\n",
    "\n",
    "    reg  = Ridge(alpha=reg_lambda, fit_intercept = False)\n",
    "\n",
    "    Q, P = initialize_Q_P(R, k, init)\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    best_val_losses = best_Q = best_P = converged_after = -1\n",
    "\n",
    "    train_idx = tuple(non_zero_idx.T)\n",
    "\n",
    "    bef = -1\n",
    "    times = []\n",
    "    for it in range(max_steps):\n",
    "        if bef != -1:\n",
    "            times.append(time.time()-bef)\n",
    "        bef = time.time()\n",
    "\n",
    "        if it % eval_every ==0:\n",
    "            train_loss = loss(R[train_idx].A1, train_idx, Q, P, reg_lambda)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            val_loss = loss(val_values, val_idx, Q, P, reg_lambda)\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            if best_val_loss < 0 or val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_Q = Q\n",
    "                best_P = P\n",
    "                current_patience = patience\n",
    "            else:\n",
    "                current_patience -=1\n",
    "\n",
    "            if current_patience ==0:\n",
    "                converged_after = it - patience * eval_every\n",
    "                break\n",
    "\n",
    "        print(\"Iteration {}, training loss: {:.3f}, validation loss: {:.3f}\".format(it, train_loss, val_loss))\n",
    "\n",
    "        if optimizer == 'sgd':\n",
    "            sgd_indices = np.arrange(len(train_idx[0]))\n",
    "            np.random.shuffle(sgd_indices)\n",
    "\n",
    "            for idx in sgd_indices:\n",
    "                u, i = train_idx[0][idx], train_idx[1][idx]\n",
    "                prediction=Q[u, :].dot(P[:, i])\n",
    "                e = (R[u, i] - prediction) # error\n",
    "\n",
    "                # Update latent factors\n",
    "                Q[u, :] += lr * (e * P[:, i]-reg_lambda * Q[u, :])\n",
    "                P[:, i] += lr * (e * Q[u, :]-reg_lambda * P[:, i])\n",
    "\n",
    "        else:\n",
    "            # fix Q and update P\n",
    "            for rating_idx in range(R.shape[1]):\n",
    "                nnz_idx = cols[rating_idx]\n",
    "                res = reg.fit(Q[nnz_idx], np.squeeze(R[nnz_idx, rating_idx].toarray()))\n",
    "                P[:, rating_idx] = res.coef_\n",
    "\n",
    "            for user_idx in range(R.shape[0]):\n",
    "                 nnz_idx = rows[user_idx]\n",
    "                 res = reg.fit(P[:, nnz_idx].T, np.squeeze(R[user_idx, nnz_idx].toarray()))\n",
    "                 Q[user_idx, :] = res.coef_\n",
    "            \n",
    "        print(\"Converged after {} iteration, ob average {:.3f}s per iteration\".format(converged_after, np.mean(times)))\n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6b7a25b8b2426eb550c8146811605d9bdb61f23d99ada7f1c3a4f71b608de8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
