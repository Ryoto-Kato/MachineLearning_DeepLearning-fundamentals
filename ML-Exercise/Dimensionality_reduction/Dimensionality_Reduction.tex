\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Dimensionality\_Reduction}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{dimensionality-reduction-1}{%
\section{Dimensionality reduction 1}\label{dimensionality-reduction-1}}

\hypertarget{pca-and-svd}{%
\subsection{PCA and SVD}\label{pca-and-svd}}

\begin{verbatim}
Linear dimensionality reduction
\end{verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{principal-component-analysis}{%
\subsection{Principal Component
Analysis}\label{principal-component-analysis}}

\begin{itemize}
\tightlist
\item
  Linear dimensionality reduction method
\item
  Find optimal orthogonal transformation such that covariance between
  the new dimensions is 0

  \begin{itemize}
  \tightlist
  \item
    Exploits eigen decomposition
  \item
    By the orthogonal transformation, we could ignore the low variance
    direction to reduce the dimensionality of the features.
  \end{itemize}
\item
  Transformations

  \begin{itemize}
  \tightlist
  \item
    Find the basis (eigen vector of covariance matrix) such that
    independent each other
  \item
    Diagonalization by changing the basis (inner-product of the original
    matrix and basis (new coordinate system))
  \end{itemize}
\end{itemize}

\$\$ \mathbf{X} \text{: Data original coordinates}

\begin{verbatim}
\\

\hat{\mathbf{X}} \text{: Centralized dataset}

\\

\mathbf{\Gamma} \text{: New coordinate system (Eigen vectors matrix)}

\\

\mathbf{\Lambda} \text{: Variance of respective direction (Eigen values matrix)}

\\
\end{verbatim}

\[
Eigen Decomposition of covariance matrix of original data
\] \Sigma\_\{\hat{\mathbf{X}}\} = \mathbf{\Gamma} \mathbf{\Lambda}
\mathbf{\Gamma}\^{}T \$\$

New coordinate system \[\mathbf{\Gamma}\]

Variance of each direction Covariance matrix on the new coordinate
\[\mathbf{\Lambda}\]

    \hypertarget{pca}{%
\section{PCA}\label{pca}}

\hypertarget{process-of-principal-component-analysis}{%
\subsubsection{Process of principal component
analysis}\label{process-of-principal-component-analysis}}

Given input Args \(\mathbf{X} \text{: array, shape[N, D]}\) - N :
\#samples, D : features dimensions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Calculate the mean over samples
  \[\mathbf{x_m} \text{: array, shape [D, 1]}\]
\item
  Centralized (Standardized) the given data matrix around the mean
  \(\mathbf{x_m}\) \[\hat{\mathbf{X}} = X - x_m[\text{none}, :]\]
  \[\text{or}\] \[\hat{\mathbf{X}} = X -  1_N@x_m^T\]
\item
  Calculate the covariance matrix of the centralized data matrix
  \[\Sigma_{\hat{\mathbf{X}}} = \frac{1}{N}\hat{\mathbf{X}}^T\hat{\mathbf{X}}\]
\item
  Eigendecomposition

  \begin{itemize}
  \item
    Derived the eigen values and eigenvectos of
    \(\Sigma_{\hat{\mathbf{X}}}\) \[
    \Sigma_{\hat{\mathbf{X}}} = \mathbf{\Gamma}\mathbf{\Lambda}\mathbf{\Gamma^T}
    \]
  \item
    \(\mathbf{\Gamma}\) : Eigenvector matrix, shape{[}D, D{]}

    \begin{itemize}
    \tightlist
    \item
      Orthonormal matrix

      \begin{itemize}
      \tightlist
      \item
        All rows are independent with each other.
      \end{itemize}
    \end{itemize}
  \item
    \(\mathbf{\Lambda}\) : Eigenvalue matrix, dig(1, \ldots. , D)

    \begin{itemize}
    \tightlist
    \item
      variance of each direction
    \item
      none-covariance between the direction
    \item
      Directions (new coordinate systems) are independent with each
      other
    \end{itemize}
  \end{itemize}
\item
  Plot the original data \(\mathbf{X}\) and the eigenvectors to a single
  diagram

  \begin{itemize}
  \tightlist
  \item
    To obtain optimal diagonal transformation system onto M-dim space
  \item
    We need to prune the eigenvectors' matrix leaving out only
    corresponding M the largest eigenvalues
  \item
    We could obtain the
  \item
    \(\mathbf{\Gamma_{prune}}\): some columns are zero
  \end{itemize}
\item
  Transform all vectors in X in this new subspace by expressing all
  vectors in X in this new basis (project the vectors in \(\mathbf{X}\)
  onto the M-dim subspace).
\end{enumerate}

    \begin{verbatim}
Transformed dataset
\end{verbatim}

\[\mathbf{Y}=\mathbf{X}\mathbf{\Gamma}\] Covariance matrix of
transformed dataset
\[\mathbf{\Sigma_\mathbf{Y}}=\mathbf{\Lambda}=\mathbf{\Gamma}^T{\mathbf{\Sigma_{\hat{\mathbf{X}}}}}\mathbf{\Gamma}\]

    The given data X

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
              \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
              \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
              \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\PY{n}{N}\PY{p}{,} \PY{n}{D} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sample: }\PY{l+s+si}{\PYZob{}}\PY{n}{N}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{features: }\PY{l+s+si}{\PYZob{}}\PY{n}{D}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{k}{for} \PY{n}{idx}\PY{p}{,} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{    }\PY{l+s+si}{\PYZob{}}\PY{n}{x}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
shape: (17, 2)
sample: 17
features: 2
    [-3 -2]
    [-2 -1]
    [-1  0]
    [0 1]
    [1 2]
    [2 3]
    [-2 -2]
    [-1 -1]
    [0 0]
    [1 1]
    [2 2]
    [-2 -3]
    [-1 -2]
    [ 0 -1]
    [1 0]
    [2 1]
    [3 2]
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate the mean over samples
  \[\mathbf{x_m} \text{: array, shape [D, 1]}\]
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Axis = over which axis we take mean}
\PY{c+c1}{\PYZsh{} Keepdims(False) for the subsequent operation}
\PY{c+c1}{\PYZsh{} To use augumentation in the following operation, }
\PY{c+c1}{\PYZsh{} we should squeeze the dimension of the derived mean of data vectors over samples}
\PY{n}{x\PYZus{}m} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{x\PYZus{}m}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{x\PYZus{}m}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[0. 0.]
shape: (2,)
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Centralized (Standardized) the given data matrix around the mean
  \(\mathbf{x_m}\) \[\hat{\mathbf{X}} = X - x_m[None, :]\] \[\text{or}\]
  \[\hat{\mathbf{X}} = X - x_m^T 1_N\]
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}hat} \PY{o}{=} \PY{n}{X} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}m}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{p}{:}\PY{p}{]}
\PY{c+c1}{\PYZsh{}\PYZsh{} alternative}
\PY{c+c1}{\PYZsh{} rx\PYZus{}m = X.mean(0, keepdims=True)}
\PY{c+c1}{\PYZsh{} one\PYZus{}n = np.ones((N, 1))}
\PY{c+c1}{\PYZsh{} print(f\PYZdq{}1\PYZus{}N shape: \PYZob{}one\PYZus{}n.shape\PYZcb{}\PYZdq{})}
\PY{c+c1}{\PYZsh{} X\PYZus{}hat = X \PYZhy{} one\PYZus{}n @ rx\PYZus{}m}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{X\PYZus{}hat}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{X\PYZus{}hat}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[-3. -2.]
 [-2. -1.]
 [-1.  0.]
 [ 0.  1.]
 [ 1.  2.]
 [ 2.  3.]
 [-2. -2.]
 [-1. -1.]
 [ 0.  0.]
 [ 1.  1.]
 [ 2.  2.]
 [-2. -3.]
 [-1. -2.]
 [ 0. -1.]
 [ 1.  0.]
 [ 2.  1.]
 [ 3.  2.]]
shape: (17, 2)
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Calculate the covariance matrix of the centralized data matrix
  \[\Sigma_{\hat{\mathbf{X}}} = \frac{1}{N}\hat{\mathbf{X^T}}\hat{\mathbf{X}}\]
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{C\PYZus{}X} \PY{o}{=} \PY{n}{X\PYZus{}hat}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)} \PY{o}{@} \PY{n}{X\PYZus{}hat}
\PY{n}{C\PYZus{}X} \PY{o}{=} \PY{n}{C\PYZus{}X} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{N}\PY{p}{)}
\PY{c+c1}{\PYZsh{} print(C\PYZus{}X)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{C\PYZus{}X}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
shape: (2, 2)
    \end{Verbatim}

    \hypertarget{function-to-get-covariance-matrix}{%
\section{Function to get covariance
matrix}\label{function-to-get-covariance-matrix}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}covariance}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        X: array[N, D]}
\PY{l+s+sd}{        data matrix including each data vectors on original space (Usually, Cartesian Space)}
\PY{l+s+sd}{    Return:}
\PY{l+s+sd}{        C\PYZus{}X: array[N, N]}
\PY{l+s+sd}{        covariance matrix of the centralized data of given data matrix X}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{c+c1}{\PYZsh{} taking mean over samples}
    \PY{c+c1}{\PYZsh{} x\PYZus{}m: array(D)}

    \PY{n}{x\PYZus{}m} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Centralized X}
    \PY{c+c1}{\PYZsh{} X\PYZus{}hat: array[N, D], dtype=Float}

    \PY{n}{X\PYZus{}hat} \PY{o}{=} \PY{n}{X} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}m}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{p}{:}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Calculating the covariance matrix of the centralized data X\PYZus{}hat}
    \PY{c+c1}{\PYZsh{} C\PYZus{}X: array[N, N], dtype=Float}

    \PY{n}{C\PYZus{}X} \PY{o}{=} \PY{n}{X\PYZus{}hat}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)} \PY{o}{@} \PY{n}{X\PYZus{}hat}
    \PY{c+c1}{\PYZsh{} Normalization}
    \PY{n}{C\PYZus{}X} \PY{o}{*}\PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{N}\PY{p}{)}

    \PY{k}{return} \PY{n}{C\PYZus{}X}
\end{Verbatim}
\end{tcolorbox}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Eigendecomposition

  \begin{itemize}
  \item
    Derived the eigen values and eigenvectos of
    \(\Sigma_{\hat{\mathbf{X}}}\) \[
    \Sigma_{\hat{\mathbf{X}}} = \mathbf{\Gamma}\mathbf{\Lambda}\mathbf{\Gamma^T}
    \]
  \item
    \(\mathbf{\Gamma}\) : Eigenvector matrix, shape{[}D, D{]}

    \begin{itemize}
    \tightlist
    \item
      Orthonormal matrix

      \begin{itemize}
      \tightlist
      \item
        All rows are independent with each other.
      \end{itemize}
    \end{itemize}
  \item
    \(\mathbf{\Lambda}\) : Eigenvalue matrix, dig(1, \ldots. , D)

    \begin{itemize}
    \tightlist
    \item
      variance of each direction
    \item
      none-covariance between the direction
    \item
      Directions (new coordinate systems) are independent with each
      other
    \end{itemize}
  \end{itemize}
\end{enumerate}

    Using Numpy eigendecomposition function - numpy.linalg.eig function

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{eig\PYZus{}values}\PY{p}{,} \PY{n}{eig\PYZus{}vectors} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{eig}\PY{p}{(}\PY{n}{C\PYZus{}X}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gamma:}\PY{l+s+si}{\PYZob{}}\PY{n}{eig\PYZus{}vectors}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{eig\PYZus{}vectors}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lammda:}\PY{l+s+si}{\PYZob{}}\PY{n}{eig\PYZus{}values}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{eig\PYZus{}values}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{max\PYZus{}dim} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{eig\PYZus{}values}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{max\PYZus{}dim}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Gamma:(2, 2)
 [[ 0.70710678 -0.70710678]
 [ 0.70710678  0.70710678]]
Lammda:(2,)
 [5.29411765 0.35294118]
0
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{I} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{eig\PYZus{}values}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{I}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

\PY{n}{\PYZus{}dig\PYZus{}eig\PYZus{}values} \PY{o}{=} \PY{n}{I} \PY{o}{*} \PY{n}{eig\PYZus{}values}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{diagonal mat}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}dig\PYZus{}eig\PYZus{}values}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(2, 2)
diagonal mat
 [[5.29411765 0.        ]
 [0.         0.35294118]]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}eigen}\PY{p}{(}\PY{n}{Cov\PYZus{}x}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Args (1):}
\PY{l+s+sd}{        Cov\PYZus{}x : array[D, D]}
\PY{l+s+sd}{            Covariance matrix of the standardized given data matrix X}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns (2):}
\PY{l+s+sd}{        g : array[D, D]}
\PY{l+s+sd}{            eigen\PYZus{}vectors matrix}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        l : array[D, D]}
\PY{l+s+sd}{            eigen\PYZus{}values diagonal matrix}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{n}{D} \PY{o}{=} \PY{n}{Cov\PYZus{}x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}

    \PY{n}{l}\PY{p}{,} \PY{n}{g} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{eig}\PY{p}{(}\PY{n}{Cov\PYZus{}x}\PY{p}{)}
    \PY{n}{I} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{n}{D}\PY{p}{)}
    \PY{n}{l} \PY{o}{=} \PY{n}{I} \PY{o}{*} \PY{n}{l}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{p}{:}\PY{p}{]}

    \PY{k}{return} \PY{n}{g}\PY{p}{,} \PY{n}{l}
\end{Verbatim}
\end{tcolorbox}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Plot the original data \(\mathbf{X}\) and the eigenvectors to a single
  diagram

  \begin{itemize}
  \tightlist
  \item
    To obtain optimal diagonal transformation system onto M-dim space
  \item
    We need to prune the eigenvectors' matrix leaving out only
    corresponding M the largest eigenvalues
  \item
    We could obtain the
  \item
    \(\mathbf{\Gamma_{prune}}\): some columns are zero
  \end{itemize}
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} plot given data}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{equal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(-3.3, 3.3, -3.3, 3.3)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_21_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} plot the mean of the data}
\PY{n}{mean} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
[<matplotlib.lines.Line2D at 0x7f0c9afc0130>]
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_22_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} plot summery}

\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{equal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{mean} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}

\PY{n}{Sigma} \PY{o}{=} \PY{n}{get\PYZus{}covariance}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n}{g}\PY{p}{,} \PY{n}{l} \PY{o}{=} \PY{n}{get\PYZus{}eigen}\PY{p}{(}\PY{n}{Sigma}\PY{p}{)}

\PY{n}{max\PYZus{}dim} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{l}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{l}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
    \PY{n}{variance} \PY{o}{=} \PY{n}{l}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}
    \PY{n}{value} \PY{o}{=} \PY{n+nb}{str}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{variance}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
    \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{n}{max\PYZus{}dim}\PY{p}{:}
        \PY{n}{plt}\PY{o}{.}\PY{n}{arrow}\PY{p}{(}\PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{width}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+si}{\PYZob{}}\PY{n}{value}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,} \PY{p}{[}\PY{n}{g}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{)}
    \PY{k}{else}\PY{p}{:}
        \PY{n}{plt}\PY{o}{.}\PY{n}{arrow}\PY{p}{(}\PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{width}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{value}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{p}{[}\PY{n}{g}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_23_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{singular-value-decomposition-svd}{%
\subsection{Singular Value Decomposition
(SVD)}\label{singular-value-decomposition-svd}}

\[
\mathbf{M} = \mathbf{U} \cdot \mathbf{\Sigma} \cdot \mathbf{V}
\]

\begin{itemize}
\tightlist
\item
  \(\mathbf{U}\): array{[}N, D{]}

  \begin{itemize}
  \tightlist
  \item
    Left Singular Matrix
  \end{itemize}
\item
  \(\mathbf{\Sigma}\): array {[}D, D{]}

  \begin{itemize}
  \tightlist
  \item
    Singular Matrix
  \end{itemize}
\item
  \(\mathbf{V}\): array{[}D, D{]}

  \begin{itemize}
  \tightlist
  \item
    Right Singular Matrix
  \end{itemize}
\end{itemize}

    \hypertarget{pca-and-svd}{%
\subsection{PCA and SVD}\label{pca-and-svd}}

\begin{itemize}
\tightlist
\item
  Relationship \[\lambda_i = \frac{s_i^2}{N}\]
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Given data}
\PY{n}{M} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{)}

\PY{n}{N}\PY{p}{,} \PY{n}{D} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{M}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sample: }\PY{l+s+si}{\PYZob{}}\PY{n}{N}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{features: }\PY{l+s+si}{\PYZob{}}\PY{n}{D}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{k}{for} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{m} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{M}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{    }\PY{l+s+si}{\PYZob{}}\PY{n}{m}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
shape: (3, 2)
sample: 17
features: 2
    [1 2]
    [6 3]
    [0 2]
    \end{Verbatim}

    Using numpy linalg library - np.linalg.svd

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{u}\PY{p}{,} \PY{n}{s}\PY{p}{,} \PY{n}{v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{svd}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{u}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{s}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{s}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{s}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{v}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{v}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(17, 17)
(2,)
<class 'numpy.ndarray'>
[9.48683298 2.44948974]
(2, 2)
[[ 0.70710678  0.70710678]
 [ 0.70710678 -0.70710678]]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{equal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{mean} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{mean}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}

\PY{n}{max\PYZus{}dim} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{s}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{max\PYZus{}dim}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{s}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{variance} \PY{o}{=} \PY{n}{s}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{/} \PY{p}{(}\PY{n}{N}\PY{p}{)}
    \PY{n}{value} \PY{o}{=} \PY{n+nb}{str}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{variance}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
    \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{n}{max\PYZus{}dim}\PY{p}{:}
        \PY{n}{plt}\PY{o}{.}\PY{n}{arrow}\PY{p}{(}\PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{width}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+si}{\PYZob{}}\PY{n}{value}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,} \PY{p}{[}\PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
    \PY{k}{else}\PY{p}{:}
        \PY{n}{plt}\PY{o}{.}\PY{n}{arrow}\PY{p}{(}\PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mean}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{width}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{value}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{p}{[}\PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[0. 0.]
0
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_29_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{dimensionality-reduction-2}{%
\section{Dimensionality reduction 2}\label{dimensionality-reduction-2}}

\hypertarget{matrix-factorization}{%
\subsection{Matrix Factorization}\label{matrix-factorization}}

\begin{verbatim}
Non-linear dimensionality reduction
\end{verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{time}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{sparse} \PY{k}{as} \PY{n+nn}{sp}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{sparse}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k+kn}{import} \PY{n}{svds}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{Ridge}

\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{recommendation-system}{%
\section{Recommendation system}\label{recommendation-system}}

\begin{itemize}
\tightlist
\item
  Restaurant recommendation system
\item
  Primary optimization problem

  \begin{itemize}
  \tightlist
  \item
    {[}Goal{]} minimize the reconstruction error

    \begin{itemize}
    \tightlist
    \item
      Matrix \(\mathbf{R}\) completion task
    \end{itemize}
  \item
    Predict the ratings a user will give to a restaurant they have not
    yet rated based on a latent factor model
  \item
    We are going to factorize the rating matrix by \(\mathbf{Q}\) and
    \(\mathbf{P}\) given \(\mathbf{R}\)
  \item
    Given \(\mathbf{R}\)

    \begin{itemize}
    \tightlist
    \item
      \(\mathbf{r_{ui}}\) : ratings to item i by user u
    \end{itemize}
  \end{itemize}
\item
  Args

  \begin{itemize}
  \tightlist
  \item
    \(\mathbf{R}\): array{[}N, D{]}

    \begin{itemize}
    \tightlist
    \item
      rating matrix, sparse
    \end{itemize}
  \item
    S=\{(u, i)\textbar{}\(r_{ui}\) != None\}
  \end{itemize}
\item
  Returns (optimum solution)

  \begin{itemize}
  \tightlist
  \item
    \(\mathbf{Q}\): array{[}N, K{]}

    \begin{itemize}
    \tightlist
    \item
      \(\mathbf{q}_u\) : Latent factor for user \(u\)
    \end{itemize}
  \item
    \(\mathbf{P}\): array{[}K, D{]}

    \begin{itemize}
    \tightlist
    \item
      \(\mathbf{p}_i\) : Latent factor for item \(i\)
    \end{itemize}
  \end{itemize}
\item
  Objective function (minimization of reconstruction error)

  \begin{itemize}
  \tightlist
  \item
    including the regularization term

    \begin{itemize}
    \tightlist
    \item
      If R is too sparse to reconstruct,

      \begin{itemize}
      \tightlist
      \item
        Regularization term becomes \textbf{Dominant}

        \begin{itemize}
        \tightlist
        \item
          push latent factors to the undetermined area (minimize the
          length)
        \end{itemize}
      \end{itemize}
    \item
      Else,

      \begin{itemize}
      \tightlist
      \item
        Sum of Squared loss becomes \textbf{Dominant}
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\[
\mathcal{L} = \min{P, Q}_{\sum{(u, i) \in S}} (R_{ui} - \mathbf{q}_u\mathbf{p}_i^T)^2 + \lambda [\sum_i{\left\lVert \mathbf{p}_i \right\rVert}^2 + \sum_u{\left\lVert \mathbf{q}_u \right\rVert}^2]
\]

    \hypertarget{how-to-solve-this-minimization-problem}{%
\subsection{How to solve this minimization
problem}\label{how-to-solve-this-minimization-problem}}

\begin{itemize}
\tightlist
\item
  Problem to solve

  \begin{itemize}
  \tightlist
  \item
    We need to optimize \textbf{TWO VARIABLES} \{P and Q\} at the same
    time \[
    \mathcal{L} = \min{P, Q}_{\sum{(u, i) \in S}} (R_{ui} - \mathbf{q}_u\mathbf{p}_i^T)^2 + \lambda [\sum_i{\left\lVert \mathbf{p}_i \right\rVert}^2 + \sum_u{\left\lVert \mathbf{q}_u \right\rVert}^2]
    \]
  \end{itemize}
\end{itemize}

\hypertarget{methods}{%
\subsubsection{Methods}\label{methods}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Alternating optimization

  \begin{itemize}
  \tightlist
  \item
    We assume that one of the free parameters is given
  \item
    Optimize the two parameters in turns
  \end{itemize}
\item
  Stochastic gradient descent (SGD)

  \begin{itemize}
  \tightlist
  \item
    Sample \(\mathbf{r}_{ui}\) (mini-batch)
  \item
    Optimize the parameters by approximating the loss of all data
    samples with sampled dataset.
  \end{itemize}
\end{enumerate}

    \hypertarget{load-and-pre-process-the-data}{%
\subsection{Load and Pre-process the
Data}\label{load-and-pre-process-the-data}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ratings} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{yelp\PYZhy{}dataset.npy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{}\PYZsh{} given data interior}
\PY{c+c1}{\PYZsh{}\PYZsh{} [user\PYZus{}id, restaurant, ratings]}
\PY{c+c1}{\PYZsh{}   [[101968   1880      1]}
\PY{c+c1}{\PYZsh{}   [101968    284      5]}
\PY{c+c1}{\PYZsh{}   [101968   1378      2]}
\PY{c+c1}{\PYZsh{}   ...}
\PY{c+c1}{\PYZsh{}   [ 72452   2100      4]}
\PY{c+c1}{\PYZsh{}   [ 72452   2050      5]}
\PY{c+c1}{\PYZsh{}   [ 74861   3979      5]]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} shape of the rating matrix}
\PY{c+c1}{\PYZsh{} Given matrix is containing the ratings by each user}

\PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{n\PYZus{}rests} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{ratings}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{ratings}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}users: }\PY{l+s+si}{\PYZob{}}\PY{n}{n\PYZus{}users} \PY{o}{+} \PY{l+m+mi}{1}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}items (restaurants): }\PY{l+s+si}{\PYZob{}}\PY{n}{n\PYZus{}rests} \PY{o}{+} \PY{l+m+mi}{1}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data type: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{type}\PY{p}{(}\PY{n}{ratings}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\#users: 337867
\#items (restaurants): 5899
data type: <class 'numpy.ndarray'>
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} We need to store this matrix as a sparse matrix to avoid out\PYZhy{}of\PYZhy{}memory issues}
\PY{n}{R} \PY{o}{=} \PY{n}{sp}\PY{o}{.}\PY{n}{coo\PYZus{}matrix}\PY{p}{(}\PY{p}{(}\PY{n}{ratings}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{(}\PY{n}{ratings}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ratings}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{shape} \PY{o}{=} \PY{p}{(}\PY{n}{n\PYZus{}users}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}rests}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{} R interior}
\PY{c+c1}{\PYZsh{}  (User\PYZus{}id, restaurant), rating}
\PY{c+c1}{\PYZsh{}     (0, 2050)	        5}
\PY{c+c1}{\PYZsh{}     (1, 36)	        1}
\PY{c+c1}{\PYZsh{}     (1, 580)	        5}
\PY{c+c1}{\PYZsh{}     (1, 628)	        5}
\PY{c+c1}{\PYZsh{}     (1, 703)	        1}
\PY{c+c1}{\PYZsh{}     (1, 774)	        5}
\PY{c+c1}{\PYZsh{}     (1, 1303)	        4}
\PY{c+c1}{\PYZsh{}     (1, 2345)	        4}
\PY{c+c1}{\PYZsh{}     (1, 2809)	        5}
\PY{c+c1}{\PYZsh{}     (1, 3870)	        4}
\PY{c+c1}{\PYZsh{}     (1, 4193)	        5}
\PY{c+c1}{\PYZsh{}     (1, 5256)	        5}
\PY{c+c1}{\PYZsh{}     (1, 5344)	        4}
\PY{c+c1}{\PYZsh{}     (1, 5703)	        4}
\PY{c+c1}{\PYZsh{}     (1, 5890)	        5}
\PY{c+c1}{\PYZsh{}     (2, 3694)	        5}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{solution-for-the-cold-start-problem}{%
\subsection{Solution for the cold start
problem}\label{solution-for-the-cold-start-problem}}

\begin{itemize}
\tightlist
\item
  Cold start problem

  \begin{itemize}
  \tightlist
  \item
    When a new user is coming into the recommendation system, we can not
    predict about his/her future rating because there is no history
  \end{itemize}
\item
  In preprocessing step

  \begin{itemize}
  \tightlist
  \item
    We recursively remove all users and restaurants with 10 or less
    ratings
  \item
    Then, we randomly select 200 data points for the validation and
    tests sets, respectively
  \item
    After this, we subtract the mean rating for each user to account for
    this global effects (standardize)
  \end{itemize}
\end{itemize}

\textbf{NOTE}: Zero in R is the rating with 0 not the `unknown' zeros in
the matrix. We store the indices for which we are rating data available
in a separate variable

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{cold\PYZus{}start\PYZus{}preprocessing}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{n}{min\PYZus{}entries}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Recursively removes rows and columns from the input matrix}
\PY{l+s+sd}{    which have less ratings than min\PYZus{}entries}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        matrix: array[n\PYZus{}users, n\PYZus{}items]}
\PY{l+s+sd}{            rating data matrix R}
\PY{l+s+sd}{        min\PYZus{}entries: int}
\PY{l+s+sd}{            minimum entries to be arrowed to exist in the matrix}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        matrix: sp.spmatrix, shape[N\PYZsq{}, D\PYZsq{}]}
\PY{l+s+sd}{        The pre\PYZhy{}processed matrix \PYZhy{}\PYZgt{} where N\PYZsq{} \PYZlt{}= N, D\PYZsq{} \PYZlt{}= D}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shape before: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{matrix}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{} V \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{shape} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{k}{while} \PY{n}{matrix}\PY{o}{.}\PY{n}{shape} \PY{o}{!=} \PY{n}{shape}\PY{p}{:}
        \PY{n}{shape} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{shape}
        \PY{c+c1}{\PYZsh{} Make stencil buffer (mask) masking more than 0 entries}
        \PY{n}{nnz} \PY{o}{=} \PY{n}{matrix} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}
        \PY{c+c1}{\PYZsh{} Make stencil buffer masking the row which has less than minimum entries}
        \PY{c+c1}{\PYZsh{} .A1 returns flatten matrix}
        \PY{n}{row\PYZus{}ixs} \PY{o}{=} \PY{n}{nnz}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{A1} \PY{o}{\PYZgt{}} \PY{n}{min\PYZus{}entries}
        \PY{c+c1}{\PYZsh{} Only leave out the rows having more than minimum entries}
        \PY{n}{matrix} \PY{o}{=} \PY{n}{matrix}\PY{p}{[}\PY{n}{row\PYZus{}ixs}\PY{p}{]}

        \PY{c+c1}{\PYZsh{} Make stencil buffer masking more than 0 entries}
        \PY{n}{nnz} \PY{o}{=} \PY{n}{matrix} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}
        \PY{c+c1}{\PYZsh{} Make stencil buffer masking the column which has less than minimum entries}
        \PY{c+c1}{\PYZsh{} .A1 returns flatten matrix}
        \PY{n}{col\PYZus{}ixs} \PY{o}{=} \PY{n}{nnz}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{A1} \PY{o}{\PYZgt{}} \PY{n}{min\PYZus{}entries}
        \PY{c+c1}{\PYZsh{} Only leave out the columns having more than minimum entries}
        \PY{n}{matrix} \PY{o}{=} \PY{n}{matrix}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{col\PYZus{}ixs}\PY{p}{]}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shape after: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{matrix}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
    \PY{n}{nnz} \PY{o}{=} \PY{n}{matrix} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}

    \PY{k}{assert} \PY{p}{(}\PY{n}{nnz}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{A1} \PY{o}{\PYZgt{}} \PY{n}{min\PYZus{}entries}\PY{p}{)}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{p}{)}
    \PY{k}{assert} \PY{p}{(}\PY{n}{nnz}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{A1} \PY{o}{\PYZgt{}} \PY{n}{min\PYZus{}entries}\PY{p}{)}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{p}{)}

    \PY{k}{return} \PY{n}{matrix}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cold\PYZus{}start\PYZus{}preprocessing}\PY{p}{(}\PY{n}{R}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
shape before: (337867, 5899)
----------- V -----------
shape after: (11275, 3531)
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<11275x3531 sparse matrix of type '<class 'numpy.int64'>'
        with 285343 stored elements in Compressed Sparse Row format>
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{subtraction-the-mean-user-rating-from-the-sparse-rating-matrix}{%
\subsection{Subtraction the mean user rating from the sparse rating
matrix}\label{subtraction-the-mean-user-rating-from-the-sparse-rating-matrix}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{dev\PYZus{}mat} \PY{o}{=} \PY{n}{cold\PYZus{}start\PYZus{}preprocessing}\PY{p}{(}\PY{n}{R}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
shape before: (337867, 5899)
----------- V -----------
shape after: (11275, 3531)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{n\PYZus{}rests} \PY{o}{=} \PY{n}{dev\PYZus{}mat}\PY{o}{.}\PY{n}{shape}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{n\PYZus{}users}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{n\PYZus{}rests}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
11275, 3531
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} mean ratings over a user}
\PY{n}{row} \PY{o}{=} \PY{n}{dev\PYZus{}mat}\PY{o}{.}\PY{n}{getrow}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{sum\PYZus{}ratings} \PY{o}{=} \PY{n}{row}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
\PY{n}{num\PYZus{}nnz} \PY{o}{=} \PY{n}{row}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{num\PYZus{}nnz: }\PY{l+s+si}{\PYZob{}}\PY{n}{num\PYZus{}nnz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum\PYZus{}ratings: }\PY{l+s+si}{\PYZob{}}\PY{n}{sum\PYZus{}ratings}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{n}{mean\PYZus{}over\PYZus{}u} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{n\PYZus{}users}\PY{p}{)}

\PY{k}{for} \PY{n}{u} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}users}\PY{p}{)}\PY{p}{:}
    \PY{n}{row} \PY{o}{=} \PY{n}{dev\PYZus{}mat}\PY{o}{.}\PY{n}{getrow}\PY{p}{(}\PY{n}{u}\PY{p}{)}
    \PY{n}{sum\PYZus{}ratings} \PY{o}{=} \PY{n}{row}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
    \PY{n}{num\PYZus{}nnz} \PY{o}{=} \PY{n}{row}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{p}{)}
    \PY{n}{mean\PYZus{}user} \PY{o}{=} \PY{n}{sum\PYZus{}ratings}\PY{o}{/}\PY{n}{num\PYZus{}nnz}
    \PY{n}{mean\PYZus{}over\PYZus{}u}\PY{p}{[}\PY{n}{u}\PY{p}{]} \PY{o}{=} \PY{n}{mean\PYZus{}user}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean ratings over a user}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{mean\PYZus{}over\PYZus{}u}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{mean\PYZus{}over\PYZus{}u}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} mean ratings over an item\PYZbs{}}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean ratings over an item}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{mean\PYZus{}over\PYZus{}i} \PY{o}{=} \PY{n}{dev\PYZus{}mat}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{mean\PYZus{}over\PYZus{}i}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{mean\PYZus{}over\PYZus{}i}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
num\_nnz: 13
sum\_ratings: 52
mean ratings over a user
[4.         4.4        3.72727273 {\ldots} 3.5        4.11764706 2.1       ]
shape: (11275,)

mean ratings over an item
[[0.01800443 0.00381375 0.04399113 {\ldots} 0.00629712 0.00700665 0.01649667]]
shape: (1, 3531)

    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} flatten the mean ratings over a user}
\PY{n}{f\PYZus{}mou} \PY{o}{=} \PY{n}{mean\PYZus{}over\PYZus{}u}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f\PYZus{}mou}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(11275,)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Standardization}
\PY{n}{std\PYZus{}mat} \PY{o}{=} \PY{n}{dev\PYZus{}mat} \PY{o}{\PYZhy{}} \PY{n}{f\PYZus{}mou}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{k+kc}{None}\PY{p}{]}
\PY{c+c1}{\PYZsh{} mean ratings over a user should be close to zero}
\PY{n}{eps} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}10}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{std\PYZus{}mat}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{k}{assert} \PY{p}{(}\PY{n}{std\PYZus{}mat}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{A1} \PY{o}{\PYZlt{}} \PY{n}{eps}\PY{p}{)}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[-3.98527329]
 [-4.38130841]
 [-3.71566129]
 {\ldots}
 [-3.48810535]
 [-4.09782265]
 [-2.07621071]]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{type}\PY{p}{(}\PY{n}{dev\PYZus{}mat}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
scipy.sparse.\_csr.csr\_matrix
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{function-1-centralization}{%
\subsection{function 1:
Centralization}\label{function-1-centralization}}

\begin{verbatim}
for subtraction the man rating per user from the non-zero elements in the input matrix
\end{verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{centralization}\PY{p}{(}\PY{n}{matrix}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Subtract the mean rating per user from the non\PYZhy{}zero elements}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{    matrix: sp.spmatrix, shape [N, D]}
\PY{l+s+sd}{            Input sparse matrix}

\PY{l+s+sd}{    Retuns:}
\PY{l+s+sd}{    matrix: sp.spmatrix, shape[N, D]}
\PY{l+s+sd}{            centralized input matrix at 0 (the mean\PYZhy{}shifted ones)}
\PY{l+s+sd}{    user\PYZus{}means: np.array, shape[N, 1]}
\PY{l+s+sd}{                The mean rating per user that can be used to recover the absolute ratings from the mean\PYZhy{}shifted ones.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{n\PYZus{}items} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{shape}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{n\PYZus{}users}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{n\PYZus{}items}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Create mask of non\PYZus{}zero entries}
    \PY{n}{nnz\PYZus{}mask} \PY{o}{=} \PY{n}{matrix} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}

    \PY{c+c1}{\PYZsh{} Take mean per user}
    \PY{c+c1}{\PYZsh{} uer\PYZus{}means: matrix [n\PYZus{}users, 1]}
    \PY{n}{user\PYZus{}means} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/} \PY{n}{nnz\PYZus{}mask}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} cerate a compressed sparse row matrix(csr\PYZus{}matrix, same type with input matrix}
    \PY{c+c1}{\PYZsh{}  }
    \PY{n}{subtract\PYZus{}mask} \PY{o}{=} \PY{n}{sp}\PY{o}{.}\PY{n}{csr\PYZus{}matrix}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{p}{)}\PY{o}{.}\PY{n}{multiply}\PY{p}{(}\PY{n}{nnz\PYZus{}mask}\PY{p}{)}

    \PY{n}{cent\PYZus{}mat} \PY{o}{=} \PY{n}{matrix} \PY{o}{\PYZhy{}} \PY{n}{subtract\PYZus{}mask}

\PY{c+c1}{\PYZsh{}     assert np.all(np.isclose(matrix.mean(1), 0))}
    \PY{k}{return} \PY{n}{cent\PYZus{}mat}\PY{p}{,} \PY{n}{user\PYZus{}means}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cent\PYZus{}mat}\PY{p}{,} \PY{n}{user\PYZus{}means} \PY{o}{=} \PY{n}{centralization}\PY{p}{(}\PY{n}{dev\PYZus{}mat}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
11275
3531
<class 'numpy.matrix'>

(11275, 1)
    \end{Verbatim}

    \hypertarget{split-the-data-into-a-train-validation-and-test-set}{%
\subsection{Split the data into a train, validation and test
set}\label{split-the-data-into-a-train-validation-and-test-set}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} create train, valid, and test dataset}
\PY{c+c1}{\PYZsh{} create dataloader}

\PY{c+c1}{\PYZsh{} Here we are using sparse matrix, then we need to sample train, valid, and test data samples randomly.}

\PY{c+c1}{\PYZsh{} Configuration}
\PY{c+c1}{\PYZsh{} The number of the valid and test samples}

\PY{n}{n\PYZus{}validation} \PY{o}{=} \PY{l+m+mi}{100}
\PY{n}{n\PYZus{}test} \PY{o}{=} \PY{l+m+mi}{100}

\PY{c+c1}{\PYZsh{} copy centralized matrix}
\PY{n}{matrix\PYZus{}cp} \PY{o}{=} \PY{n}{cent\PYZus{}mat}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{} obtain the index lists which are containing non\PYZhy{}zero variable}
\PY{n}{non\PYZus{}zero\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argwhere}\PY{p}{(}\PY{n}{matrix\PYZus{}cp}\PY{p}{)}

\PY{c+c1}{\PYZsh{} sample indices randomly (user u, item i)}
\PY{n}{ixs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{permutation}\PY{p}{(}\PY{n}{non\PYZus{}zero\PYZus{}idx}\PY{p}{)}

\PY{c+c1}{\PYZsh{} obtain u\PYZhy{}index list and i\PYZhy{}index list as tuple (u, i)}
\PY{n}{val\PYZus{}idx} \PY{o}{=} \PY{n+nb}{tuple}\PY{p}{(}\PY{n}{ixs}\PY{p}{[}\PY{p}{:}\PY{n}{n\PYZus{}validation}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{)}
\PY{n}{test\PYZus{}idx} \PY{o}{=} \PY{n+nb}{tuple}\PY{p}{(}\PY{n}{ixs}\PY{p}{[}\PY{n}{n\PYZus{}validation}\PY{p}{:}\PY{n}{n\PYZus{}validation} \PY{o}{+} \PY{n}{n\PYZus{}test}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{val\PYZus{}idx}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{test\PYZus{}idx}\PY{p}{)}

\PY{c+c1}{\PYZsh{} obtain the array (flatten) }
\PY{n}{val\PYZus{}values} \PY{o}{=} \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{val\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{A1}
\PY{n}{test\PYZus{}values} \PY{o}{=} \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{A1}

\PY{c+c1}{\PYZsh{} Eliminate valid and test samples and obtain train data.}
\PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{val\PYZus{}idx}\PY{p}{]} \PY{o}{=} \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{matrix\PYZus{}cp}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n}{matrix\PYZus{}cp}\PY{o}{.}\PY{n}{eliminate\PYZus{}zeros}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(array([10385,  3958,  7150,  9580,  3493, 10259,  3577, 10359,  6140,
        8635,  5145,  8025, 10858,  4579,  8149,  2923,  9666,  1009,
        6016,  9919,  5618,   725,  9210,  9055,  1580,  8467,  7148,
       10934,  7663,  7783,  9033,  6830,  3187,  1141,  1258,  9198,
        3677, 10827, 11243,  4787,  7959,  2523,  1751,  9235,  9624,
        5676,  4990,  1573, 10983,  9346,  5967,  8026,   578,  4251,
        5088,  5063,  4849,  6944,   889,   691,    20,   569,  8442,
        1504,  8760,  8093,  2455, 10100,  6478,  9963,  4508,  9245,
        5214, 10698,  7522,  4388,  6742,  6195,  9534,  6206,  3411,
        8800,  6671,  9401,  5549,  2163,  6971,   135,  2712,  3066,
        2790,  7581,  4579,   788,   326,  7373,  7708,  2737,  2793,
        6945], dtype=int32), array([2024, 3082, 1408, 2786,   13, 3165, 1767,
146,  246, 3200,  267,
       3355, 1500, 1191,  504,  629, 3504, 1576, 2134, 3317,  534, 2402,
        380,  660, 1321, 1605, 2967,  539, 2586, 2099,  866, 1780,  485,
        760,  101, 3473,  715, 1825, 3215,  157, 2728,  336, 1219, 1618,
       2241,  822, 1462,  989, 1127, 2325,  334, 1363, 1036, 1765, 1528,
       1191, 1911,  706, 2948, 3162, 2022,  768, 2253, 3395,  291, 2520,
       1394, 3045, 2586, 2175, 1973, 3198, 3021, 1699, 3341,  498,   98,
       1394, 2586, 3382, 2747, 3348,  822, 3208, 3144, 1419, 2700,  780,
        410, 2884, 3195, 2255, 3394, 3045,  363,  140,  744, 3002, 1306,
       2788], dtype=int32))
(array([ 8915,  7312,  8840,  4794,  1601,  8360,   387,  3820,  3135,
        2433,  6453,  8469,  1938,  9676,  8030,  3317,  5969,  7794,
        5361,  6496,  4972,  4650, 10910,  2580,  2927,  7802, 11238,
        4460,  3875,  4562,  1824,   980, 11067,   341, 10794,  2697,
        7474,  7472,  3280,  3549,   199,  3642,  1843,  7086,  5400,
        2510,  7004,  9081,  1098,  5355,   227,  9124,  7016,  3512,
       10534, 10773,  6728,  2550,  5808, 10108,  1388,  9608,  4889,
       10967, 10861,  1095,   609,  9664, 10768,  5484, 10359,  7529,
        3482, 10427,  6195,  3776,  8107,  1795,  2953,  8951,   211,
        8743,   363,  6839,  3847,  9538,  1359,  3642,  2734,  8329,
        7730,  3434,  7718, 10402,  1902,  1484,  4901,  2538,  2960,
        9509], dtype=int32), array([1500,  424, 3128, 2251,  936, 1326, 1569,
274, 2546,  922, 1904,
       3152,  904, 1059, 1642, 3447, 2010, 3021, 1222,  283,  583, 2180,
       1576, 3401, 1326, 1011, 2700, 3417, 1813, 3286,  101,  402,  336,
        632, 2002, 1262, 1862, 3485, 1207,  682, 1165, 2474, 3219, 2314,
       3047, 1508, 3029, 3189, 1001, 1936,  295, 1588, 2819, 1860,  523,
        461, 1151, 2728, 1756, 2846, 1999, 3124,  901, 2593, 3342, 2558,
       1002,  522, 1320, 1818, 1422, 3162, 2556, 2977, 2727, 1094, 1841,
       2461, 2169, 2951, 2252,  876,   65, 1394,  224, 1448,  908, 2424,
       1134, 3528, 2474, 1402,  214,  274, 3161, 1507, 3120, 1450, 2531,
       2047], dtype=int32))
(11275, 3531)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{for} \PY{n}{t}\PY{p}{,} \PY{n}{train} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{matrix\PYZus{}cp}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{t} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{k}{break}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train sample}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{train}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid sample}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{):}\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}values}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test sample}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+si}{\PYZob{}}\PY{n}{test\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{test\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{):}\PY{l+s+si}{\PYZob{}}\PY{n}{test\PYZus{}values}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
train sample
  (0, 3526)     1.0
  (0, 3120)     1.0
  (0, 2508)     1.0
  (0, 1694)     1.0
  (0, 461)      1.0
  (0, 416)      -3.0
  (0, 368)      1.0
  (0, 22)       -3.0
valid sample
(10385, 2024):-0.6119402985074629
test sample
(8915, 1500):0.48888888888888893
    \end{Verbatim}

    \hypertarget{function-2-split-the-centralized-data-into-train-valid-and-test}{%
\subsubsection{Function 2: split the centralized data into train, valid,
and
test}\label{function-2-split-the-centralized-data-into-train-valid-and-test}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{split\PYZus{}data}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{n}{n\PYZus{}val}\PY{p}{,} \PY{n}{n\PYZus{}test}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Extract validation and test entries from the input matrix}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        matrix: sp.spmatrix, shape [N, D]}
\PY{l+s+sd}{                The input data matrix}
\PY{l+s+sd}{        n\PYZus{}val:  int}
\PY{l+s+sd}{                The number of validation entries to extract}
\PY{l+s+sd}{        n\PYZus{}test: int}
\PY{l+s+sd}{                The number fo test entries to extract}
\PY{l+s+sd}{        }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        matrix\PYZus{}split:   sp.spmatrix, shape [N, D]}
\PY{l+s+sd}{                        a copy of the input matrix in which the validation and }

\PY{l+s+sd}{        val\PYZus{}idx:        tuple, shape [2, n\PYZus{}val]}
\PY{l+s+sd}{                        The indices of the validation entries}

\PY{l+s+sd}{        test\PYZus{}idx:       tuple, shape [2, n\PYZus{}test]}
\PY{l+s+sd}{                        The indices of the test entries}

\PY{l+s+sd}{        val\PYZus{}values:     np.array, shape [n\PYZus{}val]}
\PY{l+s+sd}{                        The values of the input matrix at the validation indices}

\PY{l+s+sd}{        test\PYZus{}values:    np.array, shape [n\PYZus{}train]}
\PY{l+s+sd}{                        The values of the input matrix at the test indices}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{c+c1}{\PYZsh{} copy the input matrix}
    \PY{n}{matrix\PYZus{}cp} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} obtain indices pair (User u, Item i)}
    \PY{n}{non\PYZus{}zero\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argwhere}\PY{p}{(}\PY{n}{matrix\PYZus{}cp}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} random permutation of the list of indices pair}
    \PY{n}{ixs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{permutation}\PY{p}{(}\PY{n}{non\PYZus{}zero\PYZus{}idx}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} obtain a tuple [n\PYZus{}val, 2] for u and i, respectively}
    \PY{n}{val\PYZus{}idx} \PY{o}{=}\PY{n+nb}{tuple}\PY{p}{(}\PY{n}{ixs}\PY{p}{[}\PY{p}{:}\PY{n}{n\PYZus{}val}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{)}
    \PY{n}{test\PYZus{}idx} \PY{o}{=} \PY{n+nb}{tuple}\PY{p}{(}\PY{n}{ixs}\PY{p}{[}\PY{n}{n\PYZus{}val}\PY{p}{:}\PY{n}{n\PYZus{}val} \PY{o}{+} \PY{n}{n\PYZus{}test}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} obtain the ratings for the validation data}
    \PY{n}{val\PYZus{}values} \PY{o}{=} \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{val\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{A1}

    \PY{c+c1}{\PYZsh{} obtain the ratings for the test data}
    \PY{n}{test\PYZus{}values} \PY{o}{=} \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{A1}

    \PY{c+c1}{\PYZsh{} Set zero to entries which are assigned as valid or test data}
    \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{val\PYZus{}idx}\PY{p}{]} \PY{o}{=} \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{c+c1}{\PYZsh{} Eliminate zero entries}
    \PY{n}{matrix\PYZus{}cp}\PY{o}{.}\PY{n}{eliminate\PYZus{}zeros}\PY{p}{(}\PY{p}{)}

    \PY{k}{return} \PY{n}{matrix\PYZus{}cp}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{p}{,} \PY{n}{val\PYZus{}values}\PY{p}{,} \PY{n}{test\PYZus{}values}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{dev\PYZus{}mat} \PY{o}{=} \PY{n}{cold\PYZus{}start\PYZus{}preprocessing}\PY{p}{(}\PY{n}{R}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
shape before: (337867, 5899)
----------- V -----------
shape after: (3529, 2072)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{n\PYZus{}val} \PY{o}{=} \PY{l+m+mi}{200}
\PY{n}{n\PYZus{}test} \PY{o}{=} \PY{l+m+mi}{200}
\PY{c+c1}{\PYZsh{} split data}
\PY{n}{R\PYZus{}train}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{p}{,} \PY{n}{val\PYZus{}values}\PY{p}{,} \PY{n}{test\PYZus{}values} \PY{o}{=} \PY{n}{split\PYZus{}data}\PY{p}{(}\PY{n}{dev\PYZus{}mat}\PY{p}{,} \PY{n}{n\PYZus{}val}\PY{p}{,} \PY{n}{n\PYZus{}test}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Centralization}
\PY{n}{non\PYZus{}zero\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argwhere}\PY{p}{(}\PY{n}{R\PYZus{}train}\PY{p}{)}
\PY{n}{R\PYZus{}shifted}\PY{p}{,} \PY{n}{user\PYZus{}means} \PY{o}{=} \PY{n}{centralization}\PY{p}{(}\PY{n}{R\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Apply the same shift to the validation and test data}
\PY{n}{val\PYZus{}values\PYZus{}shifted} \PY{o}{=} \PY{n}{val\PYZus{}values} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{val\PYZus{}idx}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{test\PYZus{}values\PYZus{}shifted} \PY{o}{=} \PY{n}{test\PYZus{}values} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{test\PYZus{}idx}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
3529
2072
<class 'numpy.matrix'>

(3529, 1)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{loss}\PY{p}{(}\PY{n}{values}\PY{p}{,} \PY{n}{ixs}\PY{p}{,} \PY{n}{Q}\PY{p}{,} \PY{n}{P}\PY{p}{,} \PY{n}{reg\PYZus{}lambda}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Compute the loss of the latent factor model (at indices ixs)}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        values (list of R\PYZus{}ui): np.array, shape[n\PYZus{}ixs, ]}
\PY{l+s+sd}{                The array with the ground\PYZhy{}truth values}
\PY{l+s+sd}{        ixs (list of ui itself):    tuple, shape[2, n\PYZus{}ixs]}
\PY{l+s+sd}{                The indices at which we want to evaluate the loss(usually the nonzero indices of the unshifted data matrix)}
\PY{l+s+sd}{        Q:  np.array, shape [N. k]}
\PY{l+s+sd}{            The matrix Q of a latent factor model}
\PY{l+s+sd}{        P:  np.array, shape [k, D]}
\PY{l+s+sd}{            The matrix P of a latent factor model}
\PY{l+s+sd}{        reg\PYZus{}lambda: float}
\PY{l+s+sd}{            The regulation strength}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        loss:   float}
\PY{l+s+sd}{                The loss of the latent factor model}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{c+c1}{\PYZsh{} mean of sum of squared error}
    \PY{n}{sse\PYZus{}loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{values} \PY{o}{\PYZhy{}} \PY{n}{Q}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{P}\PY{p}{)}\PY{p}{[}\PY{n}{ixs}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} regularization term }
    \PY{n}{regularization\PYZus{}loss} \PY{o}{=} \PY{n}{reg\PYZus{}lambda} \PY{o}{*} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{P}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{Q}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}

    \PY{k}{return} \PY{n}{sse\PYZus{}loss} \PY{o}{+} \PY{n}{regularization\PYZus{}loss}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{initialization-of-the-q-and-p-for-optimization}{%
\subsection{Initialization of the Q and P for
optimization}\label{initialization-of-the-q-and-p-for-optimization}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{R\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n}{N}\PY{p}{,} \PY{n}{D} \PY{o}{=} \PY{n}{R\PYZus{}train}\PY{o}{.}\PY{n}{shape}
\PY{n}{k} \PY{o}{=} \PY{n}{D}
\PY{n}{Q} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{k}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{Q}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{Q}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n}{P} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{D}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{P}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{P}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(3529, 2072)
<class 'numpy.ndarray'>
(3529, 2072)
<class 'numpy.ndarray'>
(2072, 2072)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{f\PYZus{}R\PYZus{}train} \PY{o}{=} \PY{n}{R\PYZus{}train}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f\PYZus{}R\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{f\PYZus{}R\PYZus{}train}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f\PYZus{}R\PYZus{}train}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{k}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(3529, 2072)
<class 'scipy.sparse.\_csr.csr\_matrix'>
  (0, 3)        5.0
  (0, 24)       3.0
  (0, 219)      4.0
  (0, 333)      2.0
  (0, 344)      3.0
  (0, 393)      5.0
  (0, 470)      4.0
  (0, 530)      5.0
  (0, 570)      3.0
  (0, 585)      3.0
  (0, 657)      4.0
  (0, 664)      4.0
  (0, 711)      5.0
  (0, 799)      4.0
  (0, 825)      3.0
  (0, 872)      4.0
  (0, 1069)     4.0
  (0, 1120)     5.0
  (0, 1188)     5.0
  (0, 1323)     4.0
  (0, 1627)     2.0
  (0, 1648)     4.0
  (0, 1768)     4.0
  (0, 1865)     4.0
  (0, 1946)     2.0
  :     :
  (3528, 735)   2.0
  (3528, 795)   1.0
  (3528, 818)   1.0
  (3528, 839)   4.0
  (3528, 899)   4.0
  (3528, 936)   1.0
  (3528, 1001)  2.0
  (3528, 1005)  1.0
  (3528, 1070)  4.0
  (3528, 1130)  2.0
  (3528, 1144)  2.0
  (3528, 1170)  2.0
  (3528, 1175)  2.0
  (3528, 1215)  2.0
  (3528, 1252)  2.0
  (3528, 1332)  3.0
  (3528, 1363)  1.0
  (3528, 1395)  3.0
  (3528, 1682)  3.0
  (3528, 1685)  4.0
  (3528, 1689)  2.0
  (3528, 1798)  1.0
  (3528, 1945)  5.0
  (3528, 1954)  4.0
  (3528, 1998)  1.0
2072
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} When we use svds function from the scipy.sparse.linalg, we need to set k as following}
\PY{c+c1}{\PYZsh{} `k` must be an integer satisfying `0 \PYZlt{} k \PYZlt{} min(A.shape)`.}
\PY{n}{U}\PY{p}{,} \PY{n}{s}\PY{p}{,} \PY{n}{V} \PY{o}{=} \PY{n}{svds}\PY{p}{(}\PY{n}{f\PYZus{}R\PYZus{}train}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{S} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{s}\PY{p}{)}
\PY{n}{Q} \PY{o}{=} \PY{n}{U}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{S}\PY{p}{)}
\PY{n}{P} \PY{o}{=} \PY{n}{V}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{U}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{s}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{V}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(3529, 100)
(100,)
(100, 2072)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{S} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{s}\PY{p}{)}
\PY{n}{Q} \PY{o}{=} \PY{n}{U}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{S}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{Q}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n}{P} \PY{o}{=} \PY{n}{V}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{P}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(3529, 100)
(100, 2072)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{R\PYZus{}train}\PY{o}{.}\PY{n}{dtype}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
dtype('int64')
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{function-that-initialize-the-latent-factors-q-and-p}{%
\subsubsection{Function that initialize the latent factors Q and
P}\label{function-that-initialize-the-latent-factors-q-and-p}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{initialize\PYZus{}Q\PYZus{}P}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{init} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Initialize the matrices Q and P for a latent factor model}
\PY{l+s+sd}{    Initialize them by using SVD or random}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        matrix: sp.spmatrix, shape [N, D]}
\PY{l+s+sd}{                The matrix to be factorized}
\PY{l+s+sd}{        k:      int}
\PY{l+s+sd}{                The number of latent dimension}
\PY{l+s+sd}{        init:   str in [\PYZsq{}svd\PYZsq{}, \PYZsq{}random\PYZsq{}], default:\PYZsq{}random\PYZsq{}}
\PY{l+s+sd}{                The initialization strategy. \PYZsq{}svd\PYZsq{} means that we use SVD to initialize P and Q}
\PY{l+s+sd}{                \PYZsq{}random\PYZsq{} means we initialize the entries in P and Q randomly in the interval [0. 1)}
\PY{l+s+sd}{                \PYZhy{}\PYZgt{} numpy.random.rand(shape)}
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        Q:  np.print(U.shape)}
\PY{l+s+sd}{            The initialized matrix Q of a latent factor model}
\PY{l+s+sd}{        P:  np.array, shape[k, D]}
\PY{l+s+sd}{            The initialized matrix P of a latent factor model}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}} 

    \PY{n}{N}\PY{p}{,} \PY{n}{D} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{shape}

    \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

    \PY{k}{if} \PY{n}{matrix}\PY{o}{.}\PY{n}{dtype} \PY{o}{!=} \PY{n+nb}{float}\PY{p}{:}
        \PY{n}{matrix} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}

    \PY{k}{if} \PY{n}{init}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
        \PY{n}{Q} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{k}\PY{p}{)}\PY{p}{)}
        \PY{n}{P} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{D}\PY{p}{)}\PY{p}{)}
    \PY{k}{elif} \PY{n}{init}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{svd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
        \PY{n}{U}\PY{p}{,} \PY{n}{s}\PY{p}{,} \PY{n}{V} \PY{o}{=} \PY{n}{svds}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{)}
        \PY{n}{S} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{s}\PY{p}{)}
        \PY{n}{Q} \PY{o}{=} \PY{n}{U}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{S}\PY{p}{)}
        \PY{n}{P} \PY{o}{=} \PY{n}{V}
    \PY{k}{else}\PY{p}{:}
        \PY{k}{raise} \PY{n+ne}{ValueError}
    
    \PY{k}{assert} \PY{n}{Q}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{k}\PY{p}{)}
    \PY{k}{assert} \PY{n}{P}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{D}\PY{p}{)}
    \PY{k}{return} \PY{n}{Q}\PY{p}{,} \PY{n}{P}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Q}\PY{p}{,} \PY{n}{P} \PY{o}{=} \PY{n}{initialize\PYZus{}Q\PYZus{}P}\PY{p}{(}\PY{n}{R\PYZus{}train}\PY{p}{,} \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{100}\PY{p}{,} \PY{n}{init}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{svd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{Q}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{P}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(3529, 100)
(100, 2072)
    \end{Verbatim}

    \hypertarget{optimization}{%
\subsection{Optimization}\label{optimization}}

\begin{itemize}
\tightlist
\item
  Alternating optimization

  \begin{itemize}
  \tightlist
  \item
    We need to optimize Q and P simultaneously in the primary
    optimization problem
  \item
    But this is really difficult to implement
  \item
    We are going to pretend knowing either Q or P at a moment and
    optimize the other variable
  \end{itemize}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{row}  \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
\PY{n}{col}  \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
\PY{n}{data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{n}{A} \PY{o}{=} \PY{n}{sp}\PY{o}{.}\PY{n}{coo\PYZus{}matrix}\PY{p}{(}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{p}{(}\PY{n}{row}\PY{p}{,} \PY{n}{col}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{A}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{A}\PY{p}{)}
\PY{n}{list\PYZus{}rows} \PY{o}{=} \PY{n}{A}\PY{o}{.}\PY{n}{tolil}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{rows}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=list=}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{list\PYZus{}rows}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{A} \PY{o}{=} \PY{n}{A}\PY{o}{.}\PY{n}{tocsc}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{A}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{A}\PY{p}{)}
\PY{n}{list\PYZus{}colums} \PY{o}{=} \PY{n}{A}\PY{o}{.}\PY{n}{tolil}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{rows}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=list=}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{list\PYZus{}colums}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[1 0 1 0]
 [0 1 0 0]
 [0 0 0 0]
 [0 0 0 1]]
  (0, 0)        1
  (0, 2)        1
  (1, 1)        1
  (3, 3)        1
=list=
[list([0, 2]) list([1]) list([]) list([3])]
---------------------------
[[1 0 1 0]
 [0 1 0 0]
 [0 0 0 0]
 [0 0 0 1]]
  (0, 0)        1
  (1, 1)        1
  (0, 2)        1
  (3, 3)        1
=list=
[list([0, 2]) list([1]) list([]) list([3])]
    \end{Verbatim}

    \hypertarget{scipy-sparse}{%
\subsection{Scipy sparse}\label{scipy-sparse}}

\begin{verbatim}
csc vs csr
\end{verbatim}

\begin{itemize}
\tightlist
\item
  CSC: Compressed sparse column

  \begin{itemize}
  \tightlist
  \item
    Sorted in the column indices
  \end{itemize}
\item
  CSR: Compressed sparse row

  \begin{itemize}
  \tightlist
  \item
    Sorted in the row indices They are used for
    write-once-read-many-tasks
  \end{itemize}
\end{itemize}

    \hypertarget{original-data-matrix}{%
\section{Original data matrix}\label{original-data-matrix}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{R:   scipy.sparse.\PYZus{}csr.csr\PYZus{}matrix, shape[337867, 5899]}
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{R}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{R}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{R}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
  (0, 2050)     5
  (1, 36)       1
  (1, 580)      5
  (1, 628)      5
  (1, 703)      1
  (1, 774)      5
  (1, 1303)     4
  (1, 2345)     4
  (1, 2809)     5
  (1, 3870)     4
  (1, 4193)     5
  (1, 5256)     5
  (1, 5344)     4
  (1, 5703)     4
  (1, 5890)     5
  (2, 3694)     5
  (3, 774)      1
  (3, 1291)     1
  (3, 2221)     4
  (4, 2894)     5
  (5, 1446)     4
  (5, 1648)     4
  (5, 1777)     3
  (5, 2008)     5
  (5, 2067)     4
  :     :
  (337859, 3443)        2
  (337859, 3567)        2
  (337859, 3802)        3
  (337859, 3898)        1
  (337859, 3971)        3
  (337859, 4794)        3
  (337859, 4800)        4
  (337859, 4816)        2
  (337859, 5198)        1
  (337859, 5579)        5
  (337859, 5601)        4
  (337859, 5700)        1
  (337860, 4943)        5
  (337861, 5675)        5
  (337862, 493) 3
  (337862, 1281)        5
  (337862, 2814)        4
  (337863, 1026)        4
  (337863, 2127)        5
  (337863, 2416)        4
  (337864, 5165)        2
  (337865, 1238)        5
  (337866, 251) 5
  (337866, 2932)        4
  (337866, 3779)        4
<class 'scipy.sparse.\_csr.csr\_matrix'>
(337867, 5899)
    \end{Verbatim}

    \hypertarget{pre-processed-matrix}{%
\section{Pre-processed matrix}\label{pre-processed-matrix}}

\begin{itemize}
\tightlist
\item
  Avoid cold start
\item
  Prune rows and columns which have less than min\_entries
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{R\PYZus{}prune:    scipy.sparse.\PYZus{}csr.csr\PYZus{}matrix, shape [3529, 2072]}
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{n}{min\PYZus{}entries} \PY{o}{=} \PY{l+m+mi}{20}
\PY{n}{R\PYZus{}prune} \PY{o}{=} \PY{n}{cold\PYZus{}start\PYZus{}preprocessing}\PY{p}{(}\PY{n}{R}\PY{p}{,} \PY{n}{min\PYZus{}entries}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{R\PYZus{}prune}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{R\PYZus{}prune}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
shape before: (337867, 5899)
----------- V -----------
shape after: (3529, 2072)
<class 'scipy.sparse.\_csr.csr\_matrix'>
(3529, 2072)
    \end{Verbatim}

    \hypertarget{centralized-matrix}{%
\section{Centralized matrix}\label{centralized-matrix}}

\begin{itemize}
\tightlist
\item
  Shifted matrix by corresponding mean over ratings (i) by a user (u)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{cent\PYZus{}R: scipy.sparse.\PYZus{}csr.csr\PYZus{}matrix, shape [337867, 5899]}
\PY{l+s+sd}{user\PYZus{}means: numpy.matrix, shape [337867, 1]}
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{n}{cent\PYZus{}R}\PY{p}{,} \PY{n}{user\PYZus{}means} \PY{o}{=} \PY{n}{centralization}\PY{p}{(}\PY{n}{R}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{cent\PYZus{}R}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{cent\PYZus{}R}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
337867
5899
<class 'numpy.matrix'>

(337867, 1)
<class 'scipy.sparse.\_csr.csr\_matrix'>
(337867, 5899)
<class 'numpy.matrix'>
(337867, 1)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{info}\PY{p}{(}\PY{n}{matrix}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n+nb}{type}\PY{p}{(}\PY{n}{matrix}\PY{p}{)}\PY{p}{,} \PY{n}{matrix}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{split-data-into-train-valid-and-test-set}{%
\section{Split data into train, valid, and test
set}\label{split-data-into-train-valid-and-test-set}}

\begin{itemize}
\tightlist
\item
  We can not split the matrix after centralization
\item
  This is because, in the split function

  \begin{itemize}
  \tightlist
  \item
    Replace assigned ratings to valid and test in the training matrix
    with 0
  \item
    If the train\_matrix had already been centralized, you replace
    ratings with respective user's average (mean)
  \item
    We should split the data first following centralization
  \end{itemize}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{n\PYZus{}val} \PY{o}{=} \PY{l+m+mi}{200}
\PY{n}{n\PYZus{}test} \PY{o}{=} \PY{l+m+mi}{200}
\PY{c+c1}{\PYZsh{} get shape of R\PYZus{}prune}
\PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{n\PYZus{}items} \PY{o}{=} \PY{n}{R\PYZus{}prune}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} split pruned matrix into train, valid, and test}
\PY{n}{train\PYZus{}matrix}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{p}{,} \PY{n}{val\PYZus{}values}\PY{p}{,} \PY{n}{test\PYZus{}values} \PY{o}{=} \PY{n}{split\PYZus{}data}\PY{p}{(}\PY{n}{R\PYZus{}prune}\PY{p}{,} \PY{n}{n\PYZus{}val}\PY{p}{,} \PY{n}{n\PYZus{}test}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} centralize only train\PYZus{}matrix by each user rating means}
\PY{c+c1}{\PYZsh{} assigned ratings to valid and test are already replaced with 0 }
\PY{n}{train\PYZus{}R}\PY{p}{,} \PY{n}{user\PYZus{}means} \PY{o}{=} \PY{n}{centralization}\PY{p}{(}\PY{n}{train\PYZus{}matrix}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{info}\PY{p}{(}\PY{n}{train\PYZus{}R}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
3529
2072
<class 'numpy.matrix'>

(3529, 1)
(<class 'scipy.sparse.\_csr.csr\_matrix'>, (3529, 2072))
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} val\PYZus{}idx is tuple, shape [n\PYZus{}val, 2]}
\PY{c+c1}{\PYZsh{} [n\PYZus{}val, 0]: index of the corresponding user\PYZus{}index u}
\PY{c+c1}{\PYZsh{} [n\PYZus{}val, 1]: index of the corresponding item\PYZus{}index i}

\PY{c+c1}{\PYZsh{} convert val\PYZus{}idx(tuple) to array}
\PY{c+c1}{\PYZsh{} get the list of index of user\PYZus{}index u}
\PY{n}{\PYZus{}a} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{val\PYZus{}idx}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}
\PY{c+c1}{\PYZsh{} print(\PYZus{}a)}
\PY{c+c1}{\PYZsh{} obtain the corresponding user\PYZus{}means with list of indices}
\PY{c+c1}{\PYZsh{} convert the matrix to the 1D array by using np.ravel}
\PY{n}{a} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{p}{[}\PY{n}{\PYZus{}a}\PY{p}{]}\PY{p}{)}
\PY{c+c1}{\PYZsh{} print(type(a))}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} centralize val data set.}
\PY{c+c1}{\PYZsh{} In this case, val\PYZus{}values is np.ndarray}
\PY{c+c1}{\PYZsh{} We need to use 1D array of corresponding user means, subtract from the val\PYZus{}values}
\PY{c+c1}{\PYZsh{} create matrix from the values and index}

\PY{n}{val\PYZus{}values} \PY{o}{=} \PY{n}{val\PYZus{}values} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{val\PYZus{}idx}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{val\PYZus{}R} \PY{o}{=} \PY{n}{sp}\PY{o}{.}\PY{n}{coo\PYZus{}matrix}\PY{p}{(}\PY{p}{(}\PY{n}{val\PYZus{}values}\PY{p}{,} \PY{p}{(}\PY{n}{val\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{n\PYZus{}items}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{info}\PY{p}{(}\PY{n}{val\PYZus{}R}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(<class 'scipy.sparse.\_csr.csr\_matrix'>, (3529, 2072))
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} centralize test data set.}
\PY{c+c1}{\PYZsh{} In this case, val\PYZus{}values is np.ndarray}
\PY{c+c1}{\PYZsh{} We need to use 1D array of corresponding user means, subtract from the test\PYZus{}values}
\PY{c+c1}{\PYZsh{} create matrix from the values and index}

\PY{n}{test\PYZus{}values} \PY{o}{=} \PY{n}{test\PYZus{}values} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{test\PYZus{}idx}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{test\PYZus{}R} \PY{o}{=} \PY{n}{sp}\PY{o}{.}\PY{n}{coo\PYZus{}matrix}\PY{p}{(}\PY{p}{(}\PY{n}{test\PYZus{}values}\PY{p}{,} \PY{p}{(}\PY{n}{test\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{n\PYZus{}items}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{info}\PY{p}{(}\PY{n}{test\PYZus{}R}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(<class 'scipy.sparse.\_csr.csr\_matrix'>, (3529, 2072))
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{train\PYZus{}R:    scipy.sparse.\PYZus{}csr.csr\PYZus{}matrix, shape (337867, 5899)}
\PY{l+s+sd}{            Set valid and test entries to zero, only leave out train entries}

\PY{l+s+sd}{val\PYZus{}idx:    tuple, shape [n\PYZus{}val, 2]}
\PY{l+s+sd}{            u\PYZus{}index\PYZus{}list}
\PY{l+s+sd}{            i\PYZus{}index\PYZus{}list}

\PY{l+s+sd}{val\PYZus{}values: numpy.array, shape [n\PYZus{}val, ]}
\PY{l+s+sd}{            corresponding values to the indices in val\PYZus{}idx}

\PY{l+s+sd}{test\PYZus{}idx:   tuple, shape [n\PYZus{}test, 2]}
\PY{l+s+sd}{            corresponding values to the indices in test\PYZus{}idx}


\PY{l+s+sd}{val\PYZus{}R:      scipy.sparse.\PYZus{}csr.csr\PYZus{}matrix, shape (337867, 5899)}
\PY{l+s+sd}{            selected data as validation set are having values, others are set to zero}

\PY{l+s+sd}{test\PYZus{}R:     scipy.sparse.\PYZus{}csr.csr\PYZus{}matrix, shape (337867, 5899)}
\PY{l+s+sd}{            selected data as test set are having values, others are set to zero}

\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{n}{n\PYZus{}val} \PY{o}{=} \PY{l+m+mi}{200}
\PY{n}{n\PYZus{}test} \PY{o}{=} \PY{l+m+mi}{200}
\PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{n\PYZus{}items} \PY{o}{=} \PY{n}{cent\PYZus{}R}\PY{o}{.}\PY{n}{shape}
\PY{n}{train\PYZus{}matrix}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{p}{,} \PY{n}{val\PYZus{}values}\PY{p}{,} \PY{n}{test\PYZus{}values} \PY{o}{=} \PY{n}{split\PYZus{}data}\PY{p}{(}\PY{n}{R\PYZus{}prune}\PY{p}{,} \PY{n}{n\PYZus{}val}\PY{p}{,} \PY{n}{n\PYZus{}test}\PY{p}{)}
\PY{n}{train\PYZus{}R}\PY{p}{,} \PY{n}{user\PYZus{}means} \PY{o}{=} \PY{n}{centralization}\PY{p}{(}\PY{n}{train\PYZus{}matrix}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{info}\PY{p}{(}\PY{n}{train\PYZus{}R}\PY{p}{)}\PY{p}{)}
\PY{n}{val\PYZus{}values} \PY{o}{=} \PY{n}{val\PYZus{}values} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{val\PYZus{}idx}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{val\PYZus{}R} \PY{o}{=} \PY{n}{sp}\PY{o}{.}\PY{n}{coo\PYZus{}matrix}\PY{p}{(}\PY{p}{(}\PY{n}{val\PYZus{}values}\PY{p}{,} \PY{p}{(}\PY{n}{val\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{n\PYZus{}items}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{info}\PY{p}{(}\PY{n}{val\PYZus{}R}\PY{p}{)}\PY{p}{)}
\PY{n}{test\PYZus{}values} \PY{o}{=} \PY{n}{test\PYZus{}values} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{n}{user\PYZus{}means}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{test\PYZus{}idx}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{test\PYZus{}R} \PY{o}{=} \PY{n}{sp}\PY{o}{.}\PY{n}{coo\PYZus{}matrix}\PY{p}{(}\PY{p}{(}\PY{n}{test\PYZus{}values}\PY{p}{,} \PY{p}{(}\PY{n}{test\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{n\PYZus{}items}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{info}\PY{p}{(}\PY{n}{test\PYZus{}R}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
3529
2072
<class 'numpy.matrix'>

(3529, 1)
(<class 'scipy.sparse.\_csr.csr\_matrix'>, (3529, 2072))
(<class 'scipy.sparse.\_csr.csr\_matrix'>, (337867, 5899))
(<class 'scipy.sparse.\_csr.csr\_matrix'>, (337867, 5899))
    \end{Verbatim}

    \hypertarget{initialization-of-q-and-p}{%
\section{Initialization of Q and P}\label{initialization-of-q-and-p}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} option1 : random initialization}
\PY{n}{k} \PY{o}{=} \PY{l+m+mi}{100}
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Q} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{k}\PY{p}{)}
\PY{n}{P} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{n\PYZus{}items}\PY{p}{)}

\PY{c+c1}{\PYZsh{} option2 : initialize with SVD}
\PY{n}{U}\PY{p}{,} \PY{n}{s}\PY{p}{,} \PY{n}{V} \PY{o}{=} \PY{n}{svds}\PY{p}{(}\PY{n}{train\PYZus{}R}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{)}
\PY{n}{S} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{s}\PY{p}{)}
\PY{n}{Q} \PY{o}{=} \PY{n}{U}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{S}\PY{p}{)}
\PY{n}{P} \PY{o}{=} \PY{n}{V}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{Q}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{P}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(3529, 100)
(100, 2072)
    \end{Verbatim}

    \hypertarget{compute-loss}{%
\section{Compute loss}\label{compute-loss}}

\[
\mathcal{L} = \min_{P, Q} {\sum_{(u, i) \in S}} (R_{ui} - \mathbf{q}_u\mathbf{p}_i^T)^2 + \lambda [\sum_i{\left\lVert \mathbf{p}_i \right\rVert}^2 + \sum_u{\left\lVert \mathbf{q}_u \right\rVert}^2]
\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} sum of squared error (Ridge loss, using L2 loss)}
\PY{c+c1}{\PYZsh{} We must obtain nnz\PYZus{}index from not\PYZhy{}centralized matrix if we are using np.argwhere}
\PY{c+c1}{\PYZsh{} otherwise, we will get rid of zero value as rating 0 but they were actually equal to user\PYZhy{}mean.}
\PY{n}{nnz\PYZus{}index} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argwhere}\PY{p}{(}\PY{n}{train\PYZus{}matrix}\PY{p}{)}
\PY{n}{train\PYZus{}idx} \PY{o}{=} \PY{n+nb}{tuple}\PY{p}{(}\PY{n}{nnz\PYZus{}index}\PY{o}{.}\PY{n}{T}\PY{p}{)}
\PY{n}{train\PYZus{}values} \PY{o}{=} \PY{n}{train\PYZus{}R}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{A1}
\PY{n}{sse\PYZus{}loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}values}\PY{o}{\PYZhy{}}\PY{n}{Q}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{P}\PY{p}{)}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{redge\PYZus{}lambda} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}4}
\PY{n}{regulalization\PYZus{}loss} \PY{o}{=} \PY{n}{redge\PYZus{}lambda} \PY{o}{*} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{P}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{Q}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{optimization-learning}{%
\section{Optimization (Learning)}\label{optimization-learning}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{60}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{nnz\PYZus{}index}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{nnz\PYZus{}index}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{nnz\PYZus{}index}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'numpy.ndarray'>
[   0    0    0 {\ldots} 3528 3528 3528] [   3   24  219 {\ldots} 1945 1954 1998]
    \end{Verbatim}

    \hypertarget{create-stencil-buffer-masking-index-pairs-having-non-zero-value-in-original-matrix}{%
\subsubsection{Create stencil buffer masking index pairs having non-zero
value in original
matrix}\label{create-stencil-buffer-masking-index-pairs-having-non-zero-value-in-original-matrix}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} create the stencil buffer such that mask index pair which are having argue in original matrix}
\PY{c+c1}{\PYZsh{} ones compressed sparse rows matrix such that only non\PYZhy{}zero indices have 1.}
\PY{n}{nnz\PYZus{}mask} \PY{o}{=} \PY{n}{sp}\PY{o}{.}\PY{n}{coo\PYZus{}matrix}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nnz\PYZus{}index}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{nnz\PYZus{}index}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{nnz\PYZus{}index}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{shape}\PY{o}{=}\PY{n}{R}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{dtype} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uint8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{nnz\PYZus{}mask}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[0 0 0 {\ldots} 0 0 0]
 [0 0 0 {\ldots} 0 0 0]
 [0 0 0 {\ldots} 0 0 0]
 {\ldots}
 [0 0 0 {\ldots} 0 0 0]
 [0 0 0 {\ldots} 0 0 0]
 [0 0 0 {\ldots} 0 0 0]]
    \end{Verbatim}

    \hypertarget{create-version-of-mask-sorted-in-columns-compressed-sparse-column}{%
\subsubsection{Create version of mask sorted in columns (compressed
sparse
column)}\label{create-version-of-mask-sorted-in-columns-compressed-sparse-column}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{nnz\PYZus{}mask\PYZus{}col} \PY{o}{=} \PY{n}{nnz\PYZus{}mask}\PY{o}{.}\PY{n}{tocsc}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{nnz\PYZus{}mask\PYZus{}col}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{==} \PY{n}{nnz\PYZus{}mask}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
  (0, 0)        True
  (0, 1)        True
  (0, 2)        True
  (0, 3)        True
  (0, 4)        True
  (0, 5)        True
  (0, 6)        True
  (0, 7)        True
  (0, 8)        True
  (0, 9)        True
  (0, 10)       True
  (0, 11)       True
  (0, 12)       True
  (0, 13)       True
  (0, 14)       True
  (0, 15)       True
  (0, 16)       True
  (0, 17)       True
  (0, 18)       True
  (0, 19)       True
  (0, 20)       True
  (0, 21)       True
  (0, 22)       True
  (0, 23)       True
  (0, 24)       True
  :     :
  (0, 5874)     True
  (0, 5875)     True
  (0, 5876)     True
  (0, 5877)     True
  (0, 5878)     True
  (0, 5879)     True
  (0, 5880)     True
  (0, 5881)     True
  (0, 5882)     True
  (0, 5883)     True
  (0, 5884)     True
  (0, 5885)     True
  (0, 5886)     True
  (0, 5887)     True
  (0, 5888)     True
  (0, 5889)     True
  (0, 5890)     True
  (0, 5891)     True
  (0, 5892)     True
  (0, 5893)     True
  (0, 5894)     True
  (0, 5895)     True
  (0, 5896)     True
  (0, 5897)     True
  (0, 5898)     True
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/ryotok/anaconda3/envs/dlml/lib/python3.10/site-
packages/IPython/core/interactiveshell.py:3369: SparseEfficiencyWarning:
Comparing sparse matrices using == is inefficient, try using != instead.
  exec(code\_obj, self.user\_global\_ns, self.user\_ns)
    \end{Verbatim}

    \hypertarget{create-lists-of-column-and-row}{%
\subsubsection{Create lists of column and
row}\label{create-lists-of-column-and-row}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} cols: numpy.ndarray}

\PY{n}{n\PYZus{}row}\PY{p}{,} \PY{n}{n\PYZus{}col} \PY{o}{=} \PY{n}{train\PYZus{}R}\PY{o}{.}\PY{n}{shape}

\PY{n}{cols} \PY{o}{=} \PY{n}{nnz\PYZus{}mask}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{tolil}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{rows}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{cols}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{cols}\PY{p}{)}

\PY{n}{rows} \PY{o}{=} \PY{n}{nnz\PYZus{}mask}\PY{o}{.}\PY{n}{tolil}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{rows}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{rows}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'numpy.ndarray'>
[list([78, 126, 436, 607, 684, 693, 710, 1078, 1102, 1134, 1303, 1445, 1451,
1656, 1721, 1979, 2085, 2165, 2316, 2378, 2420, 2530, 2545, 2800, 2961, 3072,
3134, 3198, 3366, 3394, 3397, 3496, 3497])
 list([141, 177, 191, 192, 214, 235, 266, 300, 359, 362, 412, 508, 515, 601,
606, 623, 630, 642, 743, 801, 855, 871, 884, 905, 1158, 1176, 1196, 1259, 1505,
1609, 1667, 1679, 1699, 1751, 1752, 1857, 1901, 1909, 1936, 2016, 2055, 2089,
2103, 2150, 2165, 2217, 2237, 2329, 2380, 2388, 2408, 2420, 2445, 2457, 2524,
2598, 2603, 2607, 2682, 2714, 2750, 2772, 2798, 2898, 2919, 2920, 2946, 3057,
3130, 3137, 3240, 3298, 3314, 3320, 3353, 3358, 3397, 3404, 3522])
 list([25, 54, 55, 136, 147, 226, 298, 325, 368, 418, 453, 506, 534, 551, 574,
590, 622, 680, 723, 740, 741, 767, 784, 795, 868, 962, 964, 967, 1023, 1133,
1136, 1153, 1164, 1190, 1234, 1236, 1278, 1352, 1354, 1355, 1356, 1361, 1473,
1563, 1612, 1628, 1633, 1669, 1692, 1785, 1807, 1824, 1861, 1920, 1950, 2043,
2068, 2125, 2140, 2243, 2311, 2314, 2323, 2383, 2417, 2430, 2475, 2503, 2513,
2559, 2606, 2624, 2651, 2704, 2795, 2831, 2838, 2848, 2889, 2926, 3178, 3203,
3302, 3358, 3451, 3476, 3499, 3500])
 {\ldots} list([]) list([]) list([])]
<class 'numpy.ndarray'>
    \end{Verbatim}

    \hypertarget{create-module-for-ridge-regression-loss}{%
\subsubsection{Create module for ridge regression
loss}\label{create-module-for-ridge-regression-loss}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{reg\PYZus{}lambda} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}4}
\PY{c+c1}{\PYZsh{} Fit intercept = False means that setting y\PYZus{}intercept to 0.}
\PY{c+c1}{\PYZsh{} This should be False if we already centralize the data in advance }

\PY{n}{reg} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha} \PY{o}{=} \PY{n}{reg\PYZus{}lambda}\PY{p}{,} \PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{initializing-q-and-p}{%
\subsubsection{Initializing Q and P}\label{initializing-q-and-p}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{65}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{init} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{svd}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{Q}\PY{p}{,} \PY{n}{P} \PY{o}{=} \PY{n}{initialize\PYZus{}Q\PYZus{}P}\PY{p}{(}\PY{n}{train\PYZus{}R}\PY{p}{,} \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{100}\PY{p}{,} \PY{n}{init} \PY{o}{=} \PY{n}{init}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{optimization-procedure}{%
\subsubsection{Optimization procedure}\label{optimization-procedure}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{66}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{max\PYZus{}steps} \PY{o}{=} \PY{l+m+mi}{10}
\PY{n}{eval\PYZus{}every} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{patience} \PY{o}{=} \PY{l+m+mi}{5}
\PY{n}{optimizer} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alt}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}2}
\PY{n}{reg\PYZus{}lambda} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{converged\PYZus{}after} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{67}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{train\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{valid\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}

\PY{n}{best\PYZus{}val\PYZus{}loss} \PY{o}{=} \PY{n}{best\PYZus{}Q} \PY{o}{=} \PY{n}{best\PYZus{}P} \PY{o}{=} \PY{n}{converged\PYZus{}after} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}

\PY{n}{bef} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} average time to execute one iteration}
\PY{n}{times} \PY{o}{=} \PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{} time stamp lists}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{68}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{i}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0
1
2
3
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{69}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{for} \PY{n}{user\PYZus{}idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{train\PYZus{}R}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
    \PY{n}{\PYZus{}rating\PYZus{}idx} \PY{o}{=} \PY{n}{rows}\PY{p}{[}\PY{n}{user\PYZus{}idx}\PY{p}{]}
    \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{\PYZus{}rating\PYZus{}idx}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{\PYZus{}rating\PYZus{}idx}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{70}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{for} \PY{n}{it} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}steps}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} If it is not the first iteration,}
    \PY{k}{if} \PY{n}{bef} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
        \PY{n}{times}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{bef}\PY{p}{)}
    \PY{n}{bef} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}

    
    \PY{c+c1}{\PYZsh{} Evaluation}
    \PY{c+c1}{\PYZsh{} Matrix factorization model evaluation}
    \PY{c+c1}{\PYZsh{} only execute every \PYZsq{}eval\PYZus{}every\PYZsq{},}
    \PY{k}{if} \PY{n}{it} \PY{o}{\PYZpc{}} \PY{n}{eval\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} evaluate model on train\PYZus{}R}
        \PY{c+c1}{\PYZsh{} calculate train loss}
        \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{n}{loss}\PY{p}{(}\PY{n}{train\PYZus{}R}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{A1}\PY{p}{,} \PY{n}{train\PYZus{}idx}\PY{p}{,} \PY{n}{Q}\PY{p}{,} \PY{n}{P}\PY{p}{,} \PY{n}{reg\PYZus{}lambda}\PY{p}{)}
        \PY{n}{train\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{train\PYZus{}loss}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} evaluate model on val\PYZus{}R}
        \PY{c+c1}{\PYZsh{} calculate val loss}
        \PY{n}{val\PYZus{}loss} \PY{o}{=} \PY{n}{loss}\PY{p}{(}\PY{n}{val\PYZus{}R}\PY{p}{[}\PY{n}{val\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{A1}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{Q}\PY{p}{,} \PY{n}{P}\PY{p}{,} \PY{n}{reg\PYZus{}lambda}\PY{p}{)}
        \PY{n}{valid\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val\PYZus{}loss}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} update the Q and P and the minimum loss so far}
        \PY{k}{if} \PY{n}{best\PYZus{}val\PYZus{}loss} \PY{o}{\PYZlt{}} \PY{l+m+mi}{0} \PY{o+ow}{or} \PY{n}{val\PYZus{}loss} \PY{o}{\PYZlt{}} \PY{n}{best\PYZus{}val\PYZus{}loss}\PY{p}{:}
            \PY{n}{best\PYZus{}val\PYZus{}loss} \PY{o}{=} \PY{n}{val\PYZus{}loss}
            \PY{n}{best\PYZus{}Q} \PY{o}{=} \PY{n}{Q}
            \PY{n}{best\PYZus{}P} \PY{o}{=} \PY{n}{P}
            \PY{c+c1}{\PYZsh{} where there is improvement, let \PYZsq{}patience\PYZsq{} remain original number}
            \PY{n}{current\PYZus{}patience} \PY{o}{=} \PY{n}{patience}
        \PY{k}{else}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} where there is no improvement, decrement the counter of patience}
            \PY{n}{current\PYZus{}patience} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{l+m+mi}{1}
        
        \PY{c+c1}{\PYZsh{} if there are no improvement in \PYZsq{}patience\PYZsq{}, we stop this evaluation}
        \PY{k}{if} \PY{n}{current\PYZus{}patience} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} report the number of iteration (steps) }
            \PY{n}{converged\PYZus{}after} \PY{o}{=} \PY{n}{it} \PY{o}{\PYZhy{}} \PY{n}{patience} \PY{o}{*} \PY{n}{eval\PYZus{}every}
            \PY{k}{break}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iteration }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{, train\PYZus{}loss: }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s2}{, validation\PYZus{}loss }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{it}\PY{p}{,} \PY{n}{train\PYZus{}loss}\PY{p}{,} \PY{n}{val\PYZus{}loss}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Learning}
    \PY{c+c1}{\PYZsh{} Optimization step}

    \PY{c+c1}{\PYZsh{} stochastic gradient descent}
    \PY{k}{if} \PY{n}{optimizer} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sgd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} random sample index of indices pair (u, i) from the tuple of non\PYZhy{}zero value indices of train\PYZus{}R}
        \PY{n}{sgd\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} shuffle the indices}
        \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{sgd\PYZus{}indices}\PY{p}{)}

        \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n}{sgd\PYZus{}indices}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} obtain u and i pair from the tuple by using random sampled index of them}
            \PY{n}{u}\PY{p}{,} \PY{n}{i} \PY{o}{=} \PY{n}{train\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{,} \PY{n}{train\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{idx}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} predict the value of R\PYZus{}ui by using factorized matrix Q and P}
            \PY{n}{prediction} \PY{o}{=} \PY{n}{Q}\PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} calculate error of prediction}
            \PY{c+c1}{\PYZsh{} this e is a helper to get the gradients}
            \PY{n}{e} \PY{o}{=} \PY{p}{(}\PY{n}{R}\PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{n}{prediction}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Update latent factors}
            \PY{n}{Q}\PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{lr} \PY{o}{*} \PY{p}{(}\PY{n}{e} \PY{o}{*} \PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{reg\PYZus{}lambda} \PY{o}{*} \PY{n}{Q}\PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}
            \PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{lr} \PY{o}{*} \PY{p}{(}\PY{n}{e} \PY{o}{*} \PY{n}{Q}\PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{reg\PYZus{}lambda} \PY{o}{*} \PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} alternating optimization}
    \PY{k}{elif} \PY{n}{optimizer} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} fix Q and update P}
        \PY{k}{for} \PY{n}{rating\PYZus{}idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{train\PYZus{}R}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Obtain user u lists in rating\PYZus{}idx which are having non\PYZhy{}zero value }
            \PY{c+c1}{\PYZsh{} e.g nnz\PYZus{}index = [179783, 195038, 234835, 282041, 303144, 321814]}
            \PY{c+c1}{\PYZsh{} and [179783, rating\PYZus{}idx], [195038, rating\PYZus{}idx],....}
            \PY{n}{\PYZus{}user\PYZus{}idx} \PY{o}{=} \PY{n}{cols}\PY{p}{[}\PY{n}{rating\PYZus{}idx}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} X   \PYZhy{}\PYZgt{} Q[\PYZus{}user\PYZus{}idx]}
            \PY{c+c1}{\PYZsh{} W.T \PYZhy{}\PYZgt{} P[\PYZus{}user\PYZus{}idx] (weights) ==\PYZgt{} .coef\PYZus{}\PYZus{}}
            \PY{c+c1}{\PYZsh{} y   \PYZhy{}\PYZgt{} np.squeeze(train\PYZus{}R[\PYZus{}user\PYZus{}idx, rating\PYZus{}idx])}
            \PY{n}{res} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Q}\PY{p}{[}\PY{n}{\PYZus{}user\PYZus{}idx}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{train\PYZus{}R}\PY{p}{[}\PY{n}{\PYZus{}user\PYZus{}idx}\PY{p}{,} \PY{n}{rating\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
            \PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{rating\PYZus{}idx}\PY{p}{]} \PY{o}{=} \PY{n}{res}\PY{o}{.}\PY{n}{coef\PYZus{}}
        
        \PY{c+c1}{\PYZsh{} fix P and update Q}
        \PY{k}{for} \PY{n}{user\PYZus{}idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{train\PYZus{}R}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Obtain item i lists in user\PYZus{}idx which are having non\PYZhy{}zero value}
            \PY{c+c1}{\PYZsh{} e,g, [100. 200, ,,, 300]}
            \PY{c+c1}{\PYZsh{} e.g, [user\PYZus{}idx, 100], [user\PYZus{}idx, 200], ...}
            \PY{n}{\PYZus{}rating\PYZus{}idx} \PY{o}{=} \PY{n}{rows}\PY{p}{[}\PY{n}{user\PYZus{}idx}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} X  \PYZhy{}\PYZgt{} P[:, \PYZus{}rating\PYZus{}idx].T}
            \PY{c+c1}{\PYZsh{} W  \PYZhy{}\PYZgt{} Q[user\PYZus{}idx]}
            \PY{c+c1}{\PYZsh{} Y  \PYZhy{}\PYZgt{} np.squeeze(train\PYZus{}R[user\PYZus{}idx, \PYZus{}rating\PYZus{}idx])}
            \PY{n}{res} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{\PYZus{}rating\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{train\PYZus{}R}\PY{p}{[}\PY{n}{user\PYZus{}idx}\PY{p}{,} \PY{n}{\PYZus{}rating\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
            \PY{n}{Q}\PY{p}{[}\PY{n}{user\PYZus{}idx}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{res}\PY{o}{.}\PY{n}{coef\PYZus{}}

\PY{k}{if} \PY{n}{max\PYZus{}steps} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1} \PY{o}{!=} \PY{n}{it}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Converged after }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ iterations, on average }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s2}{s per iteration}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{converged\PYZus{}after}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{times}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ iterations, not converged, on average }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s2}{s per iteration}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{it}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{times}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration 0, train\_loss: 150018.293, validation\_loss 43252.095
Iteration 1, train\_loss: 124524.421, validation\_loss 127707.732
Iteration 2, train\_loss: 98155.727, validation\_loss 99579.143
Iteration 3, train\_loss: 93081.593, validation\_loss 94355.281
Iteration 4, train\_loss: 90200.576, validation\_loss 91279.658
Converged after 0 iterations, on average 7.222s per iteration
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{71}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{latent\PYZus{}factor\PYZus{}alternating\PYZus{}optimization}\PY{p}{(}\PY{n}{R}\PY{p}{,} \PY{n}{non\PYZus{}zero\PYZus{}idx}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{val\PYZus{}values}\PY{p}{,} \PY{n}{reg\PYZus{}lambda}\PY{p}{,} \PY{n}{max\PYZus{}steps} \PY{o}{=} \PY{l+m+mi}{100}\PY{p}{,} \PY{n}{init} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{log\PYZus{}every} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{patience} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{eval\PYZus{}every} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{optimizer} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sgd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}2}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Perform matrix factorization using alternating optimization. }
\PY{l+s+sd}{    Training is done via patience.}
\PY{l+s+sd}{    i.e. we stop training after we observe no improvement }
\PY{l+s+sd}{    on the validation loss for a certain}
\PY{l+s+sd}{    amount of training steps. We then return the best values }
\PY{l+s+sd}{    for Q and P observed during training.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        R:              sp.spmatrix, shape [N, D]}
\PY{l+s+sd}{                        The input matrix to be factorized (train\PYZus{}matrix)}
\PY{l+s+sd}{                        It has to be centralized by mean.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        non\PYZus{}zero\PYZus{}idx:   np.array, shape [nnz, 2]}
\PY{l+s+sd}{                        The indices of the non\PYZhy{}zero entries of the **un\PYZhy{}shifted** matrix to be factorized.}
\PY{l+s+sd}{                        nnz refers to the number of non\PYZhy{}zero entries. Note that this may be different }
\PY{l+s+sd}{                        from the number of non\PYZhy{}zero entries in the input matrix(training matrix) since this indices refers}
\PY{l+s+sd}{                        original data matrix}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        k:              int}
\PY{l+s+sd}{                        The latent factor dimension}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        val\PYZus{}idx:        tuple, shape[2, n\PYZus{}val]}
\PY{l+s+sd}{                        [u1, u2, ,,,,, u\PYZus{}n\PYZus{}val]}
\PY{l+s+sd}{                        [i1, i2, ,,,,, i\PYZus{}n\PYZus{}val]}
\PY{l+s+sd}{                        Tuple pf the validation set indices}
\PY{l+s+sd}{                        n\PYZus{}val refers to the size of the validation set}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        val\PYZus{}values:     np.array, shape [n\PYZus{}val, ]}
\PY{l+s+sd}{                        The values in the validation set}

\PY{l+s+sd}{        reg\PYZus{}lambda:     float}
\PY{l+s+sd}{                        The regularization strength}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        max\PYZus{}steps:      int, optional, default = 100}
\PY{l+s+sd}{                        Maximum number of training interactions (steps, 1 steps, one optimization of two matrix factor Q and P),}
\PY{l+s+sd}{                        Note that we will step early if we observe}
\PY{l+s+sd}{                        no improvement on the validation with the step to be patient}

\PY{l+s+sd}{        init:           str in [\PYZsq{}random\PYZsq{}, \PYZsq{}svd\PYZsq{}], default \PYZsq{}random\PYZsq{}}
\PY{l+s+sd}{                        The initialization strategy for P and Q.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        log\PYZus{}every:      int, optional, default: 1}
\PY{l+s+sd}{                        Log the training status every X iterations}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        patience:       int, optional, default: 5}
\PY{l+s+sd}{                        Stop training after we observe no improvement of the valid loss for X evaluation}
\PY{l+s+sd}{                        iterations. After we stop training, we restore the best observed values for Q and P}

\PY{l+s+sd}{        eval\PYZus{}every:     int, optional, default: 1}
\PY{l+s+sd}{                        Evaluate the training and validation loss every x steps}
\PY{l+s+sd}{                        If we observe no improvement of the validation error, we decrease out patience by 1, else we reset it to *patience*}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        optimizer:      str in [\PYZsq{}sgd\PYZsq{}, \PYZsq{}alt\PYZsq{}], optional, default: \PYZsq{}sgd\PYZsq{}}
\PY{l+s+sd}{                        If \PYZsq{}sgd; stochastic gradient descent shall be used, otherwise, use alternating least squares.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{    Returns:}

\PY{l+s+sd}{        best\PYZus{}Q:             np.array, shape [N, k]}
\PY{l+s+sd}{                            Best value for Q (based on the validation loss) observed during training}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        best\PYZus{}P:             np.array, shape[k, D]}
\PY{l+s+sd}{                            Best value for P (based on validation loss) observed during training}

\PY{l+s+sd}{        validation\PYZus{}losses:  list of floats}
\PY{l+s+sd}{                            Validation loss for every evaluation iteration, can be used for plotting the validation loss}
\PY{l+s+sd}{                            over time}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        train\PYZus{}losses:       list of floats}
\PY{l+s+sd}{                            Training loss for every evaluation iteration, can be used for plotting the training loss over time}

\PY{l+s+sd}{        converged\PYZus{}after:    int}
\PY{l+s+sd}{                            it \PYZhy{} patience * eval\PYZus{}every, where it is the iteration in which patience hits 0,}
\PY{l+s+sd}{                            or \PYZhy{}1 if we hit max\PYZus{}steps before converging.}

\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{n}{nnz\PYZus{}mask} \PY{o}{=} \PY{n}{sp}\PY{o}{.}\PY{n}{coo\PYZus{}matrix}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{non\PYZus{}zero\PYZus{}idx}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{non\PYZus{}zero\PYZus{}idx}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{non\PYZus{}zero\PYZus{}idx}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{shape}\PY{o}{=}\PY{n}{R}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{dtype} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{uint8}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}

    \PY{n}{nnz\PYZus{}mask\PYZus{}col} \PY{o}{=} \PY{n}{nnz\PYZus{}mask}\PY{o}{.}\PY{n}{tocsc}\PY{p}{(}\PY{p}{)}

    \PY{n}{cols} \PY{o}{=} \PY{n}{nnz\PYZus{}mask}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{tolil}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{rows}
    \PY{n}{rows} \PY{o}{=} \PY{n}{nnz\PYZus{}mask}\PY{o}{.}\PY{n}{tolil}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{rows}

    \PY{n}{reg}  \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{reg\PYZus{}lambda}\PY{p}{,} \PY{n}{fit\PYZus{}intercept} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}

    \PY{n}{Q}\PY{p}{,} \PY{n}{P} \PY{o}{=} \PY{n}{initialize\PYZus{}Q\PYZus{}P}\PY{p}{(}\PY{n}{R}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{init}\PY{p}{)}
    \PY{n}{train\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{validation\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}

    \PY{n}{best\PYZus{}val\PYZus{}loss} \PY{o}{=} \PY{n}{best\PYZus{}Q} \PY{o}{=} \PY{n}{best\PYZus{}P} \PY{o}{=} \PY{n}{converged\PYZus{}after} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}

    \PY{n}{train\PYZus{}idx} \PY{o}{=} \PY{n+nb}{tuple}\PY{p}{(}\PY{n}{non\PYZus{}zero\PYZus{}idx}\PY{o}{.}\PY{n}{T}\PY{p}{)}

    \PY{n}{bef} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
    \PY{n}{times} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{it} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}steps}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{n}{bef} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
            \PY{n}{times}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{bef}\PY{p}{)}
        \PY{n}{bef} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}

        \PY{k}{if} \PY{n}{it} \PY{o}{\PYZpc{}} \PY{n}{eval\PYZus{}every} \PY{o}{==}\PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{n}{loss}\PY{p}{(}\PY{n}{R}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{A1}\PY{p}{,} \PY{n}{train\PYZus{}idx}\PY{p}{,} \PY{n}{Q}\PY{p}{,} \PY{n}{P}\PY{p}{,} \PY{n}{reg\PYZus{}lambda}\PY{p}{)}
            \PY{n}{train\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{train\PYZus{}loss}\PY{p}{)}

            \PY{n}{val\PYZus{}loss} \PY{o}{=} \PY{n}{loss}\PY{p}{(}\PY{n}{val\PYZus{}values}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{Q}\PY{p}{,} \PY{n}{P}\PY{p}{,} \PY{n}{reg\PYZus{}lambda}\PY{p}{)}
            \PY{n}{validation\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val\PYZus{}loss}\PY{p}{)}

            \PY{k}{if} \PY{n}{best\PYZus{}val\PYZus{}loss} \PY{o}{\PYZlt{}} \PY{l+m+mi}{0} \PY{o+ow}{or} \PY{n}{val\PYZus{}loss} \PY{o}{\PYZlt{}} \PY{n}{best\PYZus{}val\PYZus{}loss}\PY{p}{:}
                \PY{n}{best\PYZus{}val\PYZus{}loss} \PY{o}{=} \PY{n}{val\PYZus{}loss}
                \PY{n}{best\PYZus{}Q} \PY{o}{=} \PY{n}{Q}
                \PY{n}{best\PYZus{}P} \PY{o}{=} \PY{n}{P}
                \PY{n}{current\PYZus{}patience} \PY{o}{=} \PY{n}{patience}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{current\PYZus{}patience} \PY{o}{\PYZhy{}}\PY{o}{=}\PY{l+m+mi}{1}

            \PY{k}{if} \PY{n}{current\PYZus{}patience} \PY{o}{==}\PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{converged\PYZus{}after} \PY{o}{=} \PY{n}{it} \PY{o}{\PYZhy{}} \PY{n}{patience} \PY{o}{*} \PY{n}{eval\PYZus{}every}
                \PY{k}{break}

        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iteration }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{, training loss: }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s2}{, validation loss: }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{it}\PY{p}{,} \PY{n}{train\PYZus{}loss}\PY{p}{,} \PY{n}{val\PYZus{}loss}\PY{p}{)}\PY{p}{)}

        \PY{k}{if} \PY{n}{optimizer} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sgd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
            \PY{n}{sgd\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
            \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{sgd\PYZus{}indices}\PY{p}{)}

            \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n}{sgd\PYZus{}indices}\PY{p}{:}
                \PY{n}{u}\PY{p}{,} \PY{n}{i} \PY{o}{=} \PY{n}{train\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{,} \PY{n}{train\PYZus{}idx}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{idx}\PY{p}{]}
                \PY{n}{prediction}\PY{o}{=}\PY{n}{Q}\PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{)}
                \PY{n}{e} \PY{o}{=} \PY{p}{(}\PY{n}{R}\PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{prediction}\PY{p}{)} \PY{c+c1}{\PYZsh{} error}

                \PY{c+c1}{\PYZsh{} Update latent factors}
                \PY{n}{Q}\PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{lr} \PY{o}{*} \PY{p}{(}\PY{n}{e} \PY{o}{*} \PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{n}{reg\PYZus{}lambda} \PY{o}{*} \PY{n}{Q}\PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}
                \PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{lr} \PY{o}{*} \PY{p}{(}\PY{n}{e} \PY{o}{*} \PY{n}{Q}\PY{p}{[}\PY{n}{u}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{n}{reg\PYZus{}lambda} \PY{o}{*} \PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{)}

        \PY{k}{elif} \PY{n}{optimizer} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{als}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} fix Q and update P}
            \PY{k}{for} \PY{n}{rating\PYZus{}idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{R}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                \PY{n}{nnz\PYZus{}idx} \PY{o}{=} \PY{n}{cols}\PY{p}{[}\PY{n}{rating\PYZus{}idx}\PY{p}{]}
                \PY{n}{res} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Q}\PY{p}{[}\PY{n}{nnz\PYZus{}idx}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{R}\PY{p}{[}\PY{n}{nnz\PYZus{}idx}\PY{p}{,} \PY{n}{rating\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                \PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{rating\PYZus{}idx}\PY{p}{]} \PY{o}{=} \PY{n}{res}\PY{o}{.}\PY{n}{coef\PYZus{}}

            \PY{k}{for} \PY{n}{user\PYZus{}idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{R}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{n}{nnz\PYZus{}idx} \PY{o}{=} \PY{n}{rows}\PY{p}{[}\PY{n}{user\PYZus{}idx}\PY{p}{]}
                 \PY{n}{res} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{P}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{nnz\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{R}\PY{p}{[}\PY{n}{user\PYZus{}idx}\PY{p}{,} \PY{n}{nnz\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                 \PY{n}{Q}\PY{p}{[}\PY{n}{user\PYZus{}idx}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{res}\PY{o}{.}\PY{n}{coef\PYZus{}}
            
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Converged after }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ iteration, ob average }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s2}{s per iteration}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{converged\PYZus{}after}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{times}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{best\PYZus{}Q}\PY{p}{,} \PY{n}{best\PYZus{}P}\PY{p}{,} \PY{n}{validation\PYZus{}losses}\PY{p}{,} \PY{n}{train\PYZus{}losses}\PY{p}{,} \PY{n}{converged\PYZus{}after}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{72}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Q\PYZus{}sgd}\PY{p}{,} \PY{n}{P\PYZus{}sgd}\PY{p}{,} \PY{n}{val\PYZus{}loss\PYZus{}sgd}\PY{p}{,} \PY{n}{train\PYZus{}loss\PYZus{}sgd}\PY{p}{,} \PY{n}{converged\PYZus{}sgd} \PY{o}{=} \PY{n}{latent\PYZus{}factor\PYZus{}alternating\PYZus{}optimization}\PY{p}{(}
    \PY{n}{R\PYZus{}shifted}\PY{p}{,} \PY{n}{nnz\PYZus{}index}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{o}{=}\PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{val\PYZus{}values}\PY{o}{=}\PY{n}{val\PYZus{}values}\PY{p}{,} 
    \PY{n}{reg\PYZus{}lambda}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{,} \PY{n}{init}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sgd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}2}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration 0, training loss: 96807256.162, validation loss: 124961.399
Iteration 1, training loss: 287022.654, validation loss: 480.892
Iteration 2, training loss: 164496.167, validation loss: 418.935
Iteration 3, training loss: 113585.844, validation loss: 397.397
Iteration 4, training loss: 84521.368, validation loss: 403.360
Iteration 5, training loss: 65239.990, validation loss: 394.560
Iteration 6, training loss: 51620.946, validation loss: 408.072
Iteration 7, training loss: 41831.410, validation loss: 410.098
Iteration 8, training loss: 34279.319, validation loss: 420.662
Iteration 9, training loss: 28541.024, validation loss: 423.760
Iteration 10, training loss: 23992.504, validation loss: 436.397
Iteration 11, training loss: 20170.751, validation loss: 439.983
Iteration 12, training loss: 17228.444, validation loss: 441.693
Iteration 13, training loss: 14853.430, validation loss: 448.264
Iteration 14, training loss: 12818.962, validation loss: 455.994
Converged after 5 iteration, ob average 3.504s per iteration
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{73}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Q\PYZus{}als}\PY{p}{,} \PY{n}{P\PYZus{}als}\PY{p}{,} \PY{n}{val\PYZus{}loss\PYZus{}als}\PY{p}{,} \PY{n}{train\PYZus{}loss\PYZus{}als}\PY{p}{,} \PY{n}{converged\PYZus{}als} \PY{o}{=} \PY{n}{latent\PYZus{}factor\PYZus{}alternating\PYZus{}optimization}\PY{p}{(}
    \PY{n}{R\PYZus{}shifted}\PY{p}{,} \PY{n}{nnz\PYZus{}index}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{o}{=}\PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{val\PYZus{}values}\PY{o}{=}\PY{n}{val\PYZus{}values}\PY{p}{,} 
    \PY{n}{reg\PYZus{}lambda}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{,} \PY{n}{init}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{als}\PY{l+s+s1}{\PYZsq{}}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration 0, training loss: 96807256.162, validation loss: 124961.399
Iteration 1, training loss: 2203.621, validation loss: 1569.629
Iteration 2, training loss: 506.646, validation loss: 1634.326
Iteration 3, training loss: 193.021, validation loss: 1416.058
Iteration 4, training loss: 93.799, validation loss: 1046.558
Iteration 5, training loss: 52.911, validation loss: 950.265
Iteration 6, training loss: 33.317, validation loss: 1001.383
Iteration 7, training loss: 22.996, validation loss: 982.183
Iteration 8, training loss: 17.187, validation loss: 956.423
Iteration 9, training loss: 13.749, validation loss: 948.659
Iteration 10, training loss: 11.633, validation loss: 937.438
Iteration 11, training loss: 10.294, validation loss: 926.332
Iteration 12, training loss: 9.414, validation loss: 907.229
Iteration 13, training loss: 8.821, validation loss: 887.835
Iteration 14, training loss: 8.413, validation loss: 870.600
Iteration 15, training loss: 8.129, validation loss: 860.791
Iteration 16, training loss: 7.926, validation loss: 849.825
Iteration 17, training loss: 7.777, validation loss: 836.607
Iteration 18, training loss: 7.665, validation loss: 830.016
Iteration 19, training loss: 7.579, validation loss: 825.465
Iteration 20, training loss: 7.512, validation loss: 820.616
Iteration 21, training loss: 7.457, validation loss: 816.510
Iteration 22, training loss: 7.412, validation loss: 812.124
Iteration 23, training loss: 7.373, validation loss: 807.827
Iteration 24, training loss: 7.338, validation loss: 803.245
Iteration 25, training loss: 7.308, validation loss: 798.583
Iteration 26, training loss: 7.280, validation loss: 793.634
Iteration 27, training loss: 7.255, validation loss: 788.637
Iteration 28, training loss: 7.231, validation loss: 783.578
Iteration 29, training loss: 7.208, validation loss: 778.696
Iteration 30, training loss: 7.187, validation loss: 774.002
Iteration 31, training loss: 7.167, validation loss: 769.711
Iteration 32, training loss: 7.148, validation loss: 765.830
Iteration 33, training loss: 7.130, validation loss: 762.423
Iteration 34, training loss: 7.112, validation loss: 759.395
Iteration 35, training loss: 7.095, validation loss: 756.653
Iteration 36, training loss: 7.078, validation loss: 754.062
Iteration 37, training loss: 7.062, validation loss: 751.623
Iteration 38, training loss: 7.046, validation loss: 749.271
Iteration 39, training loss: 7.031, validation loss: 747.019
Iteration 40, training loss: 7.016, validation loss: 744.826
Iteration 41, training loss: 7.002, validation loss: 742.705
Iteration 42, training loss: 6.988, validation loss: 740.632
Iteration 43, training loss: 6.975, validation loss: 738.617
Iteration 44, training loss: 6.961, validation loss: 736.646
Iteration 45, training loss: 6.948, validation loss: 734.721
Iteration 46, training loss: 6.935, validation loss: 732.835
Iteration 47, training loss: 6.923, validation loss: 730.987
Iteration 48, training loss: 6.911, validation loss: 729.176
Iteration 49, training loss: 6.899, validation loss: 727.396
Iteration 50, training loss: 6.887, validation loss: 725.649
Iteration 51, training loss: 6.876, validation loss: 723.930
Iteration 52, training loss: 6.864, validation loss: 722.243
Iteration 53, training loss: 6.853, validation loss: 720.578
Iteration 54, training loss: 6.842, validation loss: 718.946
Iteration 55, training loss: 6.831, validation loss: 717.332
Iteration 56, training loss: 6.821, validation loss: 715.752
Iteration 57, training loss: 6.810, validation loss: 714.185
Iteration 58, training loss: 6.800, validation loss: 712.656
Iteration 59, training loss: 6.790, validation loss: 711.134
Iteration 60, training loss: 6.780, validation loss: 709.653
Iteration 61, training loss: 6.771, validation loss: 708.174
Iteration 62, training loss: 6.761, validation loss: 706.739
Iteration 63, training loss: 6.751, validation loss: 705.302
Iteration 64, training loss: 6.742, validation loss: 703.912
Iteration 65, training loss: 6.733, validation loss: 702.517
Iteration 66, training loss: 6.724, validation loss: 701.168
Iteration 67, training loss: 6.715, validation loss: 699.813
Iteration 68, training loss: 6.706, validation loss: 698.501
Iteration 69, training loss: 6.697, validation loss: 697.187
Iteration 70, training loss: 6.688, validation loss: 695.910
Iteration 71, training loss: 6.680, validation loss: 694.636
Iteration 72, training loss: 6.672, validation loss: 693.392
Iteration 73, training loss: 6.663, validation loss: 692.158
Iteration 74, training loss: 6.655, validation loss: 690.948
Iteration 75, training loss: 6.647, validation loss: 689.751
Iteration 76, training loss: 6.639, validation loss: 688.575
Iteration 77, training loss: 6.631, validation loss: 687.414
Iteration 78, training loss: 6.623, validation loss: 686.271
Iteration 79, training loss: 6.615, validation loss: 685.145
Iteration 80, training loss: 6.607, validation loss: 684.035
Iteration 81, training loss: 6.600, validation loss: 682.942
Iteration 82, training loss: 6.592, validation loss: 681.864
Iteration 83, training loss: 6.585, validation loss: 680.801
Iteration 84, training loss: 6.578, validation loss: 679.754
Iteration 85, training loss: 6.570, validation loss: 678.720
Iteration 86, training loss: 6.563, validation loss: 677.702
Iteration 87, training loss: 6.556, validation loss: 676.696
Iteration 88, training loss: 6.549, validation loss: 675.705
Iteration 89, training loss: 6.542, validation loss: 674.726
Iteration 90, training loss: 6.535, validation loss: 673.760
Iteration 91, training loss: 6.528, validation loss: 672.806
Iteration 92, training loss: 6.521, validation loss: 671.864
Iteration 93, training loss: 6.514, validation loss: 670.934
Iteration 94, training loss: 6.507, validation loss: 670.015
Iteration 95, training loss: 6.501, validation loss: 669.106
Iteration 96, training loss: 6.494, validation loss: 668.208
Iteration 97, training loss: 6.488, validation loss: 667.320
Iteration 98, training loss: 6.481, validation loss: 666.441
Iteration 99, training loss: 6.475, validation loss: 665.572
Converged after -1 iteration, ob average 9.792s per iteration
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{74}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Plot the validation and training losses over for each iteration}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alternating optimization, k=100}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}sgd}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sgd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}als}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{als}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training iteration}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{val\PYZus{}loss\PYZus{}sgd}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sgd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{val\PYZus{}loss\PYZus{}als}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{als}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validating interation}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{74}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.legend.Legend at 0x7f0c6a8fe3e0>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_114_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}sgd}\PY{p}{)}\PY{p}{)}
\PY{n}{train\PYZus{}loss\PYZus{}sgd} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}sgd}\PY{p}{)}
\PY{n}{val\PYZus{}loss\PYZus{}sgd} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{val\PYZus{}loss\PYZus{}sgd}\PY{p}{)}
\PY{n}{train\PYZus{}loss\PYZus{}als} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}als}\PY{p}{)}
\PY{n}{val\PYZus{}loss\PYZus{}als} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{val\PYZus{}loss\PYZus{}als}\PY{p}{)}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alternating optimization, k=100}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}sgd}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{val\PYZus{}loss\PYZus{}sgd}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{stochastic gradient descent}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iteration}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}als}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{val\PYZus{}loss\PYZus{}als}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{alternative iteration}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iteration}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'list'>
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.legend.Legend at 0x7f0c6a1d3100>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_115_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{autoencoder-and-t-sne}{%
\section{Autoencoder and t-SNE}\label{autoencoder-and-t-sne}}

Hereinafter, we will implement an autoencoder and analyze uts latent
space via interpolations annd t-SNE. For this, we will use the famous
Fasion-MNIST dataset

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{76}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{typing} \PY{k+kn}{import} \PY{n}{List}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{offsetbox} \PY{k+kn}{import} \PY{n}{AnnotationBbox}\PY{p}{,} \PY{n}{OffsetImage}
\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k+kn}{import} \PY{n}{PCA}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{manifold} \PY{k+kn}{import} \PY{n}{TSNE}

\PY{k+kn}{import} \PY{n+nn}{torchvision}
\PY{k+kn}{from} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{FashionMNIST}
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{from} \PY{n+nn}{torch} \PY{k+kn}{import} \PY{n}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}
\PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim}\PY{n+nn}{.}\PY{n+nn}{lr\PYZus{}scheduler} \PY{k+kn}{import} \PY{n}{ExponentialLR}

\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{77}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
True
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{78}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cuda}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{if} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cpu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
cuda
    \end{Verbatim}

    \hypertarget{download-dataset-and-create-datasetdataloader}{%
\subsubsection{download dataset and create
dataset/dataloader}\label{download-dataset-and-create-datasetdataloader}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{79}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{FashionMNIST}\PY{p}{(}\PY{n}{root} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{torchvision}\PY{o}{.}\PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{test\PYZus{}dataset} \PY{o}{=} \PY{n}{FashionMNIST}\PY{p}{(}\PY{n}{root}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{transform} \PY{o}{=} \PY{n}{torchvision}\PY{o}{.}\PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{download} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{80}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{train\PYZus{}dataloader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{1024}\PY{p}{,} \PY{n}{shuffle} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,} \PY{n}{pin\PYZus{}memory}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{num\PYZus{}workers} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{test\PYZus{}dataloader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{1024}\PY{p}{,} \PY{n}{shuffle} \PY{o}{=} \PY{k+kc}{False}\PY{p}{,} \PY{n}{pin\PYZus{}memory}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{num\PYZus{}workers} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{81}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} numbers of data samples in each dataset}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{train\PYZus{}dataset}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} 60000}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} 10000}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
60000
10000
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{82}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} batch size (minibatch size = 1024)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{train\PYZus{}dataloader}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} 59}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{test\PYZus{}dataloader}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} 10}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
59
10
    \end{Verbatim}

    \hypertarget{check-the-output-size}{%
\subsubsection{check the output size}\label{check-the-output-size}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{83}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{conv\PYZus{}transpose\PYZus{}output\PYZus{}size}\PY{p}{(}\PY{n}{H\PYZus{}in}\PY{p}{,} \PY{n}{W\PYZus{}in}\PY{p}{,} \PY{n}{stride} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{dilation} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{output\PYZus{}padding} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
    
    \PY{k}{if} \PY{n}{H\PYZus{}in} \PY{o}{==} \PY{n}{W\PYZus{}in}\PY{p}{:}
        \PY{n}{H\PYZus{}out} \PY{o}{=} \PY{p}{(}\PY{n}{H\PYZus{}in} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{n}{stride} \PY{o}{\PYZhy{}} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{padding} \PY{o}{+} \PY{n}{dilation} \PY{o}{*} \PY{p}{(}\PY{n}{kernel\PYZus{}size} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{output\PYZus{}padding} \PY{o}{+} \PY{l+m+mi}{1}
        \PY{n}{W\PYZus{}out} \PY{o}{=} \PY{n}{H\PYZus{}out}
    \PY{k}{else}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{should be square input}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{raise} \PY{n+ne}{ValueError}
    \PY{k}{return} \PY{n}{H\PYZus{}out}\PY{p}{,} \PY{n}{W\PYZus{}out}
    
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{84}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{output\PYZus{}size} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{output\PYZus{}size}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{conv\PYZus{}transpose\PYZus{}output\PYZus{}size}\PY{p}{(}\PY{n}{H\PYZus{}in}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,} \PY{n}{W\PYZus{}in}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,} \PY{n}{stride} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{dilation}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{output\PYZus{}padding}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{output\PYZus{}size}\PY{p}{)}
\PY{n}{output\PYZus{}size}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{conv\PYZus{}transpose\PYZus{}output\PYZus{}size}\PY{p}{(}\PY{n}{H\PYZus{}in} \PY{o}{=} \PY{n}{output\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{W\PYZus{}in} \PY{o}{=} \PY{n}{output\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{stride} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{output\PYZus{}padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{output\PYZus{}size}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{conv\PYZus{}transpose\PYZus{}output\PYZus{}size}\PY{p}{(}\PY{n}{H\PYZus{}in} \PY{o}{=} \PY{n}{output\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{W\PYZus{}in} \PY{o}{=} \PY{n}{output\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{output\PYZus{}padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{output\PYZus{}size}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{conv\PYZus{}transpose\PYZus{}output\PYZus{}size}\PY{p}{(}\PY{n}{H\PYZus{}in} \PY{o}{=} \PY{n}{output\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{W\PYZus{}in} \PY{o}{=} \PY{n}{output\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{output\PYZus{}size}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{conv\PYZus{}transpose\PYZus{}output\PYZus{}size}\PY{p}{(}\PY{n}{H\PYZus{}in} \PY{o}{=} \PY{n}{output\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{W\PYZus{}in} \PY{o}{=} \PY{n}{output\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{output\PYZus{}size}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{conv\PYZus{}transpose\PYZus{}output\PYZus{}size}\PY{p}{(}\PY{n}{H\PYZus{}in} \PY{o}{=} \PY{n}{output\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{W\PYZus{}in} \PY{o}{=} \PY{n}{output\PYZus{}size}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[(9, 9)]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{85}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{output\PYZus{}size}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[(9, 9), (20, 20), (42, 42), (42, 42), (42, 42), (44, 44)]
    \end{Verbatim}

    \hypertarget{confirm-the-output-size-by-using-dummy-model}{%
\subsubsection{confirm the output size by using dummy
model}\label{confirm-the-output-size-by-using-dummy-model}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{86}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{input} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1024}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}
\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{input shape: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{input}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{input} \PY{o}{=} \PY{n+nb}{input}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{encode} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
\PY{p}{)}
\PY{n}{encode}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{latent\PYZus{}factors} \PY{o}{=} \PY{n}{encode}\PY{p}{(}\PY{n+nb}{input}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{latent factors shape: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{latent\PYZus{}factors}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}

\PY{n}{decoder} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{n}{kernel\PYZus{}size}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{n}{kernel\PYZus{}size}\PY{p}{,} \PY{n}{stride} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{output\PYZus{}padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{n}{kernel\PYZus{}size}\PY{p}{,} \PY{n}{stride} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{output\PYZus{}padding} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{n}{kernel\PYZus{}size}\PY{p}{,} \PY{n}{padding} \PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{n}{kernel\PYZus{}size}\PY{p}{,} \PY{n}{padding} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{n}{kernel\PYZus{}size}\PY{p}{)}
\PY{p}{)}

\PY{n}{decoder}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{output} \PY{o}{=} \PY{n}{decoder}\PY{p}{(}\PY{n}{latent\PYZus{}factors}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{output shape: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{output}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
input shape: torch.Size([1024, 1, 28, 28])
latent factors shape: torch.Size([1024, 32, 3, 3])
output shape: torch.Size([1024, 1, 28, 28])
    \end{Verbatim}

    \hypertarget{autoencoder}{%
\subsubsection{Autoencoder}\label{autoencoder}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{87}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Autoencoder}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{BatchNorm2d}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{stride} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{output\PYZus{}padding}\PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{BatchNorm2d}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{stride} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{output\PYZus{}padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{padding} \PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Sigmoid}\PY{p}{(}\PY{p}{)}
        \PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{z} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{x\PYZus{}approx} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z}\PY{p}{)}

        \PY{k}{assert} \PY{n}{x}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{n}{x\PYZus{}approx}\PY{o}{.}\PY{n}{shape}
        \PY{k}{return} \PY{n}{x\PYZus{}approx}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{Autoencoder}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Autoencoder(
  (encode): Sequential(
    (0): Conv2d(1, 4, kernel\_size=(3, 3), stride=(1, 1))
    (1): LeakyReLU(negative\_slope=0.01)
    (2): Conv2d(4, 16, kernel\_size=(3, 3), stride=(1, 1))
    (3): MaxPool2d(kernel\_size=2, stride=2, padding=0, dilation=1,
ceil\_mode=False)
    (4): LeakyReLU(negative\_slope=0.01)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (6): LeakyReLU(negative\_slope=0.01)
    (7): Conv2d(16, 32, kernel\_size=(3, 3), stride=(1, 1))
    (8): MaxPool2d(kernel\_size=2, stride=2, padding=0, dilation=1,
ceil\_mode=False)
    (9): LeakyReLU(negative\_slope=0.01)
    (10): Conv2d(32, 32, kernel\_size=(3, 3), stride=(1, 1))
    (11): LeakyReLU(negative\_slope=0.01)
  )
  (decode): Sequential(
    (0): ConvTranspose2d(32, 32, kernel\_size=(3, 3), stride=(1, 1))
    (1): LeakyReLU(negative\_slope=0.01)
    (2): ConvTranspose2d(32, 16, kernel\_size=(3, 3), stride=(2, 2),
output\_padding=(1, 1))
    (3): LeakyReLU(negative\_slope=0.01)
    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (5): ConvTranspose2d(16, 16, kernel\_size=(3, 3), stride=(2, 2),
output\_padding=(1, 1))
    (6): LeakyReLU(negative\_slope=0.01)
    (7): ConvTranspose2d(16, 16, kernel\_size=(3, 3), stride=(1, 1), padding=(1,
1))
    (8): ConvTranspose2d(16, 4, kernel\_size=(3, 3), stride=(1, 1), padding=(1,
1))
    (9): ConvTranspose2d(4, 1, kernel\_size=(3, 3), stride=(1, 1))
    (10): Sigmoid()
  )
)
    \end{Verbatim}

    \hypertarget{train-the-autoencoder}{%
\subsubsection{train the autoencoder}\label{train-the-autoencoder}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{88}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{Autoencoder}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{,} \PY{n}{weight\PYZus{}decay}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}
\PY{n}{scheduler} \PY{o}{=} \PY{n}{ExponentialLR}\PY{p}{(}\PY{n}{optimizer}\PY{p}{,} \PY{n}{gamma} \PY{o}{=} \PY{l+m+mf}{0.999}\PY{p}{)}

\PY{n}{log\PYZus{}every\PYZus{}batch} \PY{o}{=} \PY{l+m+mi}{20}
\PY{n}{max\PYZus{}epochs} \PY{o}{=} \PY{l+m+mi}{50}
\PY{n}{avg\PYZus{}train\PYZus{}loss} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{avg\PYZus{}test\PYZus{}loss} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{89}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
    \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    \PY{n}{train\PYZus{}loss\PYZus{}trace} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{batch}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{label}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}dataloader}\PY{p}{)}\PY{p}{:}
        \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} predict}
        \PY{n}{predict} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} evaluate reconstruction loss (mean square loss)}
        \PY{n}{loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{mse\PYZus{}loss}\PY{p}{(}\PY{n}{predict}\PY{p}{,} \PY{n}{x}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} set 0 to the gradient}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} calculate gradients by backward propagation}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} update parameters}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} loss.detach() =\PYZgt{} separate a loss from the computational graph, and doesn\PYZsq{}t require gradient}
        \PY{c+c1}{\PYZsh{} detached tensor == tensor}
        \PY{c+c1}{\PYZsh{} tensor.item() == content}
        \PY{n}{train\PYZus{}loss\PYZus{}trace}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{k}{if} \PY{n}{batch} \PY{o}{\PYZpc{}} \PY{n}{log\PYZus{}every\PYZus{}batch} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train: Epoch }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{, batch }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{, ====\PYZgt{} loss }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{epoch}\PY{p}{,} \PY{n}{batch}\PY{p}{,} \PY{n}{loss}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} if you do not call loss.backward()}
    \PY{c+c1}{\PYZsh{} with torch.no\PYZus{}grad()}
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
        \PY{n}{test\PYZus{}loss\PYZus{}trace} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{k}{for} \PY{n}{batch}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{label}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{test\PYZus{}dataloader}\PY{p}{)}\PY{p}{:}
            \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{predict} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}
            \PY{n}{loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{mse\PYZus{}loss}\PY{p}{(}\PY{n}{predict}\PY{p}{,} \PY{n}{x}\PY{p}{)}
            \PY{n}{test\PYZus{}loss\PYZus{}trace}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{k}{if} \PY{n}{batch} \PY{o}{\PYZpc{}} \PY{n}{log\PYZus{}every\PYZus{}batch} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test: Epoch }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{, batch }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{, ====\PYZgt{} loss }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{epoch}\PY{p}{,} \PY{n}{batch}\PY{p}{,} \PY{n}{loss}\PY{p}{)}\PY{p}{)}
    \PY{n}{\PYZus{}avg\PYZus{}train\PYZus{}loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{train\PYZus{}loss\PYZus{}trace}\PY{p}{)}
    \PY{n}{avg\PYZus{}train\PYZus{}loss}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{\PYZus{}avg\PYZus{}train\PYZus{}loss}\PY{p}{)}
    \PY{n}{\PYZus{}avg\PYZus{}test\PYZus{}loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{test\PYZus{}loss\PYZus{}trace}\PY{p}{)}
    \PY{n}{avg\PYZus{}test\PYZus{}loss}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{\PYZus{}avg\PYZus{}test\PYZus{}loss}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ finished \PYZhy{}average train loss }\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}avg\PYZus{}train\PYZus{}loss}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, }\PY{l+s+s2}{\PYZdq{}}
    \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{average test loss }\PY{l+s+si}{\PYZob{}}\PY{n}{\PYZus{}avg\PYZus{}test\PYZus{}loss}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train: Epoch 0, batch 0, ====> loss 0.16013331711292267
Train: Epoch 0, batch 20, ====> loss 0.09437862783670425
Train: Epoch 0, batch 40, ====> loss 0.04736725240945816
Test: Epoch 0, batch 0, ====> loss 0.04256131500005722
Epoch 0 finished -average train loss 0.07903905351788311, average test loss
0.0422297403216362
Train: Epoch 1, batch 0, ====> loss 0.03468252345919609
Train: Epoch 1, batch 20, ====> loss 0.02998919039964676
Train: Epoch 1, batch 40, ====> loss 0.025809958577156067
Test: Epoch 1, batch 0, ====> loss 0.02399859018623829
Epoch 1 finished -average train loss 0.028447190173349138, average test loss
0.023757354356348515
Train: Epoch 2, batch 0, ====> loss 0.023207029327750206
Train: Epoch 2, batch 20, ====> loss 0.022209007292985916
Train: Epoch 2, batch 40, ====> loss 0.020589759573340416
Test: Epoch 2, batch 0, ====> loss 0.02043464593589306
Epoch 2 finished -average train loss 0.02166011042387809, average test loss
0.020236410945653916
Train: Epoch 3, batch 0, ====> loss 0.019745497032999992
Train: Epoch 3, batch 20, ====> loss 0.019053565338253975
Train: Epoch 3, batch 40, ====> loss 0.018296614289283752
Test: Epoch 3, batch 0, ====> loss 0.0181821808218956
Epoch 3 finished -average train loss 0.019061389047715625, average test loss
0.01804318781942129
Train: Epoch 4, batch 0, ====> loss 0.018276184797286987
Train: Epoch 4, batch 20, ====> loss 0.017871620133519173
Train: Epoch 4, batch 40, ====> loss 0.017441432923078537
Test: Epoch 4, batch 0, ====> loss 0.017508579418063164
Epoch 4 finished -average train loss 0.01734397740308511, average test loss
0.017352220229804517
Train: Epoch 5, batch 0, ====> loss 0.017430471256375313
Train: Epoch 5, batch 20, ====> loss 0.0162962693721056
Train: Epoch 5, batch 40, ====> loss 0.016273491084575653
Test: Epoch 5, batch 0, ====> loss 0.01580793969333172
Epoch 5 finished -average train loss 0.016360768811556244, average test loss
0.015699511766433714
Train: Epoch 6, batch 0, ====> loss 0.015458679758012295
Train: Epoch 6, batch 20, ====> loss 0.015328919515013695
Train: Epoch 6, batch 40, ====> loss 0.01520688645541668
Test: Epoch 6, batch 0, ====> loss 0.015116713009774685
Epoch 6 finished -average train loss 0.015369900518049628, average test loss
0.015010850690305232
Train: Epoch 7, batch 0, ====> loss 0.014906984753906727
Train: Epoch 7, batch 20, ====> loss 0.015012318268418312
Train: Epoch 7, batch 40, ====> loss 0.01459004171192646
Test: Epoch 7, batch 0, ====> loss 0.01468534953892231
Epoch 7 finished -average train loss 0.01479505946449304, average test loss
0.014577049016952514
Train: Epoch 8, batch 0, ====> loss 0.014018232002854347
Train: Epoch 8, batch 20, ====> loss 0.015019667334854603
Train: Epoch 8, batch 40, ====> loss 0.014290113933384418
Test: Epoch 8, batch 0, ====> loss 0.0140758091583848
Epoch 8 finished -average train loss 0.014228079310160572, average test loss
0.013973939046263695
Train: Epoch 9, batch 0, ====> loss 0.01391446590423584
Train: Epoch 9, batch 20, ====> loss 0.013830483891069889
Train: Epoch 9, batch 40, ====> loss 0.01383315771818161
Test: Epoch 9, batch 0, ====> loss 0.013857286423444748
Epoch 9 finished -average train loss 0.013902799021136962, average test loss
0.013748845364898444
Train: Epoch 10, batch 0, ====> loss 0.013867504894733429
Train: Epoch 10, batch 20, ====> loss 0.013416139408946037
Train: Epoch 10, batch 40, ====> loss 0.013110562227666378
Test: Epoch 10, batch 0, ====> loss 0.01377852726727724
Epoch 10 finished -average train loss 0.013385588034861168, average test loss
0.013672063406556845
Train: Epoch 11, batch 0, ====> loss 0.01381694246083498
Train: Epoch 11, batch 20, ====> loss 0.013519560918211937
Train: Epoch 11, batch 40, ====> loss 0.01299335341900587
Test: Epoch 11, batch 0, ====> loss 0.013000305742025375
Epoch 11 finished -average train loss 0.013262080360140842, average test loss
0.012904016952961683
Train: Epoch 12, batch 0, ====> loss 0.01260998286306858
Train: Epoch 12, batch 20, ====> loss 0.012777328491210938
Train: Epoch 12, batch 40, ====> loss 0.012436863966286182
Test: Epoch 12, batch 0, ====> loss 0.012775087729096413
Epoch 12 finished -average train loss 0.01276386582876666, average test loss
0.012674309033900499
Train: Epoch 13, batch 0, ====> loss 0.012589248828589916
Train: Epoch 13, batch 20, ====> loss 0.012949843890964985
Train: Epoch 13, batch 40, ====> loss 0.012578214518725872
Test: Epoch 13, batch 0, ====> loss 0.012859219685196877
Epoch 13 finished -average train loss 0.012558012080015772, average test loss
0.012766116391867399
Train: Epoch 14, batch 0, ====> loss 0.012705537490546703
Train: Epoch 14, batch 20, ====> loss 0.012723073363304138
Train: Epoch 14, batch 40, ====> loss 0.012368442490696907
Test: Epoch 14, batch 0, ====> loss 0.012811398133635521
Epoch 14 finished -average train loss 0.01236320755807525, average test loss
0.012699964828789235
Train: Epoch 15, batch 0, ====> loss 0.012886492535471916
Train: Epoch 15, batch 20, ====> loss 0.012407819740474224
Train: Epoch 15, batch 40, ====> loss 0.011946804821491241
Test: Epoch 15, batch 0, ====> loss 0.01273078192025423
Epoch 15 finished -average train loss 0.01218024073010784, average test loss
0.012631428521126508
Train: Epoch 16, batch 0, ====> loss 0.011905006133019924
Train: Epoch 16, batch 20, ====> loss 0.011890556663274765
Train: Epoch 16, batch 40, ====> loss 0.011957625858485699
Test: Epoch 16, batch 0, ====> loss 0.011991881765425205
Epoch 16 finished -average train loss 0.011950983965800981, average test loss
0.011898560263216496
Train: Epoch 17, batch 0, ====> loss 0.01158825121819973
Train: Epoch 17, batch 20, ====> loss 0.011519590392708778
Train: Epoch 17, batch 40, ====> loss 0.011359314434230328
Test: Epoch 17, batch 0, ====> loss 0.0118581373244524
Epoch 17 finished -average train loss 0.01179496563529059, average test loss
0.011758849024772644
Train: Epoch 18, batch 0, ====> loss 0.011773256585001945
Train: Epoch 18, batch 20, ====> loss 0.011945154517889023
Train: Epoch 18, batch 40, ====> loss 0.01179427932947874
Test: Epoch 18, batch 0, ====> loss 0.01218130812048912
Epoch 18 finished -average train loss 0.011832584358625492, average test loss
0.012073920387774707
Train: Epoch 19, batch 0, ====> loss 0.012029966339468956
Train: Epoch 19, batch 20, ====> loss 0.011206738650798798
Train: Epoch 19, batch 40, ====> loss 0.011580892838537693
Test: Epoch 19, batch 0, ====> loss 0.011518599465489388
Epoch 19 finished -average train loss 0.01148614481533483, average test loss
0.011413269583135844
Train: Epoch 20, batch 0, ====> loss 0.011281387880444527
Train: Epoch 20, batch 20, ====> loss 0.011149509809911251
Train: Epoch 20, batch 40, ====> loss 0.011358949355781078
Test: Epoch 20, batch 0, ====> loss 0.011996276676654816
Epoch 20 finished -average train loss 0.011294384171270717, average test loss
0.011877041403204202
Train: Epoch 21, batch 0, ====> loss 0.011744141578674316
Train: Epoch 21, batch 20, ====> loss 0.01149495504796505
Train: Epoch 21, batch 40, ====> loss 0.01127166673541069
Test: Epoch 21, batch 0, ====> loss 0.011472079902887344
Epoch 21 finished -average train loss 0.01121400119894642, average test loss
0.011382276564836502
Train: Epoch 22, batch 0, ====> loss 0.011226317845284939
Train: Epoch 22, batch 20, ====> loss 0.01080704852938652
Train: Epoch 22, batch 40, ====> loss 0.011074683628976345
Test: Epoch 22, batch 0, ====> loss 0.011273065581917763
Epoch 22 finished -average train loss 0.011110330758205915, average test loss
0.011154937371611596
Train: Epoch 23, batch 0, ====> loss 0.010908885858952999
Train: Epoch 23, batch 20, ====> loss 0.010882333852350712
Train: Epoch 23, batch 40, ====> loss 0.011129933409392834
Test: Epoch 23, batch 0, ====> loss 0.011203337460756302
Epoch 23 finished -average train loss 0.010996217050163423, average test loss
0.011112304124981164
Train: Epoch 24, batch 0, ====> loss 0.01108059287071228
Train: Epoch 24, batch 20, ====> loss 0.010732382535934448
Train: Epoch 24, batch 40, ====> loss 0.010887386277318
Test: Epoch 24, batch 0, ====> loss 0.011143493466079235
Epoch 24 finished -average train loss 0.010962357728789418, average test loss
0.011035792715847492
Train: Epoch 25, batch 0, ====> loss 0.011028462089598179
Train: Epoch 25, batch 20, ====> loss 0.011071786284446716
Train: Epoch 25, batch 40, ====> loss 0.010371638461947441
Test: Epoch 25, batch 0, ====> loss 0.010916877537965775
Epoch 25 finished -average train loss 0.010801384920033357, average test loss
0.01080984165892005
Train: Epoch 26, batch 0, ====> loss 0.011064086109399796
Train: Epoch 26, batch 20, ====> loss 0.01079668290913105
Train: Epoch 26, batch 40, ====> loss 0.010987512767314911
Test: Epoch 26, batch 0, ====> loss 0.011119004338979721
Epoch 26 finished -average train loss 0.010816806374843849, average test loss
0.011027568951249123
Train: Epoch 27, batch 0, ====> loss 0.011190000921487808
Train: Epoch 27, batch 20, ====> loss 0.011132100597023964
Train: Epoch 27, batch 40, ====> loss 0.010536123998463154
Test: Epoch 27, batch 0, ====> loss 0.010869049467146397
Epoch 27 finished -average train loss 0.010621135091503798, average test loss
0.010781824309378862
Train: Epoch 28, batch 0, ====> loss 0.010431870818138123
Train: Epoch 28, batch 20, ====> loss 0.010446226224303246
Train: Epoch 28, batch 40, ====> loss 0.010435184463858604
Test: Epoch 28, batch 0, ====> loss 0.010490252636373043
Epoch 28 finished -average train loss 0.010566964082546154, average test loss
0.010398110561072826
Train: Epoch 29, batch 0, ====> loss 0.010277084074914455
Train: Epoch 29, batch 20, ====> loss 0.010712441056966782
Train: Epoch 29, batch 40, ====> loss 0.010248839855194092
Test: Epoch 29, batch 0, ====> loss 0.010616184212267399
Epoch 29 finished -average train loss 0.010516095748644764, average test loss
0.010538691096007824
Train: Epoch 30, batch 0, ====> loss 0.010308841243386269
Train: Epoch 30, batch 20, ====> loss 0.010487787425518036
Train: Epoch 30, batch 40, ====> loss 0.011055653914809227
Test: Epoch 30, batch 0, ====> loss 0.010554458014667034
Epoch 30 finished -average train loss 0.010377064441977921, average test loss
0.010452309902757407
Train: Epoch 31, batch 0, ====> loss 0.010496542789041996
Train: Epoch 31, batch 20, ====> loss 0.010556673631072044
Train: Epoch 31, batch 40, ====> loss 0.009875630959868431
Test: Epoch 31, batch 0, ====> loss 0.010653515346348286
Epoch 31 finished -average train loss 0.010328117237126424, average test loss
0.010563903488218784
Train: Epoch 32, batch 0, ====> loss 0.010365121066570282
Train: Epoch 32, batch 20, ====> loss 0.010468810796737671
Train: Epoch 32, batch 40, ====> loss 0.010082604363560677
Test: Epoch 32, batch 0, ====> loss 0.01074431836605072
Epoch 32 finished -average train loss 0.010329174840728105, average test loss
0.010645839665085078
Train: Epoch 33, batch 0, ====> loss 0.01032869890332222
Train: Epoch 33, batch 20, ====> loss 0.010474893264472485
Train: Epoch 33, batch 40, ====> loss 0.010146217420697212
Test: Epoch 33, batch 0, ====> loss 0.010291075333952904
Epoch 33 finished -average train loss 0.010196614284384049, average test loss
0.010206916276365519
Train: Epoch 34, batch 0, ====> loss 0.009939552284777164
Train: Epoch 34, batch 20, ====> loss 0.010556548833847046
Train: Epoch 34, batch 40, ====> loss 0.010249217040836811
Test: Epoch 34, batch 0, ====> loss 0.010360646061599255
Epoch 34 finished -average train loss 0.010193050791651516, average test loss
0.010284452978521586
Train: Epoch 35, batch 0, ====> loss 0.009625828824937344
Train: Epoch 35, batch 20, ====> loss 0.009922629222273827
Train: Epoch 35, batch 40, ====> loss 0.010532189160585403
Test: Epoch 35, batch 0, ====> loss 0.010741481557488441
Epoch 35 finished -average train loss 0.010155773658494828, average test loss
0.01064360812306404
Train: Epoch 36, batch 0, ====> loss 0.01028341893106699
Train: Epoch 36, batch 20, ====> loss 0.010257395915687084
Train: Epoch 36, batch 40, ====> loss 0.0098432507365942
Test: Epoch 36, batch 0, ====> loss 0.010512694716453552
Epoch 36 finished -average train loss 0.010095329292244831, average test loss
0.010437004454433917
Train: Epoch 37, batch 0, ====> loss 0.009869354777038097
Train: Epoch 37, batch 20, ====> loss 0.009896316565573215
Train: Epoch 37, batch 40, ====> loss 0.009787503629922867
Test: Epoch 37, batch 0, ====> loss 0.010110492818057537
Epoch 37 finished -average train loss 0.009970763950782308, average test loss
0.010036449786275626
Train: Epoch 38, batch 0, ====> loss 0.009615234099328518
Train: Epoch 38, batch 20, ====> loss 0.01020972803235054
Train: Epoch 38, batch 40, ====> loss 0.00988971721380949
Test: Epoch 38, batch 0, ====> loss 0.010078180581331253
Epoch 38 finished -average train loss 0.009986867954544091, average test loss
0.010001556295901538
Train: Epoch 39, batch 0, ====> loss 0.00989652331918478
Train: Epoch 39, batch 20, ====> loss 0.009811175987124443
Train: Epoch 39, batch 40, ====> loss 0.010192002169787884
Test: Epoch 39, batch 0, ====> loss 0.010935988277196884
Epoch 39 finished -average train loss 0.009883820584391132, average test loss
0.010862605553120375
Train: Epoch 40, batch 0, ====> loss 0.010766780935227871
Train: Epoch 40, batch 20, ====> loss 0.009802098385989666
Train: Epoch 40, batch 40, ====> loss 0.009784935973584652
Test: Epoch 40, batch 0, ====> loss 0.009921948425471783
Epoch 40 finished -average train loss 0.00989733126519595, average test loss
0.009846709575504065
Train: Epoch 41, batch 0, ====> loss 0.009849497117102146
Train: Epoch 41, batch 20, ====> loss 0.01033532340079546
Train: Epoch 41, batch 40, ====> loss 0.010149899870157242
Test: Epoch 41, batch 0, ====> loss 0.011212645098567009
Epoch 41 finished -average train loss 0.009936800921114824, average test loss
0.011129064299166203
Train: Epoch 42, batch 0, ====> loss 0.010445576161146164
Train: Epoch 42, batch 20, ====> loss 0.009777077473700047
Train: Epoch 42, batch 40, ====> loss 0.010046890936791897
Test: Epoch 42, batch 0, ====> loss 0.011188158765435219
Epoch 42 finished -average train loss 0.00984666038746551, average test loss
0.011100718937814235
Train: Epoch 43, batch 0, ====> loss 0.010665571317076683
Train: Epoch 43, batch 20, ====> loss 0.009837196208536625
Train: Epoch 43, batch 40, ====> loss 0.009557405486702919
Test: Epoch 43, batch 0, ====> loss 0.010017311200499535
Epoch 43 finished -average train loss 0.009752039292479977, average test loss
0.009943978115916251
Train: Epoch 44, batch 0, ====> loss 0.009908780455589294
Train: Epoch 44, batch 20, ====> loss 0.009754969738423824
Train: Epoch 44, batch 40, ====> loss 0.009567401371896267
Test: Epoch 44, batch 0, ====> loss 0.010034620761871338
Epoch 44 finished -average train loss 0.009754303069311684, average test loss
0.009957351814955473
Train: Epoch 45, batch 0, ====> loss 0.009400739334523678
Train: Epoch 45, batch 20, ====> loss 0.010119070298969746
Train: Epoch 45, batch 40, ====> loss 0.009518839418888092
Test: Epoch 45, batch 0, ====> loss 0.010190078988671303
Epoch 45 finished -average train loss 0.00971799828427828, average test loss
0.010112348198890685
Train: Epoch 46, batch 0, ====> loss 0.010019086301326752
Train: Epoch 46, batch 20, ====> loss 0.00920373760163784
Train: Epoch 46, batch 40, ====> loss 0.009445738047361374
Test: Epoch 46, batch 0, ====> loss 0.009748846292495728
Epoch 46 finished -average train loss 0.009606305778152862, average test loss
0.009671254269778728
Train: Epoch 47, batch 0, ====> loss 0.009641528129577637
Train: Epoch 47, batch 20, ====> loss 0.009210172109305859
Train: Epoch 47, batch 40, ====> loss 0.009374353103339672
Test: Epoch 47, batch 0, ====> loss 0.009828917682170868
Epoch 47 finished -average train loss 0.009546061693611791, average test loss
0.009753471240401268
Train: Epoch 48, batch 0, ====> loss 0.009590196423232555
Train: Epoch 48, batch 20, ====> loss 0.009675946086645126
Train: Epoch 48, batch 40, ====> loss 0.009779122658073902
Test: Epoch 48, batch 0, ====> loss 0.010148216970264912
Epoch 48 finished -average train loss 0.009641296530173997, average test loss
0.010070438496768475
Train: Epoch 49, batch 0, ====> loss 0.010045197792351246
Train: Epoch 49, batch 20, ====> loss 0.009461924433708191
Train: Epoch 49, batch 40, ====> loss 0.009522485546767712
Test: Epoch 49, batch 0, ====> loss 0.009942996315658092
Epoch 49 finished -average train loss 0.009563922992575977, average test loss
0.009868626855313778
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{90}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{avg\PYZus{}train\PYZus{}loss}\PY{p}{)}
\PY{n}{test\PYZus{}loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{avg\PYZus{}test\PYZus{}loss}\PY{p}{)}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Autoencoder}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}loss}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}loss}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epochs}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_136_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{obtain-latent-factors}{%
\subsubsection{obtain latent factors}\label{obtain-latent-factors}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{91}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
\PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{latent} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{batch}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{test\PYZus{}dataloader}\PY{p}{)}\PY{p}{:}
        \PY{n}{latent}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} concatenating latent factors for each input}
    \PY{n}{latent} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{n}{latent}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{92}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{latent}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{latent}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{latent}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'torch.Tensor'>
torch.Size([10000, 32, 3, 3])
torch.Size([32, 3, 3])
    \end{Verbatim}

    \hypertarget{dimensionality-reduction-onto-2-dim-space-for-visualization}{%
\subsubsection{Dimensionality reduction onto 2-dim space for
visualization}\label{dimensionality-reduction-onto-2-dim-space-for-visualization}}

The shape of a latent factor is torch.Size({[}32, 3, 3{]}). To visualize
this latent space, we need to reduce dimensionality reasonably.

\begin{itemize}
\tightlist
\item
  \textbf{PCA}

  \begin{itemize}
  \tightlist
  \item
    Linear
  \item
    Capture global data latent structure
  \item
    Find the largest variant direction
  \end{itemize}
\item
  \textbf{t-SNE} (t-distributed stochastic neighboring embeddings)

  \begin{itemize}
  \tightlist
  \item
    Non-Linear
  \item
    Capture local data latent structure
  \item
    Optimize low-dimensional embeddings for each data samples which can
    reconstruct local relationship between samples in the low
    dimensional space
  \end{itemize}
\end{itemize}

    \hypertarget{random-sampling-images-to-be-visualized-from-test_dataset}{%
\subsubsection{random sampling images to be visualized from
test\_dataset}\label{random-sampling-images-to-be-visualized-from-test_dataset}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{93}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{n\PYZus{}vis\PYZus{}samples} \PY{o}{=} \PY{l+m+mi}{1000}
\PY{n}{indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{latent}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}vis\PYZus{}samples}\PY{p}{,} \PY{n}{replace} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}
\PY{n}{vis\PYZus{}samples} \PY{o}{=} \PY{n}{latent}\PY{p}{[}\PY{n}{indices}\PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{vis\PYZus{}samples}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1000, 32, 3, 3])
    \end{Verbatim}

    \hypertarget{pca-sklearn}{%
\subsubsection{PCA (sklearn)}\label{pca-sklearn}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{94}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{coords\PYZus{}pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{vis\PYZus{}samples}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{n\PYZus{}vis\PYZus{}samples}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{t-sne-sklearn}{%
\subsubsection{t-SNE (sklearn)}\label{t-sne-sklearn}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{95}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{coords\PYZus{}tsne} \PY{o}{=} \PY{n}{TSNE}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{perplexity} \PY{o}{=} \PY{l+m+mi}{50}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{vis\PYZus{}samples}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{n\PYZus{}vis\PYZus{}samples}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{visuzalization}{%
\subsubsection{Visuzalization}\label{visuzalization}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{96}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} image}
\PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} label}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
9
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_148_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{126}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{latent\PYZus{}space\PYZus{}visualization}\PY{p}{(}\PY{n}{indices}\PY{p}{,} \PY{n}{coords}\PY{p}{,} \PY{n}{technique}\PY{p}{:} \PY{n+nb}{str}\PY{p}{,} \PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n}{technique}\PY{p}{)}
    \PY{k}{for} \PY{n}{idx}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{indices}\PY{p}{,} \PY{n}{coords}\PY{p}{)}\PY{p}{:}
        \PY{n}{im} \PY{o}{=} \PY{n}{OffsetImage}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{zoom} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{ab} \PY{o}{=} \PY{n}{AnnotationBbox}\PY{p}{(}\PY{n}{im}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{,} \PY{n}{xycoords}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{frameon}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        \PY{n}{ax}\PY{o}{.}\PY{n}{add\PYZus{}artist}\PY{p}{(}\PY{n}{ab}\PY{p}{)}
        \PY{n}{ax}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{update\PYZus{}datalim}\PY{p}{(}\PY{n}{coords}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{autoscale}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{127}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{latent\PYZus{}space\PYZus{}visualization}\PY{p}{(}\PY{n}{indices}\PY{p}{,} \PY{n}{coords\PYZus{}pca}\PY{p}{,} \PY{n}{technique}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pca}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_150_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{128}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{latent\PYZus{}space\PYZus{}visualization}\PY{p}{(}\PY{n}{indices}\PY{p}{,} \PY{n}{coords\PYZus{}tsne}\PY{p}{,} \PY{n}{technique}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tsne}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_151_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{linear-interpolation-on-the-latent-space}{%
\subsubsection{Linear interpolation on the latent
space}\label{linear-interpolation-on-the-latent-space}}

\begin{itemize}
\tightlist
\item
  If the latent space has learned something meaningfull, we can leverage
  this for further analysis/downstream tasks
\item
  Especially, we can interpolate some images by sampling intermediate
  latent factor from the latent space.
\item
  For instance, we are going to generate latent variable by feeding
  input image to the encoder and linearly interpolate between them,
\item
  By feeding the interpolated latent factors to the decoder, you can
  generate an intermediate (interpolated) image between two samples.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{102}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'torch.Tensor'>
<class 'int'>
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{Test dataset:   torch.utils.data.Dataset}
\PY{l+s+sd}{                Test images [tensor, int]}

\PY{l+s+sd}{idx\PYZus{}i:          int}
\PY{l+s+sd}{                Id for the first image}

\PY{l+s+sd}{idx\PYZus{}j:          int}
\PY{l+s+sd}{                Id for the second image}

\PY{l+s+sd}{n:              n, optional, default: 1}
\PY{l+s+sd}{                Number of intermediate interpolations}
\PY{l+s+sd}{                (including original reconstrcutions)}
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{generate-latent-factors-corresponding-to-the-id-of-sample-image}{%
\subsubsection{Generate latent factors corresponding to the id of sample
image}\label{generate-latent-factors-corresponding-to-the-id-of-sample-image}}

    \hypertarget{select-the-end-to-end-sample-for-interpolation}{%
\subsubsection{select the end to end sample for
interpolation}\label{select-the-end-to-end-sample-for-interpolation}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{113}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{idx\PYZus{}i} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{idx\PYZus{}j} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{n} \PY{o}{=} \PY{l+m+mi}{10} \PY{c+c1}{\PYZsh{} the number of interpolations}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx\PYZus{}i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx\PYZus{}j}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{113}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.image.AxesImage at 0x7f0cc6e96f20>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_157_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{111}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} latent factor zi for i}
\PY{c+c1}{\PYZsh{} latent factor zj for j}
\PY{n}{z\PYZus{}i} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx\PYZus{}i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{z\PYZus{}i}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n}{z\PYZus{}j} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx\PYZus{}j}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{z\PYZus{}j}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([32, 3, 3])
torch.Size([32, 3, 3])
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{121}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{n}\PY{o}{/}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{]}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{p}{[}\PY{n}{sub} \PY{k}{for} \PY{n}{row} \PY{o+ow}{in} \PY{n}{ax} \PY{k}{for} \PY{n}{sub} \PY{o+ow}{in} \PY{n}{row}\PY{p}{]}
\PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Reconstruction after interpolation in latent space}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{interpolated\PYZus{}0\PYZus{}and\PYZus{}1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{interpolated\PYZus{}0\PYZus{}and\PYZus{}1}\PY{p}{)}

\PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{frac} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{z\PYZus{}interpolated} \PY{o}{=} \PY{n}{frac} \PY{o}{*} \PY{p}{(}\PY{n}{z\PYZus{}j} \PY{o}{\PYZhy{}} \PY{n}{z\PYZus{}i}\PY{p}{)} \PY{o}{+} \PY{n}{z\PYZus{}i}
        \PY{n}{reconstruction\PYZus{}interpolated} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z\PYZus{}interpolated}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{p}{)}
        \PY{n}{ax}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{reconstruction\PYZus{}interpolated}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{ax}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Interpolation }\PY{l+s+si}{\PYZob{}}\PY{n}{frac}\PY{o}{*}\PY{l+m+mi}{100}\PY{l+s+si}{:}\PY{l+s+s1}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556
 0.66666667 0.77777778 0.88888889 1.        ]
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_159_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{137}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{interpolate\PYZus{}between}\PY{p}{(}\PY{n}{model}\PY{p}{:} \PY{n}{Autoencoder}\PY{p}{,} \PY{n}{test\PYZus{}dataset}\PY{p}{:} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Dataset}\PY{p}{,} \PY{n}{idx\PYZus{}i}\PY{p}{:}\PY{n+nb}{int}\PY{p}{,} \PY{n}{idx\PYZus{}j}\PY{p}{:}\PY{n+nb}{int}\PY{p}{,} \PY{n}{n}\PY{p}{:}\PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Plot original images and the reconstruction of the linear interpolation in the latent space embeddings}

\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    model:               Autoencoder}
\PY{l+s+sd}{                         The (trained) autoencoder}
\PY{l+s+sd}{    test\PYZus{}dataset:        torch.utils.data.Dataset}
\PY{l+s+sd}{                         Test images}
\PY{l+s+sd}{    idx\PYZus{}i:               int}
\PY{l+s+sd}{                         Id for first image}
\PY{l+s+sd}{    idx\PYZus{}j:               int}
\PY{l+s+sd}{                         Id for second image}
\PY{l+s+sd}{    n:                   n, optional, default 12}
\PY{l+s+sd}{                         Number of intermediate interpolations (including original reconstructions)}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} setting to visualize original images}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
    \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Original Images}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx\PYZus{}i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx\PYZus{}j}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx\PYZus{}i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx\PYZus{}j}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} obtain the corresponding latent factors }
    \PY{n}{z\PYZus{}i} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx\PYZus{}i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
    \PY{n}{z\PYZus{}j} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{[}\PY{n}{idx\PYZus{}j}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} setting to visualize interpolations}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{n}\PY{o}{/}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{]}\PY{p}{)}
    \PY{n}{ax} \PY{o}{=} \PY{p}{[}\PY{n}{sub} \PY{k}{for} \PY{n}{row} \PY{o+ow}{in} \PY{n}{ax} \PY{k}{for} \PY{n}{sub} \PY{o+ow}{in} \PY{n}{row}\PY{p}{]}
    \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{reconstruction after interpolation in latent space}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{frac} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{n}{z\PYZus{}interpolated} \PY{o}{=} \PY{n}{frac} \PY{o}{*} \PY{p}{(}\PY{n}{z\PYZus{}j} \PY{o}{\PYZhy{}} \PY{n}{z\PYZus{}i}\PY{p}{)} \PY{o}{+} \PY{n}{z\PYZus{}i}
            \PY{n}{reconstruction\PYZus{}interpolated} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{z\PYZus{}interpolated}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{]}\PY{p}{)}
            \PY{n}{ax}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{reconstruction\PYZus{}interpolated}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{n}{ax}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{interpolation }\PY{l+s+si}{\PYZob{}}\PY{n}{frac}\PY{o}{*}\PY{l+m+mi}{100}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{138}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{i}\PY{p}{,} \PY{n}{j} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{139}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{j}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
2186 8131
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{140}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{interpolate\PYZus{}between}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{test\PYZus{}dataset}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{j}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_163_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Dimensionality_Reduction_files/Dimensionality_Reduction_163_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
\end{document}
