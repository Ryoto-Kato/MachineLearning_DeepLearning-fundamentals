{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN (Generative Adversarial Networks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GAN consits of two neural networks\n",
    "    - Discriminator and Generator\n",
    "- __Generator__ is trained to generate the data\n",
    "- __Discriminator__ is trained to distinguished..\n",
    "    - input is generated by generator or real image\n",
    "\n",
    "- Generative model vs Discriminative model\n",
    "    - There are two types of probabilistic model of ML\n",
    "- e.g. Multiple class classification\n",
    "    - Genetative model\n",
    "    - By sampling latent factor from a generative model, you can generate new data.\n",
    "        - Latent factor: data representatives in a latent space\n",
    "        - In multiclass classification problem, generative process is following,\n",
    "            1. sample a class label y\n",
    "            2. sample x from the conditional distribution $p(x \\mid y)$\n",
    "    - Generative model is often used for the unlabled unsupervised learning\n",
    "        - $p(y=c \\mid x, \\Sigma, \\pi, \\mu)=\\frac{p(x \\mid y=c, \\mu_{c}, \\Sigma_{c})p(y=c, \\pi_{c})}{\\sum_{c=1}^{c=C}p(x)_c}$\n",
    "        - $p(x)_c = p(x \\mid y=c, \\mu_{c}, \\Sigma_{c})p(y=c, \\pi_{c})$\n",
    "        - __Softmax__ $p(y=c \\mid x, \\Sigma, \\pi, \\mu)=\\frac{\\exp(W_c*x+W_c0)}{\\sum_{l=1}^{l=C}\\exp(Wl*x+Wl_0)}$\n",
    "            - $W_c = \\Sigma^{-1}\\mu_c$\n",
    "            - $W_c0 = -\\frac{1}{2}\\mu^T\\Sigma_{-1}\\mu_c + \\log{p(y=c)}$\n",
    "    1. Obtain maximum likelihood of $\\Sigma, \\pi, \\mu$\n",
    "    2. Derive the $W_c, W_c0$ by evaluating the above equation\n",
    "    - we assume that we are given the label \n",
    "        - class prior: p(y=c, $\\pi$)\n",
    "        - class conditional: p(x, y=c, $\\mu_{c}$, $\\Sigma_{c}$)\n",
    "            - Gauss distribution\n",
    "            - In Naive Bayes,\n",
    "                - we assume that each attribute is independent each other in the condition(y=c)\n",
    "                - Each dimension can be treated as different distribution\n",
    "                - Hence the $\\Sigma_{c}$ is diagonal and different from each class\n",
    "        - In Linear discriminative analysis\n",
    "            - we assume that every class condition has same covariance\n",
    "            - Hence the $\\Sigma$=$\\Sigma_{c1}$=$\\Sigma_{c2}$\n",
    "        - Generative model should be normalized by the sum of each class likelihood(=__Softmax__)\n",
    "    - Discriminative model: logistic regression\n",
    "        - Let the $W_c, W_c0$ be free variable\n",
    "        - Learn the $W_c, W_c0$ it self directly\n",
    "        - W={W_c={1toC}, W_c0={1toC}} //absorbing bias\n",
    "        - Loss function = Cross Entropy loss\n",
    "        - Minimization problem of the nagative log likelihood\n",
    "            - __Softmax__ \n",
    "                - $p(y=c \\mid x, \\Sigma, \\pi, \\mu)=\\frac{\\exp(W_c*x+W_c0)}{\\sum_{l=1}^{l=C}\\exp(Wl*x+Wl_0)}$\n",
    "            - __Negative Log Likelihood__ \n",
    "                - $-\\log{p(y=c \\mid x, W)}=-\\sum_{i=1}^{N}\\sum_{c=1}^{C}y_ic\\log(\\frac{\\exp(W_c*x+W_c0)}{\\sum_{l=1}^{l=C}\\exp(Wl*x+Wl_0)})$\n",
    "                - $y_ic$ is one hot vector (corresponing to the label column(row) is 1, others are zero)\n",
    "            - $W_c = \\Sigma^{-1}\\mu_c$\n",
    "            - $Wl*x+Wl_0$ is a prediction\n",
    "            - Each presition are normalized by the aggregated exponential of the prediction among classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Architecture of generative adversarial networks\n",
    "    - The role of the __generator__\n",
    "        - __Objective__ try to get better at fooling the discriminator\n",
    "        - Estimate the probability distribution of the real samples in order to provide generated samples resembling real data\n",
    "        - Construct the probability distribution as much similar as possible to the real world data distribution\n",
    "    - The role of the __discriminator__\n",
    "        - __Objective__ try to get better at identifying generated samples\n",
    "        - Estimate the probability that a given sample came from the real data rather tan being provided by the generator (real or fake)\n",
    "    - Generative Adversarial Networks has the two model which are trained to compete with each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "- https://realpython.com/generative-adversarial-networks/#implementing-the-generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toysample\n",
    "- we consider the data which consists of two feature $x_1,x_2$\n",
    "- Especially, we want to estimate the $x_2=\\sin(x_1)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fig_gan.webp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the generator\n",
    "- Args:\n",
    "    - input: random data from a latent space $z=(z_1, z_2)$\n",
    "    - output: generated data which is resembling to the real-data\n",
    "- can be MLP and CNN\n",
    "- e.g. \n",
    "    - Decoder part in Autoencoder\n",
    "    - MLP (Multiple Perceptron)\n",
    "    - CNN (Convolutional Neural Network)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the discriminator\n",
    "- Args:\n",
    "    - input: generated data or sampled data from real-world data distribution\n",
    "    - output: the probability that the inpput belongs to the real-dataset\n",
    "        - high prob: the input is from real-world\n",
    "        - low prob: the input is generated by the generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minmax game\n",
    "- D is adapted to minimize the discrimination error between real and generated samples\n",
    "- G is adapted to maximize the probability of D making a mistake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train discriminator, at each iteration you label some real samples taken from the training data as 1 and some generated samples provided from the generator as 0.\n",
    "We can train the discriminator which can minimize the binary cross entropy as binary classification problem\n",
    "![](fig_train_discriminator.webp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updates of parameter of discriminator, we can train generator to produce better generated samples. The output of generator is connected t D, whose paramters are kept frozen.\n",
    "![](fig_train_generator.webp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can consider this model as single classifier such that\n",
    "- input: random data z\n",
    "- output: probability of $p(y=1 \\mid z)$\n",
    "- label associated with each input: 1\n",
    "    - Every generated data should be classified as class 1 (fool the discriminator)\n",
    "We freeze the parameter of discriminator, using the binary cross entropy as a loss function, we can optimize the paramters of generator such that minimize the loss function (misclassifincation).\n",
    "When generator can do a good enough job, the output probability should be close to 1.\n",
    "\n",
    "- At the end\n",
    "    - __Generator__ will be able to generateo more closely resemble the real data\n",
    "    - __Discriminator__ will have more trouble to distinguish between real and generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
