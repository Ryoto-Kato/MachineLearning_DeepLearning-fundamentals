{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dev = torchvision.datasets.MNIST('./data', train=True, download = True)\n",
    "mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mnist_dev)\n",
    "type(mnist_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dev = mnist_dev.data/255.0\n",
    "print(x_dev.shape)\n",
    "y_dev = mnist_dev.targets\n",
    "print(y_dev.shape)\n",
    "mnist_dev.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable tensor(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable tensor(1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"lable\", y_dev[i])\n",
    "    plt.imshow(x_dev[i], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that setting 'train=True' gives you the development set\n",
    "We need to split dev_data into the training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([50000, 784])\n"
     ]
    }
   ],
   "source": [
    "ntrain = 50_000\n",
    "print(x_dev.shape)\n",
    "x_train, y_train = x_dev[:ntrain].flatten(1), y_dev[:ntrain]\n",
    "print(x_train.shape)\n",
    "x_val, y_val = x_dev[ntrain:].flatten(1), y_dev[ntrain:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier initialization\n",
    "### !!!Do not forget reguries_grad_()\n",
    "- Variance of the weight should be 1/n to preserve input's variance after applying affine operations\n",
    "- Var(Score) = N*Var(weight)*Var(inputs)\n",
    "- If weight is initialized by Uniform distribution, we could do by\n",
    "    - Uniform(-np.sqrt(6/fan_in + fan_out), np.sqrt(6/fan_in + fan_out))\n",
    "- If weight is initialized by Normal distribution, we could do by\n",
    "    - Normal(0, 1/n)\n",
    "    - Normal(0, 2/n) if the activation function is ReLU\n",
    "    - Standard deviation = np.sqrt(2/fan_in+fan_out)\n",
    "    - When we use torch.randn(normal distribution),\n",
    "        - weight = torch.randn(fan_in, fan_out)*np.sqrt(2/fan_in+fan_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0206,  3.9377,  4.5437,  ..., -1.8781, -3.4464,  3.3529],\n",
      "        [ 2.8381, -0.3991,  3.3709,  ...,  3.3104, -0.1115,  0.5206],\n",
      "        [ 1.0674, -0.1607,  1.7583,  ...,  3.3632, -5.6192,  0.8292],\n",
      "        ...,\n",
      "        [ 1.5152,  3.4837,  4.3324,  ..., -4.3804,  0.5320, -1.8538],\n",
      "        [-5.1094,  2.1896, -1.4007,  ..., -5.2565,  0.8538,  2.7718],\n",
      "        [ 2.8041, -2.9996,  0.5703,  ..., -2.4913, -0.3976, -1.1141]],\n",
      "       requires_grad=True)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "num_features = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "weight = torch.randn(num_features, num_classes) * np.sqrt(2/num_features+num_classes)\n",
    "weight.requires_grad_()\n",
    "bias = torch.zeros(num_classes, requires_grad=True)\n",
    "print(weight)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([1, 1, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "print(weight.shape)\n",
    "w = torch.randn([1, 1, 10, 10])\n",
    "print(w.squeeze().shape) ## shrink one dimentions which has one\n",
    "print(w.unsqueeze(0).shape) # expand dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    return log_softmax(input @ weight + bias)\n",
    "def log_softmax(input):\n",
    "    return input - input.exp().sum(-1).log().unsqueeze(-1)\n",
    "def nll_loss(output,target):\n",
    "    return -output[range(target.shape[0]), target].mean()\n",
    "loss_fn = nll_loss\n",
    "\n",
    "def get_accuracy(output, target):\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    return (pred == target).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input = x_train[:batch_size]\n",
    "target = y_train[:batch_size]\n",
    "pred = model(x_train[:batch_size])\n",
    "loss = loss_fn(pred, target)\n",
    "accuracy = get_accuracy(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "tensor(5)\n",
      "tensor(0.1562)\n",
      "tensor(inf, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred_sample = torch.argmax(pred[0], dim=-1)\n",
    "sample_target = target[0]\n",
    "print(pred_sample)\n",
    "print(sample_target)\n",
    "print(accuracy)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.4357, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.functional.cross_entropy combins log_softmax and negative log likelihood\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weight + bias\n",
    "\n",
    "loss_fc = F.cross_entropy\n",
    "pred = model(input)\n",
    "loss = loss_fc(pred, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(inf, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(nll_loss(log_softmax(pred), target))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_features, num_classes) * np.sqrt(2/(num_features + num_classes)))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_classes))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input @ self.weight + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4339, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "pred = model.forward(x_train[:ntrain])\n",
    "print(nll_loss(log_softmax(pred), y_train[:ntrain]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ntrain = #train data samples\n",
    "# batch_size = batch size\n",
    "# loss_fc = F.cross_entropy\n",
    "def train(model, num_epochs, learning_rate):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(np.ceil(ntrain/batch_size))): ## number of samples in minibatch\n",
    "            # Get mini_batch\n",
    "            start_i = i*batch_size\n",
    "            end_i = min(start_i + batch_size, ntrain)\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "\n",
    "            # Get prediction\n",
    "            pred = model(xb)\n",
    "\n",
    "            # Get loss\n",
    "            loss = loss_fc(pred, yb)\n",
    "            \n",
    "            # get gradient\n",
    "            model.zero_grad() # reset gradient of loss function wrt parameters\n",
    "            loss.backward() # compute gradients by backpropagation\n",
    "\n",
    "            # Optimization\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    param -= learning_rate * param.grad\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryotok/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9091)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "train(model, num_epochs=10, learning_rate=0.01)\n",
    "pred = log_softmax(model(x_val))\n",
    "accuracy = get_accuracy(pred, y_val)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "test = x_val[0].reshape(28,28)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(3), array(8), array(6), array(9), array(6), array(4), array(5), array(5), array(8), array(4)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "num_images = 10\n",
    "image = []\n",
    "fig = plt.figure(figsize=(10., 10.))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(2, 5), axes_pad=0.5)\n",
    "preds = []\n",
    "\n",
    "for i in range(num_images):\n",
    "    pred1 = torch.argmax(pred[i], dim=-1).numpy()\n",
    "    gt = y_val[i]\n",
    "    image.append(x_val[i].reshape(28,28))\n",
    "    preds.append(pred1)\n",
    "\n",
    "counter = 0\n",
    "print(preds)\n",
    "\n",
    "for ax, im in zip(grid, image):\n",
    "    ax.set_title(str(preds[counter]))\n",
    "    ax.imshow(im, cmap=\"gray\")\n",
    "    counter = counter + 1\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.lin(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.xavier_normal_(module.weight)\n",
    "        module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc tensor(0.9219)\n",
      "train_loss tensor(0.3224, grad_fn=<NllLossBackward0>)\n",
      "val_acc tensor(0.0938)\n",
      "val_loss tensor(6.0058, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "model.apply(initialize_weight)\n",
    "\n",
    "train(model, num_epochs=10, learning_rate=0.01)\n",
    "\n",
    "loss_func_val = F.cross_entropy\n",
    "\n",
    "train_target = y_train[:batch_size]\n",
    "pred_train =model(x_train[:batch_size])\n",
    "loss_train = loss_func_val(pred_train, train_target)\n",
    "acc_train = get_accuracy(pred_train, train_target)\n",
    "\n",
    "val_target = y_train[:batch_size]\n",
    "pred_val = model(x_val[:batch_size])\n",
    "loss_val = loss_func_val(pred_val, val_target)\n",
    "acc_val = get_accuracy(pred_val, val_target)\n",
    "\n",
    "print(\"train_acc\", acc_train)\n",
    "print(\"train_loss\", loss_train)\n",
    "\n",
    "print(\"val_acc\", acc_val)\n",
    "print(\"val_loss\", loss_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(np.ceil(len(x_train)/batch_size))):\n",
    "            start_i = i * batch_size\n",
    "            end_i = min(start_i+batch_size, len(x_train))\n",
    "            xb=x_train[start_i:end_i]\n",
    "            yb=y_train[start_i:end_i]\n",
    "\n",
    "            #prediction\n",
    "            pred = model(xb)\n",
    "\n",
    "            #evaluate loss\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "\n",
    "            #obtain gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #update params\n",
    "            optimizer.step()\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "model.apply(initialize_weight)\n",
    "\n",
    "train(model, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataset, data_name=\"\"):\n",
    "    if isinstance(dataset, torch.utils.data.TensorDataset):\n",
    "        xb, targets = dataset[:]\n",
    "        pred = model(xb)\n",
    "    else:\n",
    "        print(\"should be Tensordataset\")\n",
    "    loss = F.cross_entropy(pred, targets)\n",
    "    acc = get_accuracy(pred, targets)\n",
    "    print(data_name)\n",
    "    print(\"loss:\" , loss)\n",
    "    print(\"accuracy\", acc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "val_set = TensorDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "loss: tensor(2.3633, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.0941)\n",
      "\n",
      "valdation\n",
      "loss: tensor(2.3610, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.0943)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation(model, train_set, data_name=\"train\")\n",
    "validation(model, val_set, data_name=\"valdation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight1(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.xavier_normal_(module.weight)\n",
    "        module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_set, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(len(train_set)/batch_size)):\n",
    "            #get minibatch\n",
    "            start_i = i * batch_size\n",
    "            end_i = min(start_i+batch_size, len(train_set))\n",
    "            xb, yb = train_set[start_i:end_i]\n",
    "            # print(xb.shape)\n",
    "            #prediction\n",
    "            pred = model(xb)\n",
    "\n",
    "            #loss\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "\n",
    "            #obtain derivatives\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #update parameters\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, train_set, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "loss: tensor(0.5427, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.8620)\n",
      "\n",
      "validation\n",
      "loss: tensor(0.5754, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.8540)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = TensorDataset(x_train[:1000], y_train[:1000])\n",
    "val_set = TensorDataset(x_val[:1000], y_val[:1000])\n",
    "\n",
    "validation(model, train_set, data_name=\"train\")\n",
    "validation(model, val_set, data_name=\"validation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "- You can create a DataLoader for any Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader=DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader=DataLoader(val_set, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)\n",
    "for xb, yb in train_loader:\n",
    "    # print(len(xb))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataloader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for input, target in train_dataloader:\n",
    "            #Generate prediction\n",
    "            pred = model(input)\n",
    "\n",
    "            #loss\n",
    "            loss = F.cross_entropy(pred, target)\n",
    "\n",
    "            #calculate gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #update paramteres\n",
    "            optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "train(model, optimizer, train_loader, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "loss: tensor(1.4688, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.6817)\n",
      "\n",
      "validation\n",
      "loss: tensor(1.4340, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.7118)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = TensorDataset(x_train, y_train)\n",
    "validation_set = TensorDataset(x_val, y_val)\n",
    "validation(model, train_set, data_name=\"train\")\n",
    "validation(model, validation_set, data_name=\"validation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "- Since we do not need backpropagation for the validation set, we can use 2x large batches. NO NEED TO BE SHUFFLE For vallidation data. \n",
    "- We should shuffle training data to avoid correlation between batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(x_train, y_train)\n",
    "validation_set = TensorDataset(x_val, y_val)\n",
    "\n",
    "dataloaders={}\n",
    "dataloaders['train']=DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "dataloaders['val']=DataLoader(validation_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.train() and model.eval()\n",
    "- since batch_normalization or dropout layers are behaving different way in train and evalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloaders, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for input, target in dataloaders['train']:\n",
    "            #prediction\n",
    "            pred = model(input)\n",
    "            #loss\n",
    "            loss = F.cross_entropy(pred, target)\n",
    "            #gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #update\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for input, target in dataloaders['val']:\n",
    "            #prediction\n",
    "            pred = model(input)\n",
    "            #loss\n",
    "            # print(pred.shape)\n",
    "            loss += F.cross_entropy(pred, target).sum()\n",
    "            n_correct += (pred.argmax(-1)==target).sum()\n",
    "            n_samples += len(target)\n",
    "        avg_loss = loss / len(dataloaders['val'])\n",
    "        accuracy = n_correct / n_samples\n",
    "\n",
    "        print(f\"Epoch {epoch}: loss={avg_loss:.3f} accuracy={accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.405 accuracy=0.898\n",
      "Epoch 1: loss=0.351 accuracy=0.906\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, dataloaders, num_epochs=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "- convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output size\n",
    "def output_size(input_size, kernel_size, stride, padding, layers=0):\n",
    "    out=input_size\n",
    "    for i in range(layers):\n",
    "        out= np.floor((out+(2*padding) - kernel_size)/stride) + 1\n",
    "    return int(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size(28, 3, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(1, num_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2=nn.Conv2d(num_channels, num_channels, kernel_size=3,stride=2, padding=1)\n",
    "        self.conv3=nn.Conv2d(num_channels, num_classes, kernel_size=3, stride=2, padding=1)\n",
    "        # self.dense=nn.Linear(num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input.view(-1, 1, 28, 28)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        x = x.squeeze()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n",
      "torch.Size([3, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "test = x_train[1:4]\n",
    "print(test.shape)\n",
    "print(test.view(-1, 1, 28, 28).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48000, 28, 28])\n",
      "torch.Size([6000, 28, 28])\n",
      "torch.Size([6000, 28, 28])\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(num_channels=16, num_classes=10)\n",
    "counter = 0\n",
    "\n",
    "size_train = int(np.floor(x_dev.shape[0] * 0.8))\n",
    "size_val = int(np.floor(x_dev.shape[0] * 0.1))\n",
    "size_test = int(np.floor(x_dev.shape[0] * 0.1))\n",
    "\n",
    "x_train, y_train = x_dev[:size_train], y_dev[:size_train]\n",
    "x_val, y_val = x_dev[size_train:size_train+size_val], y_dev[size_train:size_train+size_val]\n",
    "start_idx = size_train+size_val\n",
    "x_test, y_test = x_dev[start_idx:start_idx+size_test], y_dev[start_idx:start_idx+size_test]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "print(x_train.shape[0] + x_val.shape[0] + x_test.shape[0])\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "val_set = TensorDataset(x_val, y_val)\n",
    "test_set = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "\n",
    "Dataloaders = {}\n",
    "Dataloaders['train']=train_loader\n",
    "Dataloaders['val']=val_loader\n",
    "Dataloaders['test']=test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10])\n",
      "torch.Size([16, 10])\n",
      "tensor([7, 3, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 7, 3, 7])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in Dataloaders['train']:\n",
    "    pred = cnn(xb)    \n",
    "    break\n",
    "print(pred.shape)\n",
    "print(pred.squeeze().shape)\n",
    "print(torch.argmax(pred.squeeze(), dim=-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.405 accuracy=0.878\n",
      "Epoch 1: loss=0.246 accuracy=0.931\n"
     ]
    }
   ],
   "source": [
    "model = CNN(num_channels=128, num_classes=10)\n",
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "train(model, optimizer=optimizer, dataloaders=Dataloaders, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[-0.0883, -0.0627, -0.0628],\n",
       "            [-0.0352, -0.0408, -0.0475],\n",
       "            [-0.0658, -0.0484, -0.0628]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0369,  0.1122,  0.2320],\n",
       "            [ 0.1500,  0.2117,  0.2435],\n",
       "            [ 0.2505,  0.3402,  0.2080]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.4055,  0.4594,  0.3569],\n",
       "            [ 0.3643,  0.3570,  0.3644],\n",
       "            [ 0.1990,  0.1206,  0.0939]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-0.4061, -0.2691, -0.4542],\n",
       "            [-0.1620, -0.1216, -0.2727],\n",
       "            [-0.0642, -0.0707, -0.0775]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0081,  0.0066, -0.0285],\n",
       "            [-0.0057, -0.0035, -0.0399],\n",
       "            [-0.0611,  0.0243,  0.0266]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.5390,  0.5546,  0.6769],\n",
       "            [ 0.4217,  0.4551,  0.3745],\n",
       "            [ 0.1655,  0.2119,  0.1813]]]], requires_grad=True)),\n",
       " ('conv1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.8832e-01,  2.5702e-03,  1.6848e-04, -1.6488e-03, -1.1157e-03,\n",
       "          -2.8542e-03,  5.1973e-03,  1.1122e-01, -2.0756e-02,  2.4857e-03,\n",
       "           8.1234e-02, -9.0892e-04,  4.4379e-02, -7.7207e-03, -1.0226e-02,\n",
       "           1.1841e-03, -1.3704e-02, -8.7291e-03,  3.6546e-02, -2.8544e-03,\n",
       "           1.3612e-01, -2.0133e-03, -1.7001e-03, -9.4259e-07,  4.5534e-03,\n",
       "          -4.6649e-03, -1.0912e-03, -1.0275e-03,  2.0779e-02, -7.0105e-04,\n",
       "           1.9416e-01,  8.9389e-03,  3.0485e-02,  4.7277e-01, -5.8318e-03,\n",
       "           1.7292e-01, -6.4359e-03,  5.4650e-02, -7.7491e-03,  1.6016e-01,\n",
       "           4.0794e-01,  3.1597e-02, -5.2428e-04,  6.9224e-05, -2.2104e-03,\n",
       "           1.9398e-02, -1.2667e-03, -2.0963e-03, -3.3584e-04,  2.5349e-02,\n",
       "           3.1647e-02,  6.3413e-02,  1.6472e-01,  2.6950e-01,  5.6906e-04,\n",
       "           5.0247e-01, -6.1121e-03, -6.9746e-04, -1.6140e-03, -1.2104e-03,\n",
       "          -5.9790e-03, -9.2511e-06, -1.1510e-02,  2.7157e-01,  1.8546e-02,\n",
       "          -6.5660e-03, -6.5370e-04,  3.4849e-02, -2.1167e-02, -4.8719e-04,\n",
       "          -8.6788e-03,  2.9645e-02, -9.3886e-03,  1.7861e-01,  2.2157e-02,\n",
       "          -3.7243e-04, -1.1672e-02,  1.1488e-01, -4.4120e-03,  1.5750e-01,\n",
       "           1.9418e-01,  8.6463e-05, -2.2676e-03, -1.8496e-04, -1.6547e-02,\n",
       "          -1.2747e-03, -1.4904e-02,  2.6065e-02, -2.1665e-03, -1.3471e-02,\n",
       "           1.3237e-03,  6.2344e-01,  1.9728e-01,  1.0156e-02, -1.8793e-03,\n",
       "           1.4727e-04,  2.5052e-03, -6.1549e-04, -4.8184e-03, -1.3005e-02,\n",
       "           4.7364e-01, -2.8552e-03,  2.4462e-01, -1.6191e-02, -7.8798e-04,\n",
       "          -2.5585e-03,  1.2818e-01, -1.2608e-02, -1.0393e-03, -2.8389e-04,\n",
       "          -9.0874e-04, -1.2932e-02, -1.0046e-03, -2.0523e-03, -1.2463e-02,\n",
       "          -6.6486e-04,  8.3736e-03,  8.4931e-03, -1.0052e-03, -7.1904e-03,\n",
       "           1.5750e-03,  4.0951e-01, -9.8774e-03, -1.6376e-03,  2.1764e-02,\n",
       "           7.7418e-01, -1.4058e-02, -1.4954e-03], requires_grad=True)),\n",
       " ('conv2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[ 0.0537, -0.0345, -0.0066],\n",
       "            [-0.0296,  0.0370,  0.0330],\n",
       "            [-0.0260, -0.0197, -0.0314]],\n",
       "  \n",
       "           [[-0.0635, -0.0764, -0.0396],\n",
       "            [-0.0326, -0.0207,  0.0678],\n",
       "            [-0.0663,  0.0217,  0.0861]],\n",
       "  \n",
       "           [[-0.0110, -0.0318, -0.1046],\n",
       "            [-0.0491, -0.0272, -0.0259],\n",
       "            [-0.0493, -0.0044,  0.1097]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0214, -0.0273, -0.0028],\n",
       "            [ 0.0040, -0.0182, -0.0291],\n",
       "            [ 0.0039,  0.0088, -0.0422]],\n",
       "  \n",
       "           [[ 0.0634,  0.0313, -0.0093],\n",
       "            [-0.0020, -0.0486,  0.0411],\n",
       "            [ 0.0017, -0.0086,  0.0258]],\n",
       "  \n",
       "           [[-0.0602, -0.0978, -0.0534],\n",
       "            [-0.1152, -0.0451, -0.0238],\n",
       "            [-0.1033, -0.0061,  0.0858]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0453,  0.0463,  0.0028],\n",
       "            [ 0.0336,  0.0022,  0.0033],\n",
       "            [ 0.0699,  0.0303, -0.0096]],\n",
       "  \n",
       "           [[-0.0405,  0.0419,  0.0489],\n",
       "            [-0.0311, -0.0203,  0.0040],\n",
       "            [-0.0340, -0.0062,  0.0145]],\n",
       "  \n",
       "           [[-0.0362, -0.0193,  0.0293],\n",
       "            [-0.0165,  0.0124,  0.0090],\n",
       "            [-0.0026,  0.0606, -0.0380]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.1122,  0.0497,  0.0828],\n",
       "            [ 0.1402,  0.0425,  0.1061],\n",
       "            [ 0.1360,  0.0979,  0.0981]],\n",
       "  \n",
       "           [[-0.0534, -0.0131,  0.0194],\n",
       "            [ 0.0255, -0.0046, -0.0013],\n",
       "            [ 0.0166, -0.0427,  0.0289]],\n",
       "  \n",
       "           [[-0.0330, -0.0582,  0.0582],\n",
       "            [-0.0785,  0.0329, -0.0534],\n",
       "            [-0.0588, -0.0246, -0.0445]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0138, -0.0227, -0.0713],\n",
       "            [-0.0034, -0.0172,  0.0349],\n",
       "            [-0.0289, -0.0197, -0.0066]],\n",
       "  \n",
       "           [[-0.0198, -0.0454,  0.0405],\n",
       "            [-0.0592, -0.0085,  0.0042],\n",
       "            [ 0.0374,  0.0554,  0.0487]],\n",
       "  \n",
       "           [[-0.0213, -0.0388,  0.0358],\n",
       "            [-0.0506, -0.0532, -0.0164],\n",
       "            [ 0.0324, -0.0251,  0.0393]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0256, -0.0605, -0.0624],\n",
       "            [ 0.0057, -0.0850, -0.0552],\n",
       "            [ 0.0041, -0.0146, -0.0157]],\n",
       "  \n",
       "           [[-0.0290, -0.0075, -0.0024],\n",
       "            [ 0.0078,  0.0430,  0.0422],\n",
       "            [-0.0489,  0.0363,  0.0411]],\n",
       "  \n",
       "           [[-0.0721, -0.0818,  0.0123],\n",
       "            [-0.0850,  0.0517,  0.0209],\n",
       "            [-0.0189,  0.0250,  0.0213]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 0.0294,  0.0017,  0.0189],\n",
       "            [ 0.0002,  0.0190,  0.0880],\n",
       "            [ 0.0228, -0.0188,  0.0044]],\n",
       "  \n",
       "           [[ 0.0303,  0.0438,  0.0332],\n",
       "            [ 0.0714,  0.0960,  0.0126],\n",
       "            [-0.0657, -0.0352, -0.0683]],\n",
       "  \n",
       "           [[ 0.0554,  0.0484,  0.0023],\n",
       "            [ 0.0410,  0.0718,  0.1058],\n",
       "            [-0.0221, -0.1086, -0.0696]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0275,  0.0053,  0.0036],\n",
       "            [-0.0520, -0.0369, -0.0558],\n",
       "            [ 0.0038,  0.0144,  0.0016]],\n",
       "  \n",
       "           [[-0.0340,  0.0021,  0.0366],\n",
       "            [ 0.0243,  0.0066, -0.0258],\n",
       "            [ 0.0014, -0.0076, -0.0262]],\n",
       "  \n",
       "           [[-0.0073,  0.0387,  0.0542],\n",
       "            [-0.0385,  0.1050,  0.0631],\n",
       "            [-0.0851,  0.0080, -0.0267]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0371, -0.0035, -0.0546],\n",
       "            [-0.0035,  0.0081,  0.0275],\n",
       "            [-0.0108, -0.0254,  0.0170]],\n",
       "  \n",
       "           [[-0.0141, -0.0612,  0.0321],\n",
       "            [ 0.0170, -0.0017,  0.0096],\n",
       "            [-0.0068, -0.0136, -0.0284]],\n",
       "  \n",
       "           [[ 0.0007, -0.0693,  0.0377],\n",
       "            [ 0.0359, -0.0291,  0.0408],\n",
       "            [ 0.0264,  0.0475, -0.0085]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0104, -0.0264, -0.0293],\n",
       "            [ 0.0100,  0.0148, -0.0320],\n",
       "            [-0.0394, -0.0085, -0.0200]],\n",
       "  \n",
       "           [[-0.0207,  0.0039,  0.0299],\n",
       "            [-0.0021, -0.0302, -0.0169],\n",
       "            [ 0.0649,  0.0371,  0.0167]],\n",
       "  \n",
       "           [[ 0.0501, -0.0511,  0.0300],\n",
       "            [-0.0145, -0.0531,  0.0313],\n",
       "            [-0.0463,  0.0382, -0.0241]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0214,  0.0385,  0.0109],\n",
       "            [ 0.0007, -0.0419,  0.0074],\n",
       "            [ 0.0002, -0.0476, -0.0423]],\n",
       "  \n",
       "           [[-0.0050,  0.0118, -0.0237],\n",
       "            [-0.0102, -0.0346, -0.0165],\n",
       "            [ 0.0065, -0.0172, -0.0198]],\n",
       "  \n",
       "           [[-0.0050,  0.0107,  0.0023],\n",
       "            [ 0.0407, -0.0007, -0.0232],\n",
       "            [-0.0069,  0.0151,  0.0185]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0116, -0.0769, -0.0531],\n",
       "            [-0.1283, -0.0443, -0.0022],\n",
       "            [-0.0454, -0.0466, -0.0505]],\n",
       "  \n",
       "           [[-0.0529, -0.0460,  0.0323],\n",
       "            [ 0.0122,  0.0704, -0.0245],\n",
       "            [-0.0554, -0.0107,  0.0113]],\n",
       "  \n",
       "           [[-0.0571,  0.0191,  0.0179],\n",
       "            [ 0.0439,  0.0306, -0.0575],\n",
       "            [-0.0025, -0.0432,  0.0443]]]], requires_grad=True)),\n",
       " ('conv2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 9.7754e-02,  2.7115e-01, -4.2182e-02,  6.0523e-02, -1.3488e-01,\n",
       "          -1.8705e-02,  4.6743e-02, -1.4953e-02, -2.2198e-02, -1.1766e-02,\n",
       "          -3.2180e-02, -8.6223e-03, -7.1500e-02, -3.0156e-02, -4.0587e-02,\n",
       "          -1.9170e-02,  4.1237e-02, -8.3426e-02, -1.6633e-01, -1.6453e-02,\n",
       "          -5.5595e-02, -4.5911e-02,  3.1194e-01, -2.0765e-02, -5.8968e-03,\n",
       "           1.7164e-02, -5.1465e-02, -4.7663e-02,  4.5061e-02, -9.7113e-03,\n",
       "          -7.8014e-02, -4.0468e-02, -1.1285e-02,  4.5141e-02, -6.5875e-02,\n",
       "          -3.2760e-03, -5.9709e-02,  3.4236e-02,  3.1601e-02, -2.7533e-02,\n",
       "          -1.9293e-02,  2.5357e-03, -7.6473e-02, -1.0732e-01, -2.3047e-03,\n",
       "          -2.5630e-02, -8.4882e-02,  1.0043e-03, -6.5674e-03, -4.2552e-02,\n",
       "          -2.6956e-02, -5.0653e-02, -3.1610e-02,  1.4293e-02, -4.9388e-06,\n",
       "          -1.0113e-01,  3.5184e-02,  6.7670e-02,  5.8488e-04,  2.2640e-02,\n",
       "           7.8866e-03,  5.2718e-01, -2.2908e-02, -1.6990e-02, -4.2466e-02,\n",
       "           4.5631e-02, -1.1977e-02, -6.1136e-02,  7.0130e-02, -4.3327e-02,\n",
       "           3.0841e-01,  3.4280e-02, -8.3207e-02,  1.0459e-01, -1.7068e-02,\n",
       "           2.9210e-02, -3.8099e-03, -2.9603e-02, -7.8041e-02,  1.3140e-01,\n",
       "           2.8726e-03, -4.7563e-02,  5.9989e-02, -4.3899e-02,  5.0054e-03,\n",
       "          -2.7353e-02, -4.1366e-02, -3.8835e-02, -4.5570e-02, -6.6536e-03,\n",
       "          -2.3390e-02, -6.8146e-02,  3.8822e-02,  1.8885e-03,  1.7376e-01,\n",
       "          -2.7887e-02,  9.6220e-02, -4.6487e-02,  2.1116e-02,  8.5176e-02,\n",
       "           1.1070e-01, -5.3485e-02,  2.6808e-02, -2.7555e-02, -8.7955e-02,\n",
       "           1.0215e-03, -1.8452e-02, -4.5408e-02, -1.3050e-01,  4.7301e-02,\n",
       "          -1.3618e-01,  1.4324e-03,  1.5524e-01,  3.1504e-02,  2.2065e-02,\n",
       "           5.7501e-02,  4.9451e-02, -1.0718e-01, -2.2815e-03, -1.6280e-01,\n",
       "           1.6473e-03, -7.3760e-02, -5.3529e-02, -4.0166e-02,  2.1702e-02,\n",
       "          -2.1203e-02, -2.3994e-02, -7.0834e-02], requires_grad=True)),\n",
       " ('conv3.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[-0.2976, -0.3313, -0.0056],\n",
       "            [-0.1399,  0.5222,  0.3161],\n",
       "            [-0.2226,  0.2245, -0.1094]],\n",
       "  \n",
       "           [[-0.2954,  0.0504, -0.3575],\n",
       "            [-0.0233, -0.0553, -0.3547],\n",
       "            [-0.3468, -0.1967, -0.3278]],\n",
       "  \n",
       "           [[-0.1256, -0.2715, -0.0271],\n",
       "            [-0.1893,  0.3351,  0.4213],\n",
       "            [-0.0354,  0.2482,  0.0296]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.1484, -0.0965, -0.1949],\n",
       "            [-0.2832,  0.0753,  0.1317],\n",
       "            [ 0.2519, -0.1327,  0.0553]],\n",
       "  \n",
       "           [[-0.0179,  0.0577,  0.0373],\n",
       "            [ 0.0649,  0.0079,  0.0193],\n",
       "            [ 0.0423,  0.0755,  0.0231]],\n",
       "  \n",
       "           [[-0.0934, -0.0119,  0.0529],\n",
       "            [ 0.0195, -0.1591,  0.0207],\n",
       "            [ 0.1721, -0.0572, -0.2187]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2939,  0.1202, -0.0353],\n",
       "            [ 0.1573, -0.0168,  0.0359],\n",
       "            [-0.0985, -0.0820,  0.1948]],\n",
       "  \n",
       "           [[ 0.1321,  0.0307,  0.0711],\n",
       "            [ 0.0363, -0.1857, -0.1210],\n",
       "            [ 0.0661,  0.0305,  0.1022]],\n",
       "  \n",
       "           [[ 0.2490, -0.1492, -0.1351],\n",
       "            [ 0.1506, -0.0552, -0.1277],\n",
       "            [ 0.0525, -0.3358,  0.1288]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.1277, -0.0546, -0.1342],\n",
       "            [-0.3305, -0.0395, -0.2151],\n",
       "            [-0.0803, -0.1230, -0.0642]],\n",
       "  \n",
       "           [[ 0.0501,  0.0092, -0.0294],\n",
       "            [-0.0642, -0.0250, -0.0384],\n",
       "            [-0.0332, -0.0607,  0.0742]],\n",
       "  \n",
       "           [[ 0.0855,  0.1051, -0.0384],\n",
       "            [ 0.0723, -0.0553, -0.0255],\n",
       "            [ 0.1297,  0.0678,  0.1135]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0236,  0.0061, -0.0530],\n",
       "            [ 0.2063,  0.1111,  0.0973],\n",
       "            [ 0.2790, -0.1339, -0.2311]],\n",
       "  \n",
       "           [[-0.0569, -0.1550, -0.1948],\n",
       "            [ 0.2411,  0.2990,  0.1030],\n",
       "            [-0.2080, -0.2244,  0.0346]],\n",
       "  \n",
       "           [[ 0.1761,  0.0765, -0.0673],\n",
       "            [ 0.0614,  0.0601,  0.0433],\n",
       "            [ 0.1894,  0.0509, -0.2171]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.1014, -0.1098, -0.2658],\n",
       "            [ 0.1448, -0.1709, -0.0954],\n",
       "            [ 0.0367,  0.2800,  0.4436]],\n",
       "  \n",
       "           [[ 0.0248,  0.0387,  0.0587],\n",
       "            [ 0.0099,  0.0026,  0.0401],\n",
       "            [ 0.0256, -0.0407,  0.1050]],\n",
       "  \n",
       "           [[-0.1945, -0.1786, -0.3203],\n",
       "            [ 0.1253,  0.0662,  0.1226],\n",
       "            [ 0.1333,  0.1346,  0.0060]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 0.0940, -0.0476,  0.1472],\n",
       "            [ 0.1849,  0.0992,  0.0575],\n",
       "            [ 0.1589, -0.0685, -0.0166]],\n",
       "  \n",
       "           [[ 0.0532,  0.0501,  0.1116],\n",
       "            [ 0.0824,  0.0326, -0.0996],\n",
       "            [-0.2568, -0.3142, -0.0698]],\n",
       "  \n",
       "           [[ 0.1420,  0.0243,  0.2258],\n",
       "            [ 0.1871,  0.0781, -0.0222],\n",
       "            [ 0.0482, -0.1973, -0.0651]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0680, -0.0045,  0.0022],\n",
       "            [-0.0504,  0.0075,  0.1872],\n",
       "            [-0.0772,  0.0724, -0.1256]],\n",
       "  \n",
       "           [[-0.0508,  0.0259,  0.0303],\n",
       "            [-0.0854, -0.0793, -0.0279],\n",
       "            [-0.0656, -0.0571, -0.1047]],\n",
       "  \n",
       "           [[-0.0984, -0.1032, -0.0332],\n",
       "            [ 0.0358,  0.1587,  0.0803],\n",
       "            [-0.0791, -0.1005, -0.2349]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.3837, -0.1439, -0.4193],\n",
       "            [ 0.1186,  0.2172, -0.2399],\n",
       "            [ 0.1145, -0.1254, -0.2992]],\n",
       "  \n",
       "           [[ 0.1597, -0.3060, -0.2096],\n",
       "            [ 0.4323, -0.0168, -0.0211],\n",
       "            [-0.0209, -0.3359, -0.0377]],\n",
       "  \n",
       "           [[-0.0898,  0.0512, -0.1726],\n",
       "            [ 0.0047,  0.2657, -0.0520],\n",
       "            [ 0.0635,  0.2121, -0.1197]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.1974, -0.0280, -0.0308],\n",
       "            [-0.1593,  0.0259,  0.2694],\n",
       "            [-0.0954, -0.1134, -0.0883]],\n",
       "  \n",
       "           [[ 0.1453,  0.1170,  0.0661],\n",
       "            [-0.0092, -0.0286, -0.0344],\n",
       "            [ 0.0582,  0.0844,  0.0156]],\n",
       "  \n",
       "           [[ 0.0224,  0.1506,  0.0882],\n",
       "            [ 0.0019, -0.0472,  0.0074],\n",
       "            [-0.0268,  0.0301,  0.0057]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2795, -0.0889, -0.0875],\n",
       "            [ 0.1927, -0.0324, -0.1856],\n",
       "            [-0.0753,  0.1782,  0.0957]],\n",
       "  \n",
       "           [[-0.1841, -0.0980, -0.0543],\n",
       "            [-0.0384,  0.0168, -0.1032],\n",
       "            [ 0.0718,  0.1787,  0.1118]],\n",
       "  \n",
       "           [[ 0.2056,  0.0979,  0.0920],\n",
       "            [ 0.2125, -0.4067, -0.1845],\n",
       "            [ 0.0819,  0.2222,  0.2442]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0597, -0.1005, -0.0134],\n",
       "            [ 0.0208, -0.1599, -0.0124],\n",
       "            [-0.0435, -0.3437, -0.3907]],\n",
       "  \n",
       "           [[ 0.0439,  0.0697,  0.1627],\n",
       "            [-0.0398,  0.0948,  0.0214],\n",
       "            [ 0.0437,  0.0561,  0.0691]],\n",
       "  \n",
       "           [[-0.0624,  0.0044,  0.0036],\n",
       "            [ 0.0729,  0.1315, -0.0865],\n",
       "            [ 0.1124,  0.0447,  0.0710]]]], requires_grad=True)),\n",
       " ('conv3.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.1055,  0.1765,  0.1645, -0.1818, -0.0292, -0.0334, -0.0204,  0.1093,\n",
       "           0.1194,  0.2648], requires_grad=True))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Seqential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, input):\n",
    "        return self.func(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_train[1:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(lambda x: x.view(-1, 1, 28, 28)),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.squeeze()) #Lambda(lambda x; x.view(x.size(0), -1)))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor .size(0) and .shape[0] are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 10])\n",
      "torch.Size([9, 10])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "a = model(test)\n",
    "b = model(test).view(model(test).size(0), -1)\n",
    "print(b.shape)\n",
    "print(a.shape)\n",
    "print(a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[-0.0639,  0.2737, -0.0738],\n",
       "            [-0.1488,  0.1226,  0.2101],\n",
       "            [ 0.1182, -0.1770,  0.0682]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2570, -0.2778,  0.2929],\n",
       "            [ 0.2386, -0.2516, -0.1345],\n",
       "            [ 0.2905,  0.0814,  0.2240]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0532, -0.2056,  0.2611],\n",
       "            [-0.1866,  0.0569,  0.1888],\n",
       "            [-0.1658, -0.0391,  0.0711]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1566, -0.3144, -0.1780],\n",
       "            [-0.1841, -0.0106, -0.2804],\n",
       "            [ 0.1300,  0.2352,  0.1680]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0504,  0.2105,  0.0293],\n",
       "            [-0.3181,  0.0132,  0.3163],\n",
       "            [ 0.1882, -0.3285, -0.0822]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0541,  0.0759, -0.1477],\n",
       "            [-0.3218,  0.1885,  0.1393],\n",
       "            [-0.1431,  0.1127, -0.2148]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0135, -0.0331, -0.0598],\n",
       "            [-0.1724, -0.3239,  0.2779],\n",
       "            [-0.3112, -0.2360,  0.1066]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1613, -0.0332, -0.0900],\n",
       "            [ 0.1245,  0.1906, -0.2698],\n",
       "            [-0.0943,  0.0214, -0.1457]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0405, -0.2771, -0.2917],\n",
       "            [ 0.1541, -0.1963,  0.1574],\n",
       "            [-0.1860, -0.0882, -0.3236]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2741, -0.3223, -0.2562],\n",
       "            [-0.2178, -0.1208, -0.2089],\n",
       "            [-0.2215,  0.1687, -0.0541]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2996, -0.0034, -0.0916],\n",
       "            [-0.1129, -0.1695,  0.1829],\n",
       "            [ 0.0777,  0.2709, -0.1675]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.3153, -0.1770,  0.2830],\n",
       "            [-0.2933,  0.2951, -0.2822],\n",
       "            [-0.2374, -0.0489, -0.2054]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1498, -0.3333,  0.0709],\n",
       "            [-0.2205,  0.3020, -0.2657],\n",
       "            [ 0.1007, -0.1885,  0.1883]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2333,  0.1546,  0.1730],\n",
       "            [ 0.2800, -0.0690,  0.1098],\n",
       "            [ 0.2381,  0.0255,  0.2652]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1375, -0.0321, -0.0404],\n",
       "            [-0.1839,  0.0706,  0.1902],\n",
       "            [ 0.0811,  0.1849,  0.1637]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0091,  0.2694,  0.1237],\n",
       "            [-0.1334, -0.1686,  0.1672],\n",
       "            [-0.1508,  0.2540, -0.0282]]]], requires_grad=True)),\n",
       " ('1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0862,  0.0311, -0.2186,  0.2044,  0.2826,  0.2456,  0.1836, -0.3233,\n",
       "          -0.2158,  0.2008,  0.1971,  0.2908,  0.2795, -0.0035, -0.1370,  0.2314],\n",
       "         requires_grad=True)),\n",
       " ('3.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[ 4.5558e-03, -3.9231e-02, -2.1380e-02],\n",
       "            [ 5.8731e-02, -6.4551e-03,  5.6936e-03],\n",
       "            [ 6.5197e-03, -4.2106e-02, -8.0029e-02]],\n",
       "  \n",
       "           [[-7.2533e-03, -6.4618e-02, -4.0995e-03],\n",
       "            [ 6.5475e-03, -7.0716e-02,  4.8236e-02],\n",
       "            [ 2.3477e-02,  1.3016e-02, -1.5481e-02]],\n",
       "  \n",
       "           [[-5.7941e-02,  1.1653e-02, -2.5050e-02],\n",
       "            [-3.9680e-02, -1.4664e-02,  5.8929e-02],\n",
       "            [ 4.4961e-02,  3.6820e-02,  3.8557e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-1.3668e-03,  1.6834e-03, -2.7950e-02],\n",
       "            [ 6.0521e-02, -2.9448e-03, -6.3741e-02],\n",
       "            [ 4.9046e-02,  2.7533e-02, -3.9390e-02]],\n",
       "  \n",
       "           [[ 2.5225e-02,  2.0343e-02, -4.9969e-03],\n",
       "            [ 6.9379e-02,  1.7993e-02, -6.3493e-02],\n",
       "            [ 2.9632e-02, -1.2999e-03,  3.4984e-02]],\n",
       "  \n",
       "           [[-7.7389e-03,  3.9333e-02, -1.6760e-02],\n",
       "            [-1.6291e-02, -4.8532e-02,  2.4977e-02],\n",
       "            [ 2.1533e-02,  6.3267e-03, -6.1153e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[ 7.7372e-03, -7.3772e-02, -7.3038e-02],\n",
       "            [-4.8898e-02,  6.3378e-02,  1.8403e-02],\n",
       "            [ 7.4683e-02, -5.0204e-02, -1.3053e-02]],\n",
       "  \n",
       "           [[-1.8994e-02, -1.1294e-02, -3.5982e-02],\n",
       "            [-1.5332e-02,  5.3726e-02,  4.4835e-03],\n",
       "            [-1.3436e-02, -6.7461e-02,  2.7598e-02]],\n",
       "  \n",
       "           [[-6.8571e-02,  6.3151e-02,  6.3117e-02],\n",
       "            [ 7.7620e-02, -2.6657e-02, -3.8280e-02],\n",
       "            [-7.2365e-03, -2.7585e-02, -2.4212e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 1.0722e-02, -6.6351e-02,  1.7246e-02],\n",
       "            [-3.8834e-02, -7.6560e-02,  5.7730e-02],\n",
       "            [ 6.0382e-02, -5.7553e-02,  1.1577e-02]],\n",
       "  \n",
       "           [[ 1.1909e-03, -2.3758e-02, -5.3321e-02],\n",
       "            [ 5.0686e-02,  7.0015e-02, -4.3369e-02],\n",
       "            [-6.7585e-02, -1.4761e-02,  5.0075e-02]],\n",
       "  \n",
       "           [[-6.3091e-02,  6.8248e-02, -1.4469e-02],\n",
       "            [-8.0155e-02, -6.7390e-02,  3.7771e-03],\n",
       "            [ 3.5403e-03, -7.2808e-02,  3.3176e-03]]],\n",
       "  \n",
       "  \n",
       "          [[[ 3.2953e-02, -3.5667e-02, -5.3867e-02],\n",
       "            [-4.8672e-02,  3.6995e-02, -6.8075e-02],\n",
       "            [ 6.9930e-02,  6.6712e-02,  6.3882e-02]],\n",
       "  \n",
       "           [[ 6.7857e-02, -1.1844e-02, -6.9127e-02],\n",
       "            [ 3.0424e-02, -4.6668e-02, -2.4513e-02],\n",
       "            [ 8.2600e-02,  6.6662e-02,  4.1886e-02]],\n",
       "  \n",
       "           [[-4.6771e-02, -7.9842e-02,  1.4019e-02],\n",
       "            [ 6.7043e-02, -5.9112e-02, -1.6367e-02],\n",
       "            [-2.8712e-02, -6.6714e-02,  1.1895e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-7.4363e-02,  7.8690e-03, -6.2555e-02],\n",
       "            [ 7.5736e-02, -6.3473e-02, -7.9896e-04],\n",
       "            [ 3.9966e-02, -3.2211e-02,  3.8494e-03]],\n",
       "  \n",
       "           [[ 8.1832e-02, -8.0743e-02,  1.1450e-02],\n",
       "            [ 4.9588e-02, -5.2444e-02, -2.3050e-02],\n",
       "            [ 7.4687e-03,  4.7381e-02,  2.0511e-02]],\n",
       "  \n",
       "           [[ 3.6441e-02,  1.0211e-02,  6.5320e-02],\n",
       "            [-3.6424e-03, -6.8805e-02, -1.6533e-02],\n",
       "            [ 4.7313e-02, -4.4194e-02,  4.3954e-02]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 7.3427e-02,  2.8346e-02,  4.6591e-02],\n",
       "            [-6.5492e-02,  4.2497e-02,  3.3434e-03],\n",
       "            [-7.4763e-02,  5.0872e-03,  4.4860e-02]],\n",
       "  \n",
       "           [[-4.4867e-02, -2.2753e-03,  1.9693e-02],\n",
       "            [-2.5893e-02, -3.8523e-02,  3.9604e-03],\n",
       "            [-1.9457e-02, -7.9549e-02,  6.3908e-04]],\n",
       "  \n",
       "           [[-4.2205e-02, -3.0546e-02, -6.3773e-02],\n",
       "            [-6.6263e-02, -1.4176e-02,  2.8058e-02],\n",
       "            [ 5.7706e-03, -3.7457e-02, -4.1410e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-3.0167e-02, -4.4606e-02, -2.2216e-02],\n",
       "            [ 5.0828e-02, -7.7838e-02, -5.3486e-02],\n",
       "            [-2.9878e-03,  5.6832e-02, -6.7786e-02]],\n",
       "  \n",
       "           [[ 1.1724e-02,  7.9146e-02,  3.4183e-02],\n",
       "            [-4.9997e-02, -7.7471e-02,  1.0114e-02],\n",
       "            [-5.7923e-02,  4.0646e-02, -2.5935e-02]],\n",
       "  \n",
       "           [[ 7.7202e-02, -4.9961e-02, -5.0882e-05],\n",
       "            [-1.6257e-02,  8.1106e-02, -3.6470e-02],\n",
       "            [-1.9072e-02,  5.8596e-02,  4.0367e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[ 5.4081e-02, -7.3195e-05,  5.5872e-02],\n",
       "            [-2.1067e-02, -5.3076e-02,  2.1769e-02],\n",
       "            [-8.0483e-02, -4.8533e-03,  4.4791e-02]],\n",
       "  \n",
       "           [[-7.4529e-02,  1.2173e-02, -4.8863e-02],\n",
       "            [ 2.6081e-02, -2.5508e-02,  3.8820e-02],\n",
       "            [ 3.2601e-02,  8.1682e-02,  3.7004e-02]],\n",
       "  \n",
       "           [[-5.9452e-03,  5.4732e-02, -4.7141e-02],\n",
       "            [ 7.5378e-03,  7.3791e-02,  3.8641e-02],\n",
       "            [ 2.7334e-04, -2.0806e-02, -4.0709e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-5.9087e-02,  6.6307e-02, -1.3823e-02],\n",
       "            [-6.7401e-02,  2.5685e-02, -3.1445e-02],\n",
       "            [-6.8162e-02,  6.1694e-02,  6.3394e-02]],\n",
       "  \n",
       "           [[-4.6222e-03,  7.5438e-02,  2.7916e-02],\n",
       "            [-6.0207e-02, -2.8190e-03,  3.8347e-02],\n",
       "            [-4.9746e-02, -4.1732e-02, -1.8758e-02]],\n",
       "  \n",
       "           [[ 6.0908e-02, -8.3180e-02,  8.1521e-02],\n",
       "            [ 1.9350e-02, -3.8344e-02,  5.2892e-02],\n",
       "            [ 4.6644e-02, -3.0819e-02, -4.5017e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[ 3.9244e-02, -1.0708e-02, -4.9428e-02],\n",
       "            [-7.8141e-02,  1.9582e-02, -3.7334e-02],\n",
       "            [ 7.8956e-02, -7.3757e-02,  3.8705e-02]],\n",
       "  \n",
       "           [[ 4.5641e-02,  4.5127e-02,  7.9031e-02],\n",
       "            [ 4.2689e-02, -5.1209e-02, -2.1186e-02],\n",
       "            [ 7.2089e-02,  2.4964e-02,  4.5122e-02]],\n",
       "  \n",
       "           [[-1.8128e-02,  6.1140e-02, -2.8658e-02],\n",
       "            [-3.2849e-02, -4.5066e-02,  3.0773e-02],\n",
       "            [ 7.2628e-02,  1.1673e-02,  4.3228e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 3.2453e-03,  4.3954e-02,  5.8267e-02],\n",
       "            [ 4.2177e-02, -1.6624e-03, -4.0689e-02],\n",
       "            [ 2.7113e-02,  6.7944e-02,  8.3049e-02]],\n",
       "  \n",
       "           [[-7.8179e-02,  3.2497e-02, -1.9443e-02],\n",
       "            [ 5.1219e-02, -4.3125e-02,  6.1634e-02],\n",
       "            [-4.3127e-03,  1.3342e-02, -7.3654e-02]],\n",
       "  \n",
       "           [[ 1.5000e-02,  5.3895e-02,  2.0243e-03],\n",
       "            [ 8.2188e-02,  4.4148e-02, -1.7620e-02],\n",
       "            [ 5.2099e-02,  3.8611e-02, -2.0726e-02]]]], requires_grad=True)),\n",
       " ('3.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0174,  0.0154,  0.0180,  0.0728, -0.0326, -0.0372,  0.0278,  0.0320,\n",
       "          -0.0195, -0.0684, -0.0442, -0.0803, -0.0467, -0.0015,  0.0672,  0.0446],\n",
       "         requires_grad=True)),\n",
       " ('5.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[ 0.0656, -0.0561,  0.0753],\n",
       "            [ 0.0482, -0.0551, -0.0546],\n",
       "            [ 0.0004, -0.0188, -0.0285]],\n",
       "  \n",
       "           [[ 0.0747,  0.0443,  0.0226],\n",
       "            [ 0.0175, -0.0152,  0.0015],\n",
       "            [-0.0415,  0.0273, -0.0160]],\n",
       "  \n",
       "           [[ 0.0474,  0.0047,  0.0080],\n",
       "            [-0.0697,  0.0173,  0.0293],\n",
       "            [ 0.0498,  0.0828,  0.0668]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0690, -0.0283,  0.0585],\n",
       "            [-0.0640,  0.0084, -0.0615],\n",
       "            [ 0.0335,  0.0435,  0.0554]],\n",
       "  \n",
       "           [[ 0.0481,  0.0238,  0.0618],\n",
       "            [-0.0774, -0.0424,  0.0542],\n",
       "            [ 0.0655,  0.0751, -0.0133]],\n",
       "  \n",
       "           [[ 0.0443, -0.0205,  0.0003],\n",
       "            [ 0.0364, -0.0677, -0.0442],\n",
       "            [ 0.0237,  0.0464, -0.0162]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0168,  0.0411,  0.0383],\n",
       "            [ 0.0793, -0.0692,  0.0125],\n",
       "            [ 0.0727,  0.0252, -0.0645]],\n",
       "  \n",
       "           [[ 0.0652,  0.0175,  0.0679],\n",
       "            [ 0.0002, -0.0251,  0.0241],\n",
       "            [-0.0659, -0.0507,  0.0599]],\n",
       "  \n",
       "           [[ 0.0816,  0.0779, -0.0014],\n",
       "            [ 0.0074,  0.0517,  0.0086],\n",
       "            [ 0.0424,  0.0163, -0.0244]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0568,  0.0659,  0.0335],\n",
       "            [-0.0427,  0.0154,  0.0810],\n",
       "            [ 0.0534,  0.0178, -0.0310]],\n",
       "  \n",
       "           [[ 0.0397,  0.0139,  0.0529],\n",
       "            [-0.0080,  0.0769,  0.0002],\n",
       "            [-0.0779, -0.0819,  0.0058]],\n",
       "  \n",
       "           [[-0.0679, -0.0554,  0.0589],\n",
       "            [ 0.0459, -0.0289, -0.0009],\n",
       "            [ 0.0131, -0.0668, -0.0504]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0029,  0.0588, -0.0086],\n",
       "            [-0.0329,  0.0328, -0.0722],\n",
       "            [ 0.0064, -0.0173,  0.0238]],\n",
       "  \n",
       "           [[-0.0153, -0.0141,  0.0389],\n",
       "            [-0.0665, -0.0413, -0.0571],\n",
       "            [ 0.0103,  0.0018, -0.0199]],\n",
       "  \n",
       "           [[ 0.0819, -0.0660,  0.0488],\n",
       "            [-0.0809,  0.0180,  0.0108],\n",
       "            [-0.0804, -0.0568,  0.0706]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0054,  0.0629, -0.0451],\n",
       "            [ 0.0451,  0.0773,  0.0476],\n",
       "            [-0.0787,  0.0458,  0.0693]],\n",
       "  \n",
       "           [[ 0.0661, -0.0506,  0.0283],\n",
       "            [ 0.0393, -0.0436, -0.0818],\n",
       "            [-0.0424, -0.0408, -0.0729]],\n",
       "  \n",
       "           [[ 0.0111,  0.0328,  0.0045],\n",
       "            [ 0.0216,  0.0484, -0.0816],\n",
       "            [ 0.0374, -0.0426,  0.0791]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-0.0414,  0.0671,  0.0797],\n",
       "            [-0.0113, -0.0170,  0.0244],\n",
       "            [-0.0317, -0.0813, -0.0422]],\n",
       "  \n",
       "           [[-0.0461, -0.0329, -0.0184],\n",
       "            [-0.0123,  0.0680, -0.0007],\n",
       "            [ 0.0098, -0.0162,  0.0628]],\n",
       "  \n",
       "           [[ 0.0146,  0.0728,  0.0113],\n",
       "            [ 0.0640, -0.0464,  0.0006],\n",
       "            [-0.0392, -0.0260,  0.0336]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0441, -0.0083, -0.0363],\n",
       "            [-0.0159,  0.0299,  0.0411],\n",
       "            [ 0.0426, -0.0287,  0.0604]],\n",
       "  \n",
       "           [[ 0.0796, -0.0765, -0.0291],\n",
       "            [-0.0062, -0.0434,  0.0296],\n",
       "            [-0.0151,  0.0370,  0.0657]],\n",
       "  \n",
       "           [[ 0.0747,  0.0832,  0.0137],\n",
       "            [-0.0522, -0.0093,  0.0437],\n",
       "            [-0.0629, -0.0532, -0.0778]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0344, -0.0046,  0.0740],\n",
       "            [ 0.0191, -0.0406,  0.0020],\n",
       "            [-0.0547,  0.0198, -0.0616]],\n",
       "  \n",
       "           [[-0.0149,  0.0068,  0.0535],\n",
       "            [-0.0009,  0.0226,  0.0118],\n",
       "            [-0.0368, -0.0640,  0.0233]],\n",
       "  \n",
       "           [[-0.0326, -0.0043, -0.0734],\n",
       "            [-0.0363,  0.0018,  0.0675],\n",
       "            [-0.0031,  0.0602,  0.0031]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0400,  0.0324,  0.0831],\n",
       "            [ 0.0380,  0.0517,  0.0705],\n",
       "            [-0.0346, -0.0821, -0.0696]],\n",
       "  \n",
       "           [[-0.0813, -0.0050, -0.0615],\n",
       "            [-0.0395,  0.0584,  0.0333],\n",
       "            [-0.0597, -0.0527,  0.0822]],\n",
       "  \n",
       "           [[ 0.0040,  0.0307,  0.0391],\n",
       "            [ 0.0182,  0.0171,  0.0349],\n",
       "            [ 0.0414, -0.0816,  0.0045]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0736, -0.0757, -0.0339],\n",
       "            [-0.0680, -0.0023,  0.0089],\n",
       "            [ 0.0075, -0.0735, -0.0172]],\n",
       "  \n",
       "           [[-0.0573, -0.0700,  0.0004],\n",
       "            [ 0.0159, -0.0388, -0.0744],\n",
       "            [ 0.0451, -0.0143,  0.0388]],\n",
       "  \n",
       "           [[ 0.0506,  0.0469, -0.0627],\n",
       "            [ 0.0556,  0.0518,  0.0243],\n",
       "            [ 0.0560,  0.0437, -0.0695]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0257, -0.0164, -0.0250],\n",
       "            [-0.0293, -0.0798,  0.0100],\n",
       "            [ 0.0812, -0.0713, -0.0696]],\n",
       "  \n",
       "           [[-0.0037,  0.0375, -0.0581],\n",
       "            [ 0.0116,  0.0623,  0.0034],\n",
       "            [-0.0065,  0.0501, -0.0177]],\n",
       "  \n",
       "           [[-0.0781, -0.0728, -0.0592],\n",
       "            [ 0.0680,  0.0515, -0.0367],\n",
       "            [ 0.0183, -0.0544, -0.0244]]]], requires_grad=True)),\n",
       " ('5.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0412,  0.0317,  0.0258,  0.0154, -0.0421, -0.0786,  0.0340,  0.0198,\n",
       "           0.0314,  0.0668], requires_grad=True))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.509 accuracy=0.852\n",
      "Epoch 1: loss=0.363 accuracy=0.888\n"
     ]
    }
   ],
   "source": [
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)\n",
    "\n",
    "train(model, optimizer=optimizer, dataloaders=Dataloaders, num_epochs=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of GPUs for learning\n",
    "- We should be careful if our all sets for deep learning are existing on GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Lambda()\n",
       "  (1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (2): ReLU()\n",
       "  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): Conv2d(16, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (6): ReLU()\n",
       "  (7): AdaptiveAvgPool2d(output_size=1)\n",
       "  (8): Lambda()\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move our model to the device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataloaders = {}\n",
    "Dataloaders['train'] = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "Dataloaders['val'] = DataLoader(val_set, batch_size=16, shuffle=False)\n",
    "Dataloaders['test'] = DataLoader(test_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloaders, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # training\n",
    "        model.train()\n",
    "        for input, target in dataloaders['train']:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            pred = model(input)\n",
    "            loss = F.cross_entropy(pred, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for input, target in dataloaders['val']:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            pred = model(input)\n",
    "            loss += F.cross_entropy(pred, target)\n",
    "\n",
    "            n_correct += (pred.argmax(-1) == target).sum()\n",
    "            n_samples += len(dataloaders['val'])\n",
    "\n",
    "        avg_loss = loss/len(dataloaders['val'])\n",
    "        accuracy = n_correct/n_samples\n",
    "\n",
    "        print(f\"Epoch {epoch}: loss={avg_loss :.3f} accuracy={accuracy :.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.584 accuracy=0.035\n",
      "Epoch 1: loss=0.383 accuracy=0.038\n"
     ]
    }
   ],
   "source": [
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "\n",
    "train(model, optimizer=optimizer, dataloaders=Dataloaders, num_epochs=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "- runtime: Drop the input (set weight to zero) = Drop neurons randomly by drop out rate (p=self.p)\n",
    "    - Scale the not dropped neurons output by $\\frac{1}{1-p}$\n",
    "        - To conserve input mean\n",
    "- validation: Use all neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_mean: \n",
      "-0.15959778428077698\n",
      "\n",
      "dropped_out_mean:\n",
      "-0.049686409533023834\n",
      "\n",
      "is_closed_enough?:  False\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(10, 10)\n",
    "print(\"input_mean: \")\n",
    "input_mean=input.mean()\n",
    "print(input_mean.item())\n",
    "\n",
    "mask = torch.zeros_like(input)\n",
    "p = 0.2\n",
    "mask.bernoulli_(p)\n",
    "scale = 1 / (1-p)\n",
    "droped_out = scale * mask * input\n",
    "print()\n",
    "print(\"dropped_out_mean:\")\n",
    "droped_out_mean=droped_out.mean()\n",
    "print(droped_out_mean.item())\n",
    "\n",
    "print()\n",
    "#chenck the conservation of the mean\n",
    "print(\"is_closed_enough?: \", np.isclose(input_mean.item(), droped_out_mean.item(), atol=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    # p:float, drop probability\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input:Pytorch tensor, arbitrary shape\n",
    "        Returns:\n",
    "            Pytorch tensor, same shape as input (Droped out)\n",
    "        \"\"\"\n",
    "            \n",
    "        if self.training:\n",
    "            mask = torch.zeros_like(input)\n",
    "            mask.bernoulli_(1-self.p) # set 1 to the not dropped input\n",
    "            scaling = 1 / (1 - self.p)\n",
    "            dropped_out = scaling * input * mask\n",
    "        return dropped_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00298185832798481\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "#Test dropout\n",
    "test = torch.randn(10_000)\n",
    "dropout = Dropout(0.5)\n",
    "test_droped_out = dropout(test)\n",
    "\n",
    "# Obtain item in the tensor\n",
    "print(test.mean().item())\n",
    "print(type(test.mean().item()))\n",
    "\n",
    "assert np.isclose(test.mean().item(), test_droped_out.mean().item(), atol=1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "- Batch normalization is a trick to obtain smooth loss landscape and improve training by preserving the variance of input as much as possible\n",
    "\n",
    "- It is defined as the funciton\n",
    "    $$y=\\frac{x-\\mu_x}{\\sigma_x + \\epsilon} \\cdot \\gamma + \\beta$$\n",
    "    - $\\mu_x$: mean of input over the dimension c\n",
    "    - $\\sigma_x$: standard deviation(squareroot of variance) of input over the dimension c\n",
    "    - $\\epsilon$: numerical stability helper\n",
    "    - $\\gamma$: learnable paramter to undo the normalization (=>$\\sigma_x$)\n",
    "    - $\\beta$: learnable paramter to undo the normalization (=>$\\mu_x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9234, -0.4081,  0.0358,  1.5079, -0.4827,  0.9039,  0.2910, -0.4540,\n",
      "        -0.0461, -0.9964])\n",
      "tensor([ 0.9234, -0.4081,  0.0358,  1.5079, -0.4827,  0.9039,  0.2910, -0.4540,\n",
      "        -0.0461, -0.9964])\n"
     ]
    }
   ],
   "source": [
    "# elemetn-wise multiplication\n",
    "# Since the gamma and beta are having only num_features elements, \n",
    "# we need to add dimention to N and L\n",
    "num_features = 10\n",
    "dummy_gamma=torch.randn(num_features)\n",
    "print(dummy_gamma)\n",
    "augmented=dummy_gamma[None, :, None]\n",
    "print(augmented[0, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    \"\"\" \n",
    "    Only uses batch statistics (no running mean for evaluation).\n",
    "    Batch statistics are calculated for a single dimension (*input features)\n",
    "    Gamma is initialized as 1, beta as 0\n",
    "\n",
    "    Args: num_features: number of feature to calculate batch statistics for.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Model freely undo the normalization.\n",
    "        Complete-undo brings\n",
    "            Gamma -> standard deviation (=square root of the variance)\n",
    "            Beta -> mean of input\n",
    "        \"\"\"\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Batch normalization over the dimension C of (N, C, L)\n",
    "\n",
    "        Args:\n",
    "            input: Pytorch tensor, shape[N, C, L]\n",
    "            N: number of minibatch samples\n",
    "            C: number of channels (input's dimension)\n",
    "            L: shape of the one channel\n",
    "                -RGB image: L=(H, W)\n",
    "        Return:\n",
    "            Pytorch tensor, same shape as input\n",
    "        \"\"\"\n",
    "\n",
    "        eps = 1e-5 #helper for numerical stability\n",
    "\n",
    "        aggregate_dims = [0, 2]\n",
    "        mean = torch.mean(input, dim=aggregate_dims, keepdim=True)\n",
    "        std = torch.std(input, dim=aggregate_dims, keepdim=True)\n",
    "\n",
    "        input_normalized=(input-mean) / (std + eps)\n",
    "        return self.gamma[None, :, None] * input_normalized + self.beta[None, :, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batch normalization\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "test = torch.randn(8, 2, 4)\n",
    "\n",
    "b1 = BatchNorm(2)\n",
    "test_b1 = b1(test)\n",
    "\n",
    "# nn.BatchNorm1d\n",
    "    # affine\n",
    "    #   True >this module has leranable parameter\n",
    "    # track_running_stats\n",
    "    #   True >track running mean and standard deviation\n",
    "\n",
    "b2 = nn.BatchNorm1d(2, affine=False, track_running_stats=False)\n",
    "test_b2 = b2(test)\n",
    "\n",
    "# torch.allclose \n",
    "    # checks if all input and other satisfy the condition\n",
    "    # elementwise, for all elements of input and other\n",
    "    # rtol > relattive tolerance\n",
    "    # atol > absolute tolerance \n",
    "assert torch.allclose(test_b1, test_b2, rtol=2*1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet\n",
    "- Resnet is the first network which is introduced residual connections\n",
    "- Skip connection can solve problems such as vanishing and exploding gradients as the network gets deeper and deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func=func\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the stride is more than 1 or the in_channels is not equal to out_channels, we need to modify the shape of the input to add the second convolution output and before the final relu activation.\n",
    "\n",
    "In this case, we should care the stride size and the subtraction between out_channel and in_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n",
      "torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "stride = 2\n",
    "x = torch.randn(1, 1, 4, 4)\n",
    "print(x.shape)\n",
    "_x=x[:, :, ::stride, ::stride]\n",
    "print(_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtraction 16\n",
      "original channel 1\n",
      "torch.Size([1, 17, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "out_cha=32\n",
    "in_cha=16\n",
    "print(\"subtraction\", out_cha-in_cha)\n",
    "print(\"original channel\", x.shape[1])\n",
    "\n",
    "# F.pad configuration\n",
    "# padding setting = (padding_left, right, top, bottom, front, back)\n",
    "\n",
    "padded_inpput = F.pad(_x, (0, 0, 0, 0, 0, out_cha-in_cha), mode=\"constant\", value=0) \n",
    "print(padded_inpput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The residual block used by ResNet\n",
    "\n",
    "    Args:\n",
    "        in_channels: The number of channels (feature map) of the incoming embedding\n",
    "        out_channels: The number of channels after the first convolution\n",
    "        stride: Stride size of th first convolution, used for downsampling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        if stride > 1 or in_channels != out_channels:\n",
    "            #Add strides in the skip connection and zeros for the new channels\n",
    "            self.skip = Lambda(lambda x: F.pad(x[:, :, ::stride, ::stride],\n",
    "             (0,0,0,0,0,out_channels - in_channels), mode=\"constant\", value=0))\n",
    "        else:\n",
    "            self.skip = nn.Sequential()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # bias term in the convolution layer will be erased by the batch-normalization\n",
    "        self.conv2=nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x1 = F.relu(self.bn1(self.conv1(input)))\n",
    "        x2 = self.bn2(self.conv2(x1))\n",
    "        return F.relu(x2 + self.skip(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual blocks\n",
    "\n",
    "    Args:\n",
    "        in_channels: The number of channels of the incomming embedding\n",
    "        out_channels: The number of channels of the outgoing embedding\n",
    "        stride: Stride size a the first convolution later for the downsampling\n",
    "        num_blocks: Number of residual blocks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, num_blocks):\n",
    "        super().__init__()\n",
    "        blocks = [ResidualBlock(in_channels, out_channels, stride=stride)]\n",
    "        for _ in range(num_blocks-1):\n",
    "            blocks.append(ResidualBlock(out_channels, out_channels))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Structure\n",
    "- a covolition latyer is always containing\n",
    "    - Convolution layer\n",
    "    - Batch Normalization\n",
    "    - ReLU activation\n",
    "- The each residual block has two comvolution layer with 1 skip connection\n",
    "- We should __squeeze__ the final output to feed it to the fully-connected(dense) layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Architectures\n",
    "- Args:\n",
    "    - input: 32x32 images\n",
    "    - Per-pixel mean subtracted\n",
    "1. 3x3 Conv layer (input_cha=3, out cha=16)\n",
    "    - Batch\n",
    "    - ReLU activation\n",
    "2. __SAME__ Residual stack (num_blocks = n, input_cha=16, out_channel=16)\n",
    "    - Residual block (1st, stride=1)\n",
    "        - 3x3 Same Convolution (Spacial reduction: 32->32)\n",
    "        - Batch\n",
    "        - ReLU\n",
    "    - Residual block (2st-n)\n",
    "        - 3x3 Same Convolution\n",
    "        - Batch\n",
    "        - ReLU\n",
    "3. __VALID__ Residual stach (num_blocks=n, input_cha=16, out_cha=32)\n",
    "    - Residual block (1st, strider=2)\n",
    "        - 3x3 Valid Convolution (Spacial reduction: 32->16)\n",
    "        - Batch\n",
    "        - ReLU\n",
    "    - Residual block (2st-n)\n",
    "        - 3x3 Same Convolution\n",
    "        - Batch\n",
    "        - ReLU\n",
    "4. __VALID__ Residual stach (num_blocks=n, input_cha=32, out_cha=64)\n",
    "    - Residual block (1st, strider=2)\n",
    "        - 3x3 Valid Convolution (Spacial reduction: 16->8)\n",
    "        - Batch\n",
    "        - ReLU\n",
    "    - Residual block (2st-n)\n",
    "        - 3x3 Same Convolution\n",
    "        - Batch\n",
    "        - ReLU\n",
    "5. Average Pooling to reduce spacial dimension from 8x8 to 1x1 per channel\n",
    "6. Lambda squeeze the output and flatten\n",
    "7. Fully connected NN (input_dim=64, out_dim=num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "num_classes = 10\n",
    "\n",
    "resnet = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(num_features=16),\n",
    "    nn.ReLU(),\n",
    "    ResidualStack(in_channels=16, out_channels=16, stride=1, num_blocks=n),\n",
    "    ResidualStack(in_channels=16, out_channels=32, stride=2, num_blocks=n),\n",
    "    ResidualStack(in_channels=32, out_channels=64, stride=2, num_blocks=n),\n",
    "    nn.AdaptiveAvgPool2d(output_size=1),\n",
    "    Lambda(lambda x: x.squeeze()),\n",
    "    nn.Linear(in_features=64, out_features=num_classes)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the module(layer) weight and bias\n",
    "- We need to initialize the weights of our model appropriately\n",
    "- module.weight, module.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Lambda()\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (5): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Lambda()\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (6): AdaptiveAvgPool2d(output_size=1)\n",
       "  (7): Lambda()\n",
       "  (8): Linear(in_features=64, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize_weight_Resnet(module): ##intilalize each layer(module)\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        nn.init.constant_(module.weight, 1) #initialize gamma with 1\n",
    "        nn.init.constant_(module.bias, 0) #initialize beta with 0\n",
    "\n",
    "resnet.apply(initialize_weight_Resnet)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of resnet\n",
    "### Data loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define data download class from the dataset server\n",
    "- Define the transformation\n",
    "    - mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]\n",
    "    - Use precomputed mean and standard deviation of respective channel\n",
    "- Down load the data using the transmormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Subset(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"\n",
    "    Ger a subset of the CIFAR10 dataset provided thougth torch.vision\n",
    "    return the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, idx=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        if idx is None:\n",
    "            return\n",
    "        \n",
    "        self.data=self.data[idx]\n",
    "        target_np = np.array(self.targets)\n",
    "        self.targets=target_np[idx].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation in precomputing\n",
    "- We need to make the model be robust to wide variety of the transformation\n",
    "- Usually a real-world image does not contain the full-body image of object\n",
    "- Cropping is good data augmentation for the occulusion\n",
    "- Horizonfilp is good data augmentation foe the robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "ntrain = 45_000\n",
    "## creating dataset for dataloader\n",
    "train_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain), download=True, transform=transform_train)\n",
    "val_set = CIFAR10Subset(root='./data', train=True, idx = range(ntrain, 50_000), download=True, transform=transform_eval)\n",
    "test_set = CIFAR10Subset(root='./data', train=False, download=True, transform=transform_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size torch.Size([3, 32, 32])\n",
      "target label 6\n"
     ]
    }
   ],
   "source": [
    "print(\"image size\", train_set[0][0].shape)\n",
    "print(\"target label\", train_set[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "def PredandImage_visuallization(pred, x_val):\n",
    "    transform_topilimage = transforms.ToPILImage()\n",
    "    num_images = 10\n",
    "    image = []\n",
    "    fig = plt.figure(figsize=(30., 30.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(2, 5), axes_pad=0.5)\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        pred1 = torch.argmax(pred[i], dim=-1).numpy()\n",
    "        gt = y_val[i]\n",
    "        input_image = transform_topilimage(x_val[i])\n",
    "        # print(x_val[i].shape)\n",
    "        image.append(input_image)\n",
    "        preds.append(pred1)\n",
    "\n",
    "    counter = 0\n",
    "    print(preds)\n",
    "\n",
    "    for ax, im in zip(grid, image):\n",
    "        ax.set_title(str(preds[counter]))\n",
    "        ax.imshow(im)\n",
    "        counter = counter + 1\n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def Image_Grid_visuallization(dataset):\n",
    "    transform_topilimage = transforms.ToPILImage()\n",
    "    num_images = 10\n",
    "    images = []\n",
    "    fig = plt.figure(figsize=(10., 10.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(2, 5), axes_pad=0.5)\n",
    "    targets = []\n",
    "\n",
    "    for i in range(num_images):\n",
    "        ima, target = transform_topilimage(dataset[i][0]), dataset[i][1]\n",
    "        images.append(ima)\n",
    "        targets.append(target)\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for ax, im in zip(grid, images):\n",
    "        ax.set_title(str(targets[counter]))\n",
    "        ax.imshow(im)\n",
    "        counter = counter + 1\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data loader\n",
    "# By enabling the pinned memory transfer, GPU pinns the certain memory partition and it can be accessed by the host device efficiently with low latency\n",
    "dataloaders = {}\n",
    "dataloaders['train']=torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "dataloaders['val']=torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "dataloaders['test']=torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we push the model to our GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Lambda()\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (5): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Lambda()\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (6): AdaptiveAvgPool2d(output_size=1)\n",
       "  (7): Lambda()\n",
       "  (8): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "resnet.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to define a helper that does one epoch of training or evaluation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, dataloaders, train):\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation\n",
    "\n",
    "    Args:\n",
    "        model:The model used for prediction\n",
    "        optimizer: Optimization algorithm for the model\n",
    "        dataloader: iterable data loader providing the data to run our model on \n",
    "        train: Flag if we run for training or evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "\n",
    "    epoch_loss=0.0\n",
    "    epoch_acc=0.0\n",
    "\n",
    "    # Iterate over data\n",
    "    for xb, yb in dataloaders:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(train):\n",
    "            pred = model(xb)\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "            top1 = torch.argmax(pred, dim=1)\n",
    "            ncrorrect = torch.sum(top1==yb)\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += ncrorrect.item()\n",
    "\n",
    "    epoch_loss /= len(dataloaders.dataset)\n",
    "    epoch_acc /= len(dataloaders.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement a method for fitting our model\n",
    "For many models early stopping can save a lot of training time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience):\n",
    "    \"\"\"\n",
    "    Fit the given model on the dataset\n",
    "\n",
    "    Args:\n",
    "        model: The model used for prediction\n",
    "        optimizer: OPtimization algorithm for the model\n",
    "        lr_schedular: Learning rate scheduler that improved training in late epochs with learning rate decay\n",
    "        dataloaders: Dataloaders for training and validation\n",
    "        max_epochs: Maximum Number of epochs for training\n",
    "        patience: Number of epochs to wait with early stopping the training if validation loss has decreased\n",
    "\n",
    "    Returns:\n",
    "        Loss and accuracy in this epoch\n",
    "    \"\"\"\n",
    "\n",
    "    best_acc = 0\n",
    "    curr_partience =0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss, train_acc = run_epoch(model, optimizer, dataloaders['train'], train=True)\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\")\n",
    "\n",
    "        val_loss, val_acc = run_epoch(model, None, dataloaders['val'], train=False)\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, val loss: {val_loss:.2e}, accuracy: {val_acc * 100:.2f}%\")\n",
    "\n",
    "        if val_acc >= best_acc:\n",
    "            best_epoch = epoch\n",
    "            best_acc = val_acc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if epoch-best_epoch >= patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/10, train loss: 1.55e-02, accuracy: 28.26%\n",
      "Epoch   1/10, val loss: 1.26e-02, accuracy: 41.36%\n",
      "Epoch   2/10, train loss: 1.12e-02, accuracy: 47.30%\n",
      "Epoch   2/10, val loss: 1.08e-02, accuracy: 51.22%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb Cell 119\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lr_scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mMultiStepLR(optimizer, milestones\u001b[39m=\u001b[39m[\u001b[39m100\u001b[39m, \u001b[39m150\u001b[39m], gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train(resnet, optimizer, lr_scheduler, dataloaders, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "\u001b[1;32m/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb Cell 119\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m curr_partience \u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m run_epoch(model, optimizer, dataloaders[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m >3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mmax_epochs\u001b[39m}\u001b[39;00m\u001b[39m, train loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.2e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb Cell 119\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, optimizer, dataloaders, train)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     ncrorrect \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(top1\u001b[39m==\u001b[39myb)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mif\u001b[39;00m train:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y224sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "\n",
    "# train the model\n",
    "train(resnet, optimizer, lr_scheduler, dataloaders, max_epochs=10, patience=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.6e-03, accuracy: 80.52%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = run_epoch(resnet, None, dataloaders['test'], train=False)\n",
    "print(f\"Test loss: {test_loss:.1e}, accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    }
   ],
   "source": [
    "print(test_loader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_set=[]\n",
    "counter = 0\n",
    "for _, dataset in enumerate(test_set):\n",
    "    if counter > 9:\n",
    "        break\n",
    "    _test_set.append(dataset)\n",
    "    counter=counter+1\n",
    "\n",
    "_test_loader = torch.utils.data.DataLoader(_test_set, batch_size=10, shuffle=None, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_MAPPING_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb Cell 123\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y233sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m, target \u001b[39min\u001b[39;00m _test_loader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y233sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39minput\u001b[39m, target \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y233sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     pred\u001b[39m=\u001b[39mresnet(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y233sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print(pred.shape)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y233sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pred_sample\u001b[39m=\u001b[39mpred\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR"
     ]
    }
   ],
   "source": [
    "for input, target in _test_loader:\n",
    "    input, target = input.to(device), target.to(device)\n",
    "    pred=resnet(input)\n",
    "\n",
    "# print(pred.shape)\n",
    "pred_sample=pred.cpu()\n",
    "pred_input=input.cpu()\n",
    "print(pred_sample.shape)\n",
    "print(pred_input.shape)\n",
    "PredandImage_visuallization(pred_sample, pred_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary `torch`\n",
    "- `torch.tensor`: PyTorch tensors work like numpy arrays, but can remember gradients and be sent to GPUs\n",
    "- `torch.nn`\n",
    "    - `torch.nn.funciotnal` : Provides various useful functions (non stateful) for training neural networks, e.g. activation functions and loss functions\n",
    "    - `nn.Module` : Subclass from this to create a callable that acts like a function, but can remember stat. It knows what parameters and submodules it contains and privides various functionality based on that\n",
    "    - `nn.Parameters` : Wraps a tensor and tells the containing Module that it needs updating dusring backpropagation\n",
    "    - `torch.nn` : Many useful layers are already implemented in this library e.g. nn.Linear, nn.Conv2d\n",
    "    - `nn.Sequential` : Provides an easy way of defining purely stacked modules\n",
    "\n",
    "- `torch.optim` : Optimizers such as SGD or Adam, which let you easily update and train the paramters inside the passed model\n",
    "\n",
    "- `Dataset(Tensordataset)` : interface for data using only the __len__ and __getitem__ functions. Tensors can be converetd into a Dataset by using Tensor Dataset\n",
    "\n",
    "- `DataLoader` : Takes any Dataset and provides an iterator for returning mini-bathces with various advanced functionality\n",
    " \n",
    "- `GPU` : To use your GPU you need to move your model and each mini-batch to your GPU using __.to(devide)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6b7a25b8b2426eb550c8146811605d9bdb61f23d99ada7f1c3a4f71b608de8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
