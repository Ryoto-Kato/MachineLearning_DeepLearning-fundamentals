{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dev = torchvision.datasets.MNIST('./data', train=True, download = True)\n",
    "mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mnist_dev)\n",
    "type(mnist_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dev = mnist_dev.data/255.0\n",
    "print(x_dev.shape)\n",
    "y_dev = mnist_dev.targets\n",
    "print(y_dev.shape)\n",
    "mnist_dev.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable tensor(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable tensor(1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"lable\", y_dev[i])\n",
    "    plt.imshow(x_dev[i], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that setting 'train=True' gives you the development set\n",
    "We need to split dev_data into the training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([50000, 784])\n"
     ]
    }
   ],
   "source": [
    "ntrain = 50_000\n",
    "print(x_dev.shape)\n",
    "x_train, y_train = x_dev[:ntrain].flatten(1), y_dev[:ntrain]\n",
    "print(x_train.shape)\n",
    "x_val, y_val = x_dev[ntrain:].flatten(1), y_dev[ntrain:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier initialization\n",
    "### !!!Do not forget reguries_grad_()\n",
    "- Variance of the weight should be 1/n to preserve input's variance after applying affine operations\n",
    "- Var(Score) = N*Var(weight)*Var(inputs)\n",
    "- If weight is initialized by Uniform distribution, we could do by\n",
    "    - Uniform(-np.sqrt(6/fan_in + fan_out), np.sqrt(6/fan_in + fan_out))\n",
    "- If weight is initialized by Normal distribution, we could do by\n",
    "    - Normal(0, 1/n)\n",
    "    - Normal(0, 2/n) if the activation function is ReLU\n",
    "    - Standard deviation = np.sqrt(2/fan_in+fan_out)\n",
    "    - When we use torch.randn(normal distribution),\n",
    "        - weight = torch.randn(fan_in, fan_out)*np.sqrt(2/fan_in+fan_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.4624,  0.6131,  3.9426,  ...,  3.0239, -0.0944, -2.7930],\n",
      "        [ 1.5956, -0.0139,  0.0781,  ..., -1.4710, -0.6141, -0.7576],\n",
      "        [ 3.1354,  0.0481,  0.6660,  ...,  2.8552, -3.2162,  2.3031],\n",
      "        ...,\n",
      "        [ 6.6017, -2.9145,  4.4924,  ..., -1.9962, -4.4572,  3.2287],\n",
      "        [-4.8538, -1.2173,  3.6854,  ..., -2.5184,  3.0058, -0.2853],\n",
      "        [ 3.2863,  6.5322,  3.6511,  ..., -1.5077,  7.9040,  6.6984]],\n",
      "       requires_grad=True)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "num_features = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "weight = torch.randn(num_features, num_classes) * np.sqrt(2/num_features+num_classes)\n",
    "weight.requires_grad_()\n",
    "bias = torch.zeros(num_classes, requires_grad=True)\n",
    "print(weight)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([1, 1, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "print(weight.shape)\n",
    "w = torch.randn([1, 1, 10, 10])\n",
    "print(w.squeeze().shape) ## shrink one dimentions which has one\n",
    "print(w.unsqueeze(0).shape) # expand dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    return log_softmax(input @ weight + bias)\n",
    "def log_softmax(input):\n",
    "    return input - input.exp().sum(-1).log().unsqueeze(-1)\n",
    "def nll_loss(output,target):\n",
    "    return -output[range(target.shape[0]), target].mean()\n",
    "loss_fn = nll_loss\n",
    "\n",
    "def get_accuracy(output, target):\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    return (pred == target).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input = x_train[:batch_size]\n",
    "target = y_train[:batch_size]\n",
    "pred = model(x_train[:batch_size])\n",
    "loss = loss_fn(pred, target)\n",
    "accuracy = get_accuracy(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n",
      "tensor(5)\n",
      "tensor(0.1094)\n",
      "tensor(44.8779, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred_sample = torch.argmax(pred[0], dim=-1)\n",
    "sample_target = target[0]\n",
    "print(pred_sample)\n",
    "print(sample_target)\n",
    "print(accuracy)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44.8779, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.functional.cross_entropy combins log_softmax and negative log likelihood\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weight + bias\n",
    "\n",
    "loss_fc = F.cross_entropy\n",
    "pred = model(input)\n",
    "loss = loss_fc(pred, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44.8779, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(nll_loss(log_softmax(pred), target))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_features, num_classes) * np.sqrt(2/(num_features + num_classes)))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_classes))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input @ self.weight + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4087, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "pred = model.forward(x_train[:ntrain])\n",
    "print(nll_loss(log_softmax(pred), y_train[:ntrain]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ntrain = #train data samples\n",
    "# batch_size = batch size\n",
    "# loss_fc = F.cross_entropy\n",
    "def train(model, num_epochs, learning_rate):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(np.ceil(ntrain/batch_size))): ## number of samples in minibatch\n",
    "            # Get mini_batch\n",
    "            start_i = i*batch_size\n",
    "            end_i = min(start_i + batch_size, ntrain)\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "\n",
    "            # Get prediction\n",
    "            pred = model(xb)\n",
    "\n",
    "            # Get loss\n",
    "            loss = loss_fc(pred, yb)\n",
    "            \n",
    "            # get gradient\n",
    "            model.zero_grad() # reset gradient of loss function wrt parameters\n",
    "            loss.backward() # compute gradients by backpropagation\n",
    "\n",
    "            # Optimization\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    param -= learning_rate * param.grad\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryotok/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9086)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "train(model, num_epochs=10, learning_rate=0.01)\n",
    "pred = log_softmax(model(x_val))\n",
    "accuracy = get_accuracy(pred, y_val)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "test = x_val[0].reshape(28,28)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(3), array(8), array(6), array(9), array(6), array(9), array(5), array(5), array(8), array(4)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "num_images = 10\n",
    "image = []\n",
    "fig = plt.figure(figsize=(10., 10.))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(2, 5), axes_pad=0.5)\n",
    "preds = []\n",
    "\n",
    "for i in range(num_images):\n",
    "    pred1 = torch.argmax(pred[i], dim=-1).numpy()\n",
    "    gt = y_val[i]\n",
    "    image.append(x_val[i].reshape(28,28))\n",
    "    preds.append(pred1)\n",
    "\n",
    "counter = 0\n",
    "print(preds)\n",
    "\n",
    "for ax, im in zip(grid, image):\n",
    "    ax.set_title(str(preds[counter]))\n",
    "    ax.imshow(im, cmap=\"gray\")\n",
    "    counter = counter + 1\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.lin(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.xavier_normal_(module.weight)\n",
    "        module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc tensor(0.9219)\n",
      "train_loss tensor(0.3241, grad_fn=<NllLossBackward0>)\n",
      "val_acc tensor(0.0938)\n",
      "val_loss tensor(6.0697, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "model.apply(initialize_weight)\n",
    "\n",
    "train(model, num_epochs=10, learning_rate=0.01)\n",
    "\n",
    "loss_func_val = F.cross_entropy\n",
    "\n",
    "train_target = y_train[:batch_size]\n",
    "pred_train =model(x_train[:batch_size])\n",
    "loss_train = loss_func_val(pred_train, train_target)\n",
    "acc_train = get_accuracy(pred_train, train_target)\n",
    "\n",
    "val_target = y_train[:batch_size]\n",
    "pred_val = model(x_val[:batch_size])\n",
    "loss_val = loss_func_val(pred_val, val_target)\n",
    "acc_val = get_accuracy(pred_val, val_target)\n",
    "\n",
    "print(\"train_acc\", acc_train)\n",
    "print(\"train_loss\", loss_train)\n",
    "\n",
    "print(\"val_acc\", acc_val)\n",
    "print(\"val_loss\", loss_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(np.ceil(len(x_train)/batch_size))):\n",
    "            start_i = i * batch_size\n",
    "            end_i = min(start_i+batch_size, len(x_train))\n",
    "            xb=x_train[start_i:end_i]\n",
    "            yb=y_train[start_i:end_i]\n",
    "\n",
    "            #prediction\n",
    "            pred = model(xb)\n",
    "\n",
    "            #evaluate loss\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "\n",
    "            #obtain gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #update params\n",
    "            optimizer.step()\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "model.apply(initialize_weight)\n",
    "\n",
    "train(model, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataset, data_name=\"\"):\n",
    "    if isinstance(dataset, torch.utils.data.TensorDataset):\n",
    "        xb, targets = dataset[:]\n",
    "        pred = model(xb)\n",
    "    else:\n",
    "        print(\"should be Tensordataset\")\n",
    "    loss = F.cross_entropy(pred, targets)\n",
    "    acc = get_accuracy(pred, targets)\n",
    "    print(data_name)\n",
    "    print(\"loss:\" , loss)\n",
    "    print(\"accuracy\", acc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "val_set = TensorDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "loss: tensor(2.3049, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.1642)\n",
      "\n",
      "valdation\n",
      "loss: tensor(2.3076, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.1595)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation(model, train_set, data_name=\"train\")\n",
    "validation(model, val_set, data_name=\"valdation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight1(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.xavier_normal_(module.weight)\n",
    "        module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_set, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(len(train_set)/batch_size)):\n",
    "            #get minibatch\n",
    "            start_i = i * batch_size\n",
    "            end_i = min(start_i+batch_size, len(train_set))\n",
    "            xb, yb = train_set[start_i:end_i]\n",
    "            # print(xb.shape)\n",
    "            #prediction\n",
    "            pred = model(xb)\n",
    "\n",
    "            #loss\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "\n",
    "            #obtain derivatives\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #update parameters\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, train_set, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "loss: tensor(0.5403, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.8680)\n",
      "\n",
      "validation\n",
      "loss: tensor(0.5715, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.8540)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = TensorDataset(x_train[:1000], y_train[:1000])\n",
    "val_set = TensorDataset(x_val[:1000], y_val[:1000])\n",
    "\n",
    "validation(model, train_set, data_name=\"train\")\n",
    "validation(model, val_set, data_name=\"validation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "- You can create a DataLoader for any Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader=DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader=DataLoader(val_set, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)\n",
    "for xb, yb in train_loader:\n",
    "    # print(len(xb))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataloader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for input, target in train_dataloader:\n",
    "            #Generate prediction\n",
    "            pred = model(input)\n",
    "\n",
    "            #loss\n",
    "            loss = F.cross_entropy(pred, target)\n",
    "\n",
    "            #calculate gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #update paramteres\n",
    "            optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "train(model, optimizer, train_loader, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "loss: tensor(1.4752, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.6639)\n",
      "\n",
      "validation\n",
      "loss: tensor(1.4534, grad_fn=<NllLossBackward0>)\n",
      "accuracy tensor(0.6875)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = TensorDataset(x_train, y_train)\n",
    "validation_set = TensorDataset(x_val, y_val)\n",
    "validation(model, train_set, data_name=\"train\")\n",
    "validation(model, validation_set, data_name=\"validation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "- Since we do not need backpropagation for the validation set, we can use 2x large batches. NO NEED TO BE SHUFFLE For vallidation data. \n",
    "- We should shuffle training data to avoid correlation between batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(x_train, y_train)\n",
    "validation_set = TensorDataset(x_val, y_val)\n",
    "\n",
    "dataloaders={}\n",
    "dataloaders['train']=DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "dataloaders['val']=DataLoader(validation_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.train() and model.eval()\n",
    "- since batch_normalization or dropout layers are behaving different way in train and evalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloaders, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for input, target in dataloaders['train']:\n",
    "            #prediction\n",
    "            pred = model(input)\n",
    "            #loss\n",
    "            loss = F.cross_entropy(pred, target)\n",
    "            #gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #update\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for input, target in dataloaders['val']:\n",
    "            #prediction\n",
    "            pred = model(input)\n",
    "            #loss\n",
    "            # print(pred.shape)\n",
    "            loss += F.cross_entropy(pred, target).sum()\n",
    "            n_correct += (pred.argmax(-1)==target).sum()\n",
    "            n_samples += len(target)\n",
    "        avg_loss = loss / len(dataloaders['val'])\n",
    "        accuracy = n_correct / n_samples\n",
    "\n",
    "        print(f\"Epoch {epoch}: loss={avg_loss:.3f} accuracy={accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.404 accuracy=0.896\n",
      "Epoch 1: loss=0.351 accuracy=0.906\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(num_features=28*28, num_classes=10)\n",
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, dataloaders, num_epochs=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "- convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output size\n",
    "def output_size(input_size, kernel_size, stride, padding, layers=0):\n",
    "    out=input_size\n",
    "    for i in range(layers):\n",
    "        out= np.floor((out+(2*padding) - kernel_size)/stride) + 1\n",
    "    return int(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size(28, 3, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(1, num_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2=nn.Conv2d(num_channels, num_channels, kernel_size=3,stride=2, padding=1)\n",
    "        self.conv3=nn.Conv2d(num_channels, num_classes, kernel_size=3, stride=2, padding=1)\n",
    "        # self.dense=nn.Linear(num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input.view(-1, 1, 28, 28)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        x = x.squeeze()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n",
      "torch.Size([3, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "test = x_train[1:4]\n",
    "print(test.shape)\n",
    "print(test.view(-1, 1, 28, 28).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48000, 28, 28])\n",
      "torch.Size([6000, 28, 28])\n",
      "torch.Size([6000, 28, 28])\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(num_channels=16, num_classes=10)\n",
    "counter = 0\n",
    "\n",
    "size_train = int(np.floor(x_dev.shape[0] * 0.8))\n",
    "size_val = int(np.floor(x_dev.shape[0] * 0.1))\n",
    "size_test = int(np.floor(x_dev.shape[0] * 0.1))\n",
    "\n",
    "x_train, y_train = x_dev[:size_train], y_dev[:size_train]\n",
    "x_val, y_val = x_dev[size_train:size_train+size_val], y_dev[size_train:size_train+size_val]\n",
    "start_idx = size_train+size_val\n",
    "x_test, y_test = x_dev[start_idx:start_idx+size_test], y_dev[start_idx:start_idx+size_test]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "print(x_train.shape[0] + x_val.shape[0] + x_test.shape[0])\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "val_set = TensorDataset(x_val, y_val)\n",
    "test_set = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "\n",
    "Dataloaders = {}\n",
    "Dataloaders['train']=train_loader\n",
    "Dataloaders['val']=val_loader\n",
    "Dataloaders['test']=test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10])\n",
      "torch.Size([16, 10])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in Dataloaders['train']:\n",
    "    pred = cnn(xb)    \n",
    "    break\n",
    "print(pred.shape)\n",
    "print(pred.squeeze().shape)\n",
    "print(torch.argmax(pred.squeeze(), dim=-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.428 accuracy=0.878\n",
      "Epoch 1: loss=0.265 accuracy=0.925\n"
     ]
    }
   ],
   "source": [
    "model = CNN(num_channels=128, num_classes=10)\n",
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "train(model, optimizer=optimizer, dataloaders=Dataloaders, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[ 0.3543,  0.3671,  0.3369],\n",
       "            [ 0.2322,  0.3640,  0.2417],\n",
       "            [ 0.2273,  0.2135,  0.2195]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.8967,  0.8784,  0.8854],\n",
       "            [ 0.5762,  0.5225,  0.4217],\n",
       "            [ 0.1923,  0.1078,  0.0317]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2420, -0.2251, -0.2210],\n",
       "            [-0.1347, -0.1419, -0.2001],\n",
       "            [-0.1704, -0.1447, -0.1568]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 0.3249,  0.3895,  0.4439],\n",
       "            [ 0.3373,  0.4529,  0.2898],\n",
       "            [ 0.2932,  0.2352,  0.1511]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1305, -0.0975,  0.0050],\n",
       "            [ 0.0096, -0.0903, -0.0146],\n",
       "            [ 0.1271,  0.0990,  0.1658]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.3576,  0.4144,  0.3739],\n",
       "            [ 0.4545,  0.4882,  0.4504],\n",
       "            [ 0.4836,  0.4661,  0.4027]]]], requires_grad=True)),\n",
       " ('conv1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 2.3896e-03, -3.6601e-05,  6.5802e-01, -2.1622e-03, -5.9674e-04,\n",
       "          -6.6647e-03, -1.6372e-02, -1.3547e-03,  1.4467e-01,  1.2772e-01,\n",
       "           1.7407e-02,  3.2647e-03,  4.9095e-04,  4.4994e-05, -2.3504e-03,\n",
       "          -1.5098e-03, -5.8639e-04, -4.0891e-04, -2.2537e-03,  1.8539e-03,\n",
       "           4.0526e-03, -9.9059e-04, -1.1789e-02, -1.4846e-03, -7.4665e-05,\n",
       "          -1.2710e-03,  3.7420e-02, -1.1559e-03,  1.5696e-01,  1.3794e-01,\n",
       "           5.4386e-01, -1.9394e-03, -5.4152e-03,  3.2637e-02, -6.7428e-03,\n",
       "          -2.3453e-02, -4.5934e-02, -1.9415e-02,  3.7334e-02,  4.1388e-01,\n",
       "           2.9486e-01, -1.9855e-04,  2.6649e-02, -1.6944e-03, -4.4118e-04,\n",
       "           9.8606e-04, -2.9223e-03,  6.4053e-02,  1.0689e-01,  8.3544e-02,\n",
       "          -9.8757e-04, -2.1779e-02,  8.7239e-02, -2.2126e-03,  9.3799e-05,\n",
       "          -1.4501e-03,  0.0000e+00, -1.2905e-03, -2.6933e-03,  7.8168e-04,\n",
       "          -6.6297e-03,  1.0996e-02, -3.3654e-03,  1.3555e-01, -5.9512e-04,\n",
       "           7.5953e-02, -4.0734e-03, -1.6838e-02, -6.3203e-03, -5.9109e-03,\n",
       "           5.9779e-02, -1.0786e-03, -2.8637e-04, -1.0059e-02,  3.8948e-04,\n",
       "          -9.4683e-03, -6.3715e-03,  4.3439e-05, -2.7142e-03, -1.5174e-03,\n",
       "           2.1227e-03, -8.0094e-04, -4.2470e-03, -2.1753e-03,  8.9483e-04,\n",
       "           3.3713e-03, -4.4471e-03, -1.7813e-02, -5.5503e-03, -3.9691e-03,\n",
       "          -1.7776e-03,  4.8854e-01,  7.8957e-02, -6.1217e-04, -4.5113e-04,\n",
       "          -1.6101e-02, -4.6758e-03,  5.3348e-01,  9.3937e-03, -5.5114e-03,\n",
       "           2.3099e-01,  1.6458e-01, -2.1317e-03,  3.0828e-02,  2.1020e-01,\n",
       "           2.9941e-01,  3.3133e-02, -3.2046e-03,  4.8612e-03, -4.2563e-03,\n",
       "           2.5126e-03, -3.3511e-03,  1.6060e-02, -4.0777e-03,  5.7527e-01,\n",
       "           9.2692e-03, -4.6403e-02, -3.0052e-03,  2.6286e-01, -1.9847e-03,\n",
       "          -1.2057e-02, -5.4557e-03,  2.9174e-02, -8.1469e-04,  1.3111e-01,\n",
       "           6.1621e-03,  4.3747e-02, -3.0089e-03], requires_grad=True)),\n",
       " ('conv2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[-0.1314,  0.0393,  0.0872],\n",
       "            [ 0.1236,  0.0264, -0.0023],\n",
       "            [-0.0269, -0.0897, -0.0233]],\n",
       "  \n",
       "           [[-0.1962, -0.0333,  0.1968],\n",
       "            [ 0.0905,  0.0509, -0.0688],\n",
       "            [ 0.0653, -0.0552, -0.0499]],\n",
       "  \n",
       "           [[ 0.0561, -0.0561, -0.0661],\n",
       "            [-0.0224,  0.0035, -0.0283],\n",
       "            [-0.0147, -0.0009,  0.0580]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0736,  0.0516,  0.0657],\n",
       "            [ 0.0424,  0.0523, -0.0984],\n",
       "            [ 0.0169, -0.0340, -0.0446]],\n",
       "  \n",
       "           [[ 0.0179,  0.0058,  0.0019],\n",
       "            [ 0.0142, -0.0425,  0.0052],\n",
       "            [ 0.0373,  0.0107, -0.0059]],\n",
       "  \n",
       "           [[-0.1022, -0.0171,  0.1311],\n",
       "            [ 0.0714,  0.0502, -0.0310],\n",
       "            [-0.0498, -0.0965, -0.0981]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1637, -0.1100, -0.0848],\n",
       "            [ 0.0038,  0.0126,  0.0005],\n",
       "            [-0.0378,  0.0027, -0.0035]],\n",
       "  \n",
       "           [[-0.1919, -0.1401, -0.1036],\n",
       "            [ 0.1397,  0.0157,  0.0007],\n",
       "            [ 0.0777,  0.0174,  0.0468]],\n",
       "  \n",
       "           [[-0.0385, -0.0388, -0.0312],\n",
       "            [-0.0470,  0.0032, -0.0235],\n",
       "            [ 0.0043,  0.0032,  0.0234]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0723, -0.1137, -0.1301],\n",
       "            [ 0.0907,  0.0416, -0.0079],\n",
       "            [ 0.0205, -0.0408,  0.0323]],\n",
       "  \n",
       "           [[ 0.0915,  0.0296,  0.0375],\n",
       "            [ 0.0073,  0.0277,  0.0006],\n",
       "            [ 0.0120,  0.0493, -0.0342]],\n",
       "  \n",
       "           [[-0.1124, -0.1136, -0.0450],\n",
       "            [ 0.0692, -0.0189,  0.0561],\n",
       "            [-0.0116, -0.0369,  0.0278]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0442, -0.0489,  0.0230],\n",
       "            [ 0.0650,  0.0448,  0.0750],\n",
       "            [-0.0564, -0.0378, -0.0445]],\n",
       "  \n",
       "           [[-0.0420, -0.0769, -0.0005],\n",
       "            [ 0.0605,  0.1286,  0.1398],\n",
       "            [-0.0556,  0.0484,  0.0049]],\n",
       "  \n",
       "           [[ 0.0131,  0.0008, -0.0855],\n",
       "            [ 0.0688, -0.0542, -0.0002],\n",
       "            [ 0.0360, -0.0456, -0.0384]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0301,  0.0009, -0.0318],\n",
       "            [-0.0152,  0.0868,  0.0280],\n",
       "            [-0.1008, -0.0818, -0.0859]],\n",
       "  \n",
       "           [[-0.0338, -0.0008, -0.0672],\n",
       "            [ 0.0058,  0.0137, -0.0688],\n",
       "            [ 0.0101, -0.0687,  0.0083]],\n",
       "  \n",
       "           [[-0.0155, -0.0513,  0.0075],\n",
       "            [ 0.0494,  0.0737,  0.1165],\n",
       "            [-0.1582, -0.0858, -0.1700]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-0.0010, -0.0266,  0.0466],\n",
       "            [ 0.0165, -0.0061, -0.0369],\n",
       "            [ 0.0221, -0.0426, -0.0656]],\n",
       "  \n",
       "           [[-0.1602,  0.0380,  0.1005],\n",
       "            [ 0.0048,  0.0669, -0.0504],\n",
       "            [ 0.1084, -0.0933, -0.0478]],\n",
       "  \n",
       "           [[-0.0056,  0.0188, -0.0502],\n",
       "            [ 0.0035,  0.0081,  0.0417],\n",
       "            [-0.0305, -0.0047,  0.0069]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0801,  0.1002,  0.0561],\n",
       "            [ 0.0491,  0.0306, -0.0465],\n",
       "            [ 0.0767, -0.0664, -0.0940]],\n",
       "  \n",
       "           [[ 0.0133, -0.0215, -0.0191],\n",
       "            [ 0.0233, -0.0101, -0.0323],\n",
       "            [-0.0590, -0.0130, -0.0099]],\n",
       "  \n",
       "           [[-0.1093,  0.0274,  0.1269],\n",
       "            [ 0.0577,  0.0853, -0.0266],\n",
       "            [ 0.0174, -0.0772, -0.1018]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0248, -0.0156,  0.0078],\n",
       "            [-0.0012, -0.0083,  0.0290],\n",
       "            [-0.0108, -0.0470,  0.0267]],\n",
       "  \n",
       "           [[ 0.0146, -0.0582, -0.0136],\n",
       "            [ 0.0091,  0.0107, -0.0488],\n",
       "            [-0.0052,  0.0025,  0.0161]],\n",
       "  \n",
       "           [[ 0.0068,  0.0191, -0.0380],\n",
       "            [-0.0015, -0.0142, -0.0175],\n",
       "            [-0.0077, -0.0382, -0.0348]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0375, -0.0445, -0.0115],\n",
       "            [ 0.0507, -0.0367, -0.0199],\n",
       "            [ 0.0217, -0.0483,  0.0552]],\n",
       "  \n",
       "           [[ 0.0490, -0.0021, -0.0108],\n",
       "            [ 0.0174,  0.0315, -0.0090],\n",
       "            [-0.0073, -0.0263,  0.0448]],\n",
       "  \n",
       "           [[ 0.0138, -0.0004, -0.0048],\n",
       "            [ 0.0479,  0.0081, -0.0438],\n",
       "            [ 0.0561, -0.0518, -0.0134]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0445, -0.0911, -0.0141],\n",
       "            [ 0.0076, -0.0277,  0.0516],\n",
       "            [ 0.0150,  0.0866, -0.0295]],\n",
       "  \n",
       "           [[-0.0350, -0.0764, -0.0155],\n",
       "            [ 0.0455, -0.0174, -0.0280],\n",
       "            [ 0.0288,  0.0647,  0.0012]],\n",
       "  \n",
       "           [[-0.0025, -0.0098,  0.0036],\n",
       "            [-0.0599, -0.0274,  0.0181],\n",
       "            [ 0.0094, -0.0298, -0.0085]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0300,  0.0186, -0.0334],\n",
       "            [ 0.0220, -0.0259,  0.0278],\n",
       "            [ 0.0520, -0.0416, -0.0471]],\n",
       "  \n",
       "           [[ 0.0535,  0.0621, -0.0160],\n",
       "            [-0.0085,  0.0183,  0.0132],\n",
       "            [ 0.0228, -0.0217, -0.0240]],\n",
       "  \n",
       "           [[-0.0309, -0.0874, -0.0451],\n",
       "            [ 0.0102, -0.0781, -0.0264],\n",
       "            [ 0.0142,  0.0277, -0.0739]]]], requires_grad=True)),\n",
       " ('conv2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.1087e-02,  4.9497e-02,  3.8907e-02,  1.4857e-01, -1.9020e-02,\n",
       "           3.3960e-02,  6.9071e-02, -4.3113e-02, -1.3733e-02,  9.7522e-02,\n",
       "          -3.3046e-02, -3.4600e-02,  5.2325e-01,  5.2541e-02, -1.9691e-02,\n",
       "          -1.7959e-02,  3.3035e-01, -2.6240e-02, -7.2311e-02, -8.9043e-02,\n",
       "           6.6321e-03, -7.7384e-02, -1.2023e-01,  3.6299e-02, -7.3116e-05,\n",
       "          -7.5809e-02,  1.3567e-02, -1.6605e-01, -3.6257e-02, -2.0452e-01,\n",
       "          -1.1510e-01, -4.1988e-02, -2.0049e-02,  1.6052e-02, -4.7492e-02,\n",
       "          -5.5779e-02,  2.3698e-03, -5.1311e-02, -2.5235e-02, -5.8792e-02,\n",
       "           1.0393e-02,  7.4240e-02,  6.2426e-04, -4.6363e-02, -2.9948e-02,\n",
       "          -2.4760e-02, -4.3743e-05,  8.0735e-03, -6.6112e-02, -4.5946e-02,\n",
       "           2.0371e-02, -3.5178e-02, -1.4886e-02, -5.3831e-02, -5.0606e-03,\n",
       "          -5.2138e-02, -1.6347e-02, -7.5185e-02, -7.4608e-02,  1.0479e-01,\n",
       "          -5.7153e-02, -5.3636e-02, -1.4802e-01,  1.6822e-01, -5.7595e-03,\n",
       "          -2.7535e-02, -1.1847e-01,  5.0644e-03, -7.9331e-02,  2.3628e-02,\n",
       "          -4.1149e-02, -8.3134e-03,  1.3356e-01, -5.1516e-02, -8.8858e-02,\n",
       "          -4.8939e-02, -4.8007e-02, -9.2572e-02,  3.8212e-02,  4.3392e-02,\n",
       "           9.7149e-03,  1.0891e-02,  2.3012e-01, -7.0873e-02,  1.9646e-01,\n",
       "           2.5752e-01,  6.3009e-02, -8.1525e-03, -9.8432e-03, -2.0468e-02,\n",
       "          -4.7008e-02,  7.9376e-03, -2.2907e-02, -6.2926e-02, -1.4920e-01,\n",
       "          -2.5231e-02, -4.7340e-02, -4.9540e-02, -3.5739e-02,  7.3402e-02,\n",
       "           1.0421e-02,  4.1301e-02, -6.3297e-02, -5.9334e-03, -6.0090e-03,\n",
       "          -5.5047e-03,  3.3888e-02,  6.3329e-03,  2.3106e-02, -7.7448e-02,\n",
       "           1.5609e-02, -5.4995e-02,  2.0932e-01, -2.6149e-02, -1.9827e-02,\n",
       "          -4.8133e-02,  7.7464e-02, -5.8850e-02, -3.0772e-02, -3.5635e-02,\n",
       "           1.7076e-01,  1.0916e-02, -1.5393e-01, -5.2072e-02,  1.8588e-02,\n",
       "          -2.1247e-02, -6.2555e-03,  6.2989e-03], requires_grad=True)),\n",
       " ('conv3.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[ 9.1472e-02,  3.3997e-02,  6.2188e-02],\n",
       "            [-7.7003e-02, -8.7422e-03,  1.9554e-01],\n",
       "            [ 5.9897e-02,  1.4059e-01,  1.8915e-01]],\n",
       "  \n",
       "           [[-1.7099e-01, -2.2155e-01,  2.0901e-03],\n",
       "            [-4.3678e-02, -2.8620e-01, -1.6874e-01],\n",
       "            [ 1.8461e-01,  1.8915e-02, -2.5414e-02]],\n",
       "  \n",
       "           [[-9.6015e-02, -3.3370e-01, -7.3765e-03],\n",
       "            [-1.3196e-01, -1.8569e-01, -1.3442e-01],\n",
       "            [ 1.3328e-01, -2.1735e-02,  9.0496e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 1.3048e-01, -1.4207e-01,  5.6223e-02],\n",
       "            [ 3.0023e-02, -6.9999e-02,  1.8133e-01],\n",
       "            [-5.1509e-02,  1.0638e-01,  2.4288e-01]],\n",
       "  \n",
       "           [[-5.7052e-02, -8.0517e-03, -2.3366e-02],\n",
       "            [-2.9217e-02,  7.8051e-02, -1.8009e-02],\n",
       "            [ 5.4426e-03,  1.6243e-02, -1.1398e-02]],\n",
       "  \n",
       "           [[-1.5178e-01, -9.2598e-02, -5.8720e-02],\n",
       "            [ 4.4943e-02,  8.4939e-03, -1.0761e-01],\n",
       "            [ 1.1818e-01,  1.6336e-01,  7.9617e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[-4.2165e-02, -3.1234e-02, -1.8525e-01],\n",
       "            [-1.9838e-01, -1.5757e-01, -1.0772e-01],\n",
       "            [-3.3409e-02, -1.0952e-01, -1.0050e-01]],\n",
       "  \n",
       "           [[-1.7104e-02,  5.9715e-02, -9.0304e-02],\n",
       "            [-2.8617e-01, -1.4085e-01, -1.3464e-01],\n",
       "            [ 3.7482e-01, -3.7902e-02,  5.9443e-02]],\n",
       "  \n",
       "           [[-1.4566e-01, -9.9477e-02, -1.7084e-01],\n",
       "            [-3.2784e-01, -1.4971e-01, -1.4189e-01],\n",
       "            [ 2.7101e-02,  1.4955e-02,  5.0911e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-5.9218e-03,  3.6818e-02, -9.0583e-02],\n",
       "            [ 2.4197e-02, -2.3915e-01, -1.8712e-02],\n",
       "            [ 2.0690e-01, -1.6331e-01, -9.9103e-04]],\n",
       "  \n",
       "           [[-5.0617e-02, -4.5915e-02,  4.8162e-02],\n",
       "            [ 2.0081e-02,  2.3799e-04,  5.4735e-02],\n",
       "            [ 3.4447e-02,  6.2858e-02,  3.7552e-02]],\n",
       "  \n",
       "           [[-4.1316e-02, -4.4305e-02, -7.9731e-02],\n",
       "            [-1.3835e-01, -3.5035e-02, -3.0017e-02],\n",
       "            [ 1.1876e-01, -1.4076e-01, -1.3218e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[-3.9299e-02,  3.3242e-02, -6.2659e-02],\n",
       "            [ 6.9582e-02,  3.2465e-01, -5.0489e-02],\n",
       "            [ 1.1290e-01, -8.1247e-03,  3.2381e-03]],\n",
       "  \n",
       "           [[-3.9156e-02,  2.7418e-01,  1.8030e-01],\n",
       "            [ 1.1943e-01,  2.5722e-01,  2.2795e-01],\n",
       "            [-4.5769e-01, -3.5149e-01, -4.9517e-02]],\n",
       "  \n",
       "           [[ 2.7728e-01,  2.3110e-01,  1.6452e-01],\n",
       "            [ 9.5408e-02,  1.2907e-02,  2.0374e-02],\n",
       "            [-1.9570e-01,  2.9536e-02, -7.4912e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-9.9556e-02, -7.0614e-02, -9.0351e-03],\n",
       "            [ 1.1175e-01,  3.2877e-01,  7.4598e-03],\n",
       "            [ 3.6146e-02, -2.4916e-02, -1.6082e-02]],\n",
       "  \n",
       "           [[ 7.4637e-03, -4.2621e-02,  5.2277e-02],\n",
       "            [-3.4956e-02, -9.4266e-03,  9.1357e-02],\n",
       "            [-2.6115e-02,  7.2218e-03, -2.4092e-02]],\n",
       "  \n",
       "           [[ 8.2780e-02,  2.3156e-01,  6.1335e-02],\n",
       "            [-1.4866e-01,  5.2551e-03,  2.2904e-02],\n",
       "            [-1.1486e-01, -1.9433e-01, -8.1292e-02]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 2.0201e-02, -1.9348e-01, -3.5955e-02],\n",
       "            [-2.8592e-02, -2.6184e-01,  1.7932e-02],\n",
       "            [ 1.2075e-01, -3.2953e-02, -1.6990e-01]],\n",
       "  \n",
       "           [[-2.2114e-01, -5.1327e-02, -9.3797e-02],\n",
       "            [ 1.5950e-01,  2.6832e-02,  1.6758e-01],\n",
       "            [-1.7083e-01, -7.8466e-02, -1.7576e-01]],\n",
       "  \n",
       "           [[-1.0108e-01, -5.7494e-02,  5.4992e-02],\n",
       "            [ 2.2646e-02,  4.5256e-02,  1.4044e-01],\n",
       "            [-6.9852e-02,  8.1219e-02, -1.0890e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-1.4629e-01, -1.1107e-01, -3.3012e-02],\n",
       "            [ 2.1685e-02, -7.2013e-02, -2.3465e-02],\n",
       "            [ 3.5535e-02, -8.5629e-02, -2.7377e-01]],\n",
       "  \n",
       "           [[ 1.2372e-02, -5.3392e-04, -3.7089e-02],\n",
       "            [ 1.9200e-02, -2.7853e-02,  7.2991e-02],\n",
       "            [ 2.3072e-02, -4.1562e-05, -2.1080e-02]],\n",
       "  \n",
       "           [[ 3.5262e-02, -6.3023e-02, -3.6920e-02],\n",
       "            [ 4.8305e-02, -7.9583e-02,  1.4652e-01],\n",
       "            [-9.7558e-02, -1.3064e-01, -6.7066e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.4405e-02,  1.1649e-01,  5.8611e-02],\n",
       "            [ 1.2770e-01,  2.1251e-01,  1.9991e-01],\n",
       "            [-1.4375e-01, -8.8226e-02,  1.0097e-01]],\n",
       "  \n",
       "           [[ 3.4951e-02, -8.1175e-03,  9.2430e-03],\n",
       "            [-1.0228e-01,  2.4662e-02,  8.5272e-02],\n",
       "            [ 5.1700e-02,  3.6511e-03,  1.1675e-01]],\n",
       "  \n",
       "           [[-6.6608e-02,  3.5483e-03, -1.3538e-01],\n",
       "            [-1.7713e-01,  1.6143e-01,  9.6586e-02],\n",
       "            [-1.4094e-01,  4.4945e-03,  5.3706e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 2.5110e-02,  1.6200e-01,  5.8377e-02],\n",
       "            [ 3.3828e-01,  7.8458e-02,  2.9451e-01],\n",
       "            [-1.7524e-01, -3.3894e-02,  1.1932e-01]],\n",
       "  \n",
       "           [[-6.3444e-03,  3.7251e-03,  3.0209e-02],\n",
       "            [-2.3461e-02, -2.9326e-02, -8.8710e-02],\n",
       "            [-7.8356e-02,  3.0950e-02, -5.3496e-02]],\n",
       "  \n",
       "           [[-1.2650e-02,  6.2457e-02, -1.0296e-01],\n",
       "            [-1.4414e-01, -4.9015e-02,  1.8784e-02],\n",
       "            [ 1.3398e-01,  2.1234e-01,  7.4884e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[ 1.3050e-02,  1.3384e-01, -8.8611e-02],\n",
       "            [ 9.1893e-02, -4.5282e-02,  2.2810e-01],\n",
       "            [-4.4742e-02,  8.5030e-02, -2.4715e-01]],\n",
       "  \n",
       "           [[-1.6519e-01, -2.1171e-02, -2.4685e-02],\n",
       "            [-3.3948e-02,  2.8322e-01,  1.6824e-01],\n",
       "            [-2.0690e-02, -1.6863e-01,  2.5140e-02]],\n",
       "  \n",
       "           [[ 7.9521e-02,  1.0825e-01, -1.4344e-01],\n",
       "            [ 2.3025e-01, -1.7600e-01,  5.6144e-02],\n",
       "            [-4.9876e-02, -1.2246e-01, -3.6554e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 2.4258e-02,  1.4132e-01,  1.0686e-01],\n",
       "            [ 7.8276e-02,  1.1421e-01,  2.1605e-01],\n",
       "            [ 9.5931e-03,  1.2615e-01, -3.5353e-01]],\n",
       "  \n",
       "           [[ 1.0109e-01,  4.8735e-02,  8.2501e-04],\n",
       "            [-5.6927e-02,  4.2638e-03, -1.1887e-01],\n",
       "            [ 2.2704e-02,  4.4208e-04,  4.4132e-02]],\n",
       "  \n",
       "           [[-1.1515e-01,  2.6400e-03,  5.7093e-02],\n",
       "            [ 5.2738e-03,  5.1029e-02,  8.4295e-02],\n",
       "            [ 9.2820e-02, -6.3745e-03,  4.0438e-02]]]], requires_grad=True)),\n",
       " ('conv3.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.1270,  0.2931,  0.0037, -0.1631,  0.0098,  0.0935, -0.0344,  0.1659,\n",
       "           0.3322,  0.2807], requires_grad=True))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Seqential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, input):\n",
    "        return self.func(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_train[1:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(lambda x: x.view(-1, 1, 28, 28)),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.squeeze()) #Lambda(lambda x; x.view(x.size(0), -1)))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor .size(0) and .shape[0] are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 10])\n",
      "torch.Size([9, 10])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "a = model(test)\n",
    "b = model(test).view(model(test).size(0), -1)\n",
    "print(b.shape)\n",
    "print(a.shape)\n",
    "print(a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[ 0.2856,  0.2463,  0.2811],\n",
       "            [ 0.1771, -0.2487, -0.0838],\n",
       "            [ 0.3195,  0.2273, -0.1136]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2021,  0.2758,  0.0600],\n",
       "            [ 0.2458, -0.1466,  0.0742],\n",
       "            [ 0.1434, -0.1201, -0.1703]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1685,  0.1603, -0.3287],\n",
       "            [-0.1355,  0.0617, -0.1395],\n",
       "            [ 0.0892, -0.3230, -0.1972]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2559, -0.3023,  0.2707],\n",
       "            [-0.3049, -0.1956, -0.0402],\n",
       "            [-0.3300, -0.1131,  0.0932]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0596, -0.1304,  0.2791],\n",
       "            [ 0.2621,  0.2056, -0.3207],\n",
       "            [ 0.1881, -0.1002, -0.1272]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2709, -0.1873, -0.0089],\n",
       "            [-0.3197,  0.3189, -0.1598],\n",
       "            [ 0.1146, -0.2175,  0.3044]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1839, -0.1764,  0.3315],\n",
       "            [-0.1279, -0.1840, -0.0786],\n",
       "            [-0.0866, -0.1820,  0.1953]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0477, -0.0143,  0.2875],\n",
       "            [-0.0430,  0.1984,  0.1924],\n",
       "            [ 0.1791,  0.2920, -0.1864]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.3171, -0.0860,  0.0404],\n",
       "            [-0.1930,  0.2721, -0.0371],\n",
       "            [ 0.1652,  0.2782,  0.3105]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1958,  0.3107,  0.0251],\n",
       "            [ 0.3001, -0.2740,  0.0264],\n",
       "            [-0.1962, -0.2064,  0.2029]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1155,  0.2940, -0.1689],\n",
       "            [ 0.0132, -0.1974,  0.1366],\n",
       "            [ 0.1096, -0.2875, -0.1188]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2378,  0.1786, -0.2068],\n",
       "            [ 0.0154, -0.0484, -0.1352],\n",
       "            [-0.0918, -0.1993,  0.1356]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1362,  0.0671, -0.0604],\n",
       "            [ 0.1620, -0.2639,  0.1505],\n",
       "            [-0.0764, -0.1870,  0.0128]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0537,  0.0380,  0.1840],\n",
       "            [ 0.1473,  0.2929, -0.3121],\n",
       "            [-0.0592,  0.0077,  0.2073]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0182,  0.1537,  0.1662],\n",
       "            [-0.0045,  0.1046,  0.2385],\n",
       "            [ 0.0271, -0.0733, -0.3285]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.3297,  0.1443,  0.3173],\n",
       "            [-0.1501, -0.1525,  0.0967],\n",
       "            [-0.2036,  0.0140, -0.2548]]]], requires_grad=True)),\n",
       " ('1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.2531,  0.0625, -0.2922,  0.2749, -0.1973, -0.1092,  0.2132,  0.1189,\n",
       "          -0.1564, -0.2121,  0.1504, -0.2476, -0.1283,  0.1588, -0.1416,  0.1905],\n",
       "         requires_grad=True)),\n",
       " ('3.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[-0.0627, -0.0232, -0.0486],\n",
       "            [-0.0028, -0.0533, -0.0768],\n",
       "            [-0.0701,  0.0620, -0.0124]],\n",
       "  \n",
       "           [[ 0.0241,  0.0795,  0.0099],\n",
       "            [ 0.0624,  0.0215, -0.0559],\n",
       "            [-0.0569, -0.0586, -0.0238]],\n",
       "  \n",
       "           [[-0.0636,  0.0293,  0.0638],\n",
       "            [ 0.0010,  0.0663,  0.0040],\n",
       "            [-0.0019, -0.0586, -0.0527]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0027,  0.0315, -0.0372],\n",
       "            [ 0.0032,  0.0666, -0.0535],\n",
       "            [ 0.0277, -0.0116, -0.0728]],\n",
       "  \n",
       "           [[-0.0541,  0.0219,  0.0269],\n",
       "            [-0.0064,  0.0790, -0.0036],\n",
       "            [ 0.0180, -0.0642,  0.0024]],\n",
       "  \n",
       "           [[ 0.0065, -0.0514, -0.0551],\n",
       "            [ 0.0258, -0.0279,  0.0742],\n",
       "            [-0.0783,  0.0327, -0.0611]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0827,  0.0679, -0.0638],\n",
       "            [ 0.0603,  0.0349, -0.0774],\n",
       "            [ 0.0798,  0.0429,  0.0116]],\n",
       "  \n",
       "           [[ 0.0603, -0.0245, -0.0472],\n",
       "            [-0.0648,  0.0806, -0.0385],\n",
       "            [-0.0536, -0.0707,  0.0411]],\n",
       "  \n",
       "           [[-0.0571, -0.0809,  0.0472],\n",
       "            [ 0.0221, -0.0546, -0.0131],\n",
       "            [-0.0642,  0.0535,  0.0219]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0157, -0.0711, -0.0311],\n",
       "            [ 0.0568, -0.0790, -0.0200],\n",
       "            [ 0.0325,  0.0334, -0.0429]],\n",
       "  \n",
       "           [[-0.0137,  0.0559, -0.0677],\n",
       "            [-0.0564,  0.0771,  0.0554],\n",
       "            [ 0.0574, -0.0157, -0.0056]],\n",
       "  \n",
       "           [[-0.0636,  0.0560, -0.0530],\n",
       "            [-0.0420,  0.0098, -0.0759],\n",
       "            [-0.0597,  0.0538,  0.0155]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0034,  0.0756,  0.0815],\n",
       "            [ 0.0458,  0.0797,  0.0637],\n",
       "            [-0.0632,  0.0090, -0.0279]],\n",
       "  \n",
       "           [[ 0.0726, -0.0328,  0.0657],\n",
       "            [ 0.0067, -0.0075, -0.0779],\n",
       "            [ 0.0318, -0.0353,  0.0041]],\n",
       "  \n",
       "           [[ 0.0665, -0.0045, -0.0461],\n",
       "            [-0.0167, -0.0827,  0.0170],\n",
       "            [-0.0240, -0.0488,  0.0794]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0425, -0.0018,  0.0622],\n",
       "            [ 0.0194,  0.0381, -0.0331],\n",
       "            [-0.0453, -0.0605, -0.0074]],\n",
       "  \n",
       "           [[ 0.0627,  0.0394, -0.0010],\n",
       "            [-0.0152,  0.0229,  0.0321],\n",
       "            [ 0.0498,  0.0574,  0.0365]],\n",
       "  \n",
       "           [[-0.0280, -0.0100, -0.0327],\n",
       "            [ 0.0559, -0.0623, -0.0396],\n",
       "            [-0.0739,  0.0138,  0.0573]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-0.0014, -0.0662,  0.0825],\n",
       "            [-0.0496,  0.0534, -0.0764],\n",
       "            [-0.0270, -0.0564, -0.0062]],\n",
       "  \n",
       "           [[ 0.0274, -0.0076, -0.0525],\n",
       "            [-0.0406,  0.0552, -0.0116],\n",
       "            [-0.0166, -0.0038,  0.0611]],\n",
       "  \n",
       "           [[-0.0037, -0.0629, -0.0241],\n",
       "            [ 0.0239, -0.0823,  0.0494],\n",
       "            [ 0.0325, -0.0787, -0.0661]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0720,  0.0124,  0.0554],\n",
       "            [-0.0821,  0.0607,  0.0045],\n",
       "            [ 0.0435, -0.0641,  0.0461]],\n",
       "  \n",
       "           [[-0.0121, -0.0212, -0.0725],\n",
       "            [ 0.0700, -0.0073, -0.0415],\n",
       "            [ 0.0458,  0.0239,  0.0308]],\n",
       "  \n",
       "           [[-0.0639, -0.0508, -0.0482],\n",
       "            [-0.0342, -0.0702,  0.0829],\n",
       "            [ 0.0318, -0.0264, -0.0228]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0490,  0.0193, -0.0350],\n",
       "            [ 0.0421,  0.0200,  0.0317],\n",
       "            [ 0.0248,  0.0797,  0.0005]],\n",
       "  \n",
       "           [[ 0.0301, -0.0288,  0.0105],\n",
       "            [-0.0145, -0.0200, -0.0404],\n",
       "            [ 0.0747, -0.0454,  0.0193]],\n",
       "  \n",
       "           [[ 0.0733,  0.0046,  0.0191],\n",
       "            [-0.0454, -0.0688,  0.0484],\n",
       "            [-0.0369,  0.0533,  0.0152]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0341,  0.0745,  0.0298],\n",
       "            [-0.0277,  0.0398,  0.0428],\n",
       "            [ 0.0814,  0.0749,  0.0598]],\n",
       "  \n",
       "           [[-0.0562, -0.0754, -0.0149],\n",
       "            [ 0.0118,  0.0309,  0.0463],\n",
       "            [ 0.0059, -0.0424, -0.0291]],\n",
       "  \n",
       "           [[ 0.0772, -0.0111,  0.0477],\n",
       "            [ 0.0675, -0.0708, -0.0200],\n",
       "            [-0.0435, -0.0508, -0.0181]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0033,  0.0210,  0.0442],\n",
       "            [-0.0002, -0.0375,  0.0344],\n",
       "            [ 0.0729, -0.0737, -0.0515]],\n",
       "  \n",
       "           [[-0.0325,  0.0606,  0.0256],\n",
       "            [-0.0832, -0.0825,  0.0055],\n",
       "            [ 0.0831, -0.0055,  0.0725]],\n",
       "  \n",
       "           [[-0.0144,  0.0461,  0.0694],\n",
       "            [-0.0086,  0.0556,  0.0163],\n",
       "            [ 0.0565,  0.0384, -0.0185]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0785,  0.0411, -0.0822],\n",
       "            [-0.0115,  0.0827, -0.0169],\n",
       "            [-0.0581,  0.0657,  0.0239]],\n",
       "  \n",
       "           [[ 0.0019, -0.0585, -0.0586],\n",
       "            [-0.0367,  0.0353, -0.0473],\n",
       "            [ 0.0530, -0.0589, -0.0278]],\n",
       "  \n",
       "           [[-0.0025,  0.0119, -0.0620],\n",
       "            [ 0.0477,  0.0425, -0.0177],\n",
       "            [-0.0568,  0.0405, -0.0332]]]], requires_grad=True)),\n",
       " ('3.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0595,  0.0527,  0.0587,  0.0278,  0.0224, -0.0705,  0.0570,  0.0776,\n",
       "           0.0445, -0.0171, -0.0124,  0.0060,  0.0313,  0.0181,  0.0258,  0.0328],\n",
       "         requires_grad=True)),\n",
       " ('5.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[-7.6273e-02, -4.7601e-02, -9.3128e-03],\n",
       "            [ 6.9796e-02, -8.1282e-02, -4.2578e-02],\n",
       "            [-6.0091e-02,  4.0081e-02, -5.4794e-02]],\n",
       "  \n",
       "           [[-6.3443e-02, -7.8824e-02,  2.8580e-02],\n",
       "            [-2.6790e-03, -3.4332e-02, -6.0843e-02],\n",
       "            [ 4.2620e-02, -7.6781e-02, -8.1035e-02]],\n",
       "  \n",
       "           [[-2.2913e-02, -1.2597e-02,  6.3439e-02],\n",
       "            [-1.9789e-02,  4.6749e-02,  4.6747e-03],\n",
       "            [-4.8406e-02, -9.7981e-03, -5.2049e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 6.7000e-02,  2.8091e-02, -5.1174e-02],\n",
       "            [ 3.9401e-02, -4.9392e-02,  7.3653e-02],\n",
       "            [ 5.8072e-02,  4.9716e-02, -3.1819e-02]],\n",
       "  \n",
       "           [[-4.0535e-02, -4.8382e-02,  4.3909e-02],\n",
       "            [-3.6829e-02,  7.0943e-02, -4.5895e-02],\n",
       "            [-1.7992e-02, -8.1235e-02, -8.7686e-03]],\n",
       "  \n",
       "           [[ 3.3896e-02, -7.2996e-02,  5.6859e-02],\n",
       "            [-7.9134e-02, -3.6031e-02, -8.2889e-02],\n",
       "            [-4.7464e-02,  6.7226e-02,  6.9391e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.3380e-02,  6.6062e-03,  2.4113e-02],\n",
       "            [ 4.1429e-02,  5.1354e-02,  6.4536e-02],\n",
       "            [ 6.3265e-02,  3.7223e-02,  4.7452e-02]],\n",
       "  \n",
       "           [[-8.2353e-02, -4.1201e-02,  2.3711e-02],\n",
       "            [-3.5958e-02,  2.8194e-02,  3.2266e-02],\n",
       "            [ 2.3201e-02,  5.5741e-02,  6.5229e-02]],\n",
       "  \n",
       "           [[-3.7693e-02, -6.7541e-02, -5.2377e-02],\n",
       "            [ 7.6211e-03, -2.8873e-02,  6.7215e-02],\n",
       "            [ 1.2092e-02,  5.2690e-02, -2.7097e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-4.1836e-02, -7.9872e-02,  1.5266e-02],\n",
       "            [ 4.6456e-02,  3.7790e-02, -7.6520e-02],\n",
       "            [-4.0094e-02,  7.6556e-02,  3.4658e-03]],\n",
       "  \n",
       "           [[ 4.2017e-02, -3.5317e-02, -5.8148e-02],\n",
       "            [ 7.9096e-02,  2.8930e-03, -2.1422e-02],\n",
       "            [-7.5011e-02,  4.5941e-02, -2.0016e-02]],\n",
       "  \n",
       "           [[ 3.0134e-02, -6.6089e-03, -6.4931e-02],\n",
       "            [-2.0733e-02, -3.3365e-02,  1.7638e-02],\n",
       "            [-2.0885e-02,  7.2557e-02,  6.4544e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[ 8.7587e-03, -8.1402e-02, -8.1878e-02],\n",
       "            [ 5.2718e-02, -2.7553e-03,  2.7722e-03],\n",
       "            [-1.4443e-02,  7.1588e-02,  7.5217e-02]],\n",
       "  \n",
       "           [[-5.8644e-02,  3.9172e-02,  5.4045e-02],\n",
       "            [-7.3726e-02, -6.3919e-02,  7.7140e-02],\n",
       "            [-7.0129e-02,  8.1886e-02, -4.1124e-02]],\n",
       "  \n",
       "           [[ 5.1853e-02, -1.4270e-02, -5.5151e-02],\n",
       "            [ 1.4824e-03,  5.4616e-03, -7.1590e-02],\n",
       "            [ 7.2377e-02, -1.3740e-02,  6.5574e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-7.6948e-02,  1.4348e-02,  1.1318e-02],\n",
       "            [-6.0588e-03,  4.1915e-02,  7.1874e-02],\n",
       "            [ 3.2120e-02,  5.1319e-02,  1.0729e-02]],\n",
       "  \n",
       "           [[ 1.1590e-02, -4.3169e-02,  2.6002e-02],\n",
       "            [ 4.4881e-02,  7.8767e-02,  3.7193e-02],\n",
       "            [ 4.6806e-02, -4.0963e-03, -9.6778e-05]],\n",
       "  \n",
       "           [[ 3.5429e-02, -7.4385e-02,  3.1598e-02],\n",
       "            [ 4.0429e-02,  8.0639e-03, -1.9624e-02],\n",
       "            [-5.9627e-02, -2.5596e-02,  6.4650e-02]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 1.2060e-02,  7.2281e-02, -1.9538e-03],\n",
       "            [-7.5259e-03,  2.4813e-02, -1.7171e-02],\n",
       "            [ 2.0412e-02,  5.8832e-02, -3.5606e-02]],\n",
       "  \n",
       "           [[ 1.4321e-02,  4.7202e-02, -4.8047e-02],\n",
       "            [ 4.8496e-02, -2.2366e-02,  6.6787e-02],\n",
       "            [ 2.9322e-02, -5.1081e-02,  1.7992e-02]],\n",
       "  \n",
       "           [[ 6.1925e-02,  4.6226e-02,  3.0433e-02],\n",
       "            [-5.7207e-02, -5.6200e-02,  9.7646e-03],\n",
       "            [-5.8598e-02, -3.4873e-02, -1.2886e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 6.7677e-02,  6.2209e-02, -7.8229e-02],\n",
       "            [ 3.5724e-02, -5.1282e-02,  2.5448e-02],\n",
       "            [-5.7019e-02, -2.2858e-02,  2.6875e-02]],\n",
       "  \n",
       "           [[ 4.2677e-02, -1.4907e-02,  7.9334e-02],\n",
       "            [-6.8135e-02,  4.3441e-02,  2.9111e-02],\n",
       "            [ 6.2652e-03, -1.6771e-02,  3.4928e-02]],\n",
       "  \n",
       "           [[ 4.0794e-03,  1.9247e-02,  8.2937e-02],\n",
       "            [ 4.0928e-02, -7.9977e-02, -3.8691e-02],\n",
       "            [ 4.0885e-02, -6.5004e-02,  6.8164e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[-3.2295e-03,  3.7964e-02, -4.6415e-02],\n",
       "            [-1.9255e-02,  5.9920e-03,  9.2507e-03],\n",
       "            [-6.4434e-02,  2.1758e-02, -2.4288e-02]],\n",
       "  \n",
       "           [[-5.7605e-02, -3.3881e-02,  1.0401e-02],\n",
       "            [-5.9984e-02, -4.0080e-02, -5.9023e-02],\n",
       "            [ 2.4140e-02,  3.6465e-02, -3.9133e-02]],\n",
       "  \n",
       "           [[-1.3298e-02, -5.7003e-02, -7.7684e-02],\n",
       "            [-5.4683e-02, -3.6092e-02,  6.8687e-02],\n",
       "            [ 1.0006e-02,  3.2226e-02,  3.1270e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-3.5786e-02, -8.1710e-02, -6.3979e-03],\n",
       "            [ 6.7208e-02,  7.7511e-03, -7.5095e-02],\n",
       "            [-7.3547e-02, -4.8181e-02,  4.1641e-02]],\n",
       "  \n",
       "           [[ 7.9873e-02,  3.4377e-02,  4.8429e-02],\n",
       "            [ 6.8368e-02, -2.2772e-02,  6.4009e-03],\n",
       "            [ 6.5566e-02, -6.5659e-03, -7.4103e-02]],\n",
       "  \n",
       "           [[ 6.8749e-02,  5.6857e-02,  2.5349e-02],\n",
       "            [ 6.3444e-02,  1.5563e-02, -4.0414e-02],\n",
       "            [-2.5565e-03, -2.5716e-02,  6.2008e-03]]],\n",
       "  \n",
       "  \n",
       "          [[[ 4.3758e-02, -7.2479e-02, -1.7468e-02],\n",
       "            [-6.9703e-02,  4.1447e-02,  5.8486e-02],\n",
       "            [-5.7211e-02, -4.6702e-02, -4.0044e-02]],\n",
       "  \n",
       "           [[-9.8372e-03, -5.9645e-02, -7.9289e-02],\n",
       "            [-2.0630e-02,  7.0734e-03, -3.1425e-02],\n",
       "            [ 2.7667e-02, -4.6394e-02, -5.4833e-03]],\n",
       "  \n",
       "           [[ 6.3284e-02, -1.9272e-02, -8.2945e-02],\n",
       "            [ 3.5483e-02, -4.8093e-02, -6.4520e-03],\n",
       "            [ 2.3605e-02,  1.6008e-02,  2.3668e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-5.7718e-02, -7.7353e-02,  3.8205e-02],\n",
       "            [ 7.5022e-02, -1.9907e-03, -2.8837e-02],\n",
       "            [ 3.5657e-03,  1.2331e-02,  9.6656e-03]],\n",
       "  \n",
       "           [[-5.1255e-03, -5.5591e-02, -4.4921e-02],\n",
       "            [-4.8729e-02,  4.0449e-02, -7.9240e-02],\n",
       "            [-7.7360e-02,  1.8610e-02,  7.5922e-02]],\n",
       "  \n",
       "           [[-8.1282e-02, -4.1161e-02,  3.3236e-03],\n",
       "            [ 6.1557e-02, -4.2938e-03,  5.3059e-02],\n",
       "            [ 7.1561e-02,  5.3973e-02,  6.4042e-03]]]], requires_grad=True)),\n",
       " ('5.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0406,  0.0384,  0.0657,  0.0412,  0.0198,  0.0403, -0.0159, -0.0158,\n",
       "          -0.0334, -0.0340], requires_grad=True))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.469 accuracy=0.855\n",
      "Epoch 1: loss=0.374 accuracy=0.881\n"
     ]
    }
   ],
   "source": [
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)\n",
    "\n",
    "train(model, optimizer=optimizer, dataloaders=Dataloaders, num_epochs=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of GPUs for learning\n",
    "- We should be careful if our all sets for deep learning are existing on GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Lambda()\n",
       "  (1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (2): ReLU()\n",
       "  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): Conv2d(16, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (6): ReLU()\n",
       "  (7): AdaptiveAvgPool2d(output_size=1)\n",
       "  (8): Lambda()\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move our model to the device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataloaders = {}\n",
    "Dataloaders['train'] = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "Dataloaders['val'] = DataLoader(val_set, batch_size=16, shuffle=False)\n",
    "Dataloaders['test'] = DataLoader(test_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloaders, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # training\n",
    "        model.train()\n",
    "        for input, target in dataloaders['train']:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            pred = model(input)\n",
    "            loss = F.cross_entropy(pred, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for input, target in dataloaders['val']:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            pred = model(input)\n",
    "            loss += F.cross_entropy(pred, target)\n",
    "\n",
    "            n_correct += (pred.argmax(-1) == target).sum()\n",
    "            n_samples += len(dataloaders['val'])\n",
    "\n",
    "        avg_loss = loss/len(dataloaders['val'])\n",
    "        accuracy = n_correct/n_samples\n",
    "\n",
    "        print(f\"Epoch {epoch}: loss={avg_loss :.3f} accuracy={accuracy :.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.542 accuracy=0.035\n",
      "Epoch 1: loss=0.337 accuracy=0.038\n"
     ]
    }
   ],
   "source": [
    "model.apply(initialize_weight1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "\n",
    "train(model, optimizer=optimizer, dataloaders=Dataloaders, num_epochs=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "- runtime: Drop the input (set weight to zero) = Drop neurons randomly by drop out rate (p=self.p)\n",
    "    - Scale the not dropped neurons output by $\\frac{1}{1-p}$\n",
    "        - To conserve input mean\n",
    "- validation: Use all neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_mean: \n",
      "0.05747007206082344\n",
      "\n",
      "dropped_out_mean:\n",
      "-0.09536542743444443\n",
      "\n",
      "is_closed_enough?:  False\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(10, 10)\n",
    "print(\"input_mean: \")\n",
    "input_mean=input.mean()\n",
    "print(input_mean.item())\n",
    "\n",
    "mask = torch.zeros_like(input)\n",
    "p = 0.2\n",
    "mask.bernoulli_(p)\n",
    "scale = 1 / (1-p)\n",
    "droped_out = scale * mask * input\n",
    "print()\n",
    "print(\"dropped_out_mean:\")\n",
    "droped_out_mean=droped_out.mean()\n",
    "print(droped_out_mean.item())\n",
    "\n",
    "print()\n",
    "#chenck the conservation of the mean\n",
    "print(\"is_closed_enough?: \", np.isclose(input_mean.item(), droped_out_mean.item(), atol=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    # p:float, drop probability\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input:Pytorch tensor, arbitrary shape\n",
    "        Returns:\n",
    "            Pytorch tensor, same shape as input (Droped out)\n",
    "        \"\"\"\n",
    "            \n",
    "        if self.training:\n",
    "            mask = torch.zeros_like(input)\n",
    "            mask.bernoulli_(1-self.p) # set 1 to the not dropped input\n",
    "            scaling = 1 / (1 - self.p)\n",
    "            dropped_out = scaling * input * mask\n",
    "        return dropped_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007554699666798115\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "#Test dropout\n",
    "test = torch.randn(10_000)\n",
    "dropout = Dropout(0.5)\n",
    "test_droped_out = dropout(test)\n",
    "\n",
    "# Obtain item in the tensor\n",
    "print(test.mean().item())\n",
    "print(type(test.mean().item()))\n",
    "\n",
    "assert np.isclose(test.mean().item(), test_droped_out.mean().item(), atol=1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "- Batch normalization is a trick to obtain smooth loss landscape and improve training by preserving the variance of input as much as possible\n",
    "\n",
    "- It is defined as the funciton\n",
    "    $$y=\\frac{x-\\mu_x}{\\sigma_x + \\epsilon} \\cdot \\gamma + \\beta$$\n",
    "    - $\\mu_x$: mean of input over the dimension c\n",
    "    - $\\sigma_x$: standard deviation(squareroot of variance) of input over the dimension c\n",
    "    - $\\epsilon$: numerical stability helper\n",
    "    - $\\gamma$: learnable paramter to undo the normalization (=>$\\sigma_x$)\n",
    "    - $\\beta$: learnable paramter to undo the normalization (=>$\\mu_x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.9895, -0.2670,  0.2553, -0.8938,  1.2556, -0.5927,  0.1257,  0.4676,\n",
      "        -0.6929,  0.8454])\n",
      "tensor([ 1.9895, -0.2670,  0.2553, -0.8938,  1.2556, -0.5927,  0.1257,  0.4676,\n",
      "        -0.6929,  0.8454])\n"
     ]
    }
   ],
   "source": [
    "# elemetn-wise multiplication\n",
    "# Since the gamma and beta are having only num_features elements, \n",
    "# we need to add dimention to N and L\n",
    "num_features = 10\n",
    "dummy_gamma=torch.randn(num_features)\n",
    "print(dummy_gamma)\n",
    "augmented=dummy_gamma[None, :, None]\n",
    "print(augmented[0, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    \"\"\" \n",
    "    Only uses batch statistics (no running mean for evaluation).\n",
    "    Batch statistics are calculated for a single dimension (*input features)\n",
    "    Gamma is initialized as 1, beta as 0\n",
    "\n",
    "    Args: num_features: number of feature to calculate batch statistics for.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Model freely undo the normalization.\n",
    "        Complete-undo brings\n",
    "            Gamma -> standard deviation (=square root of the variance)\n",
    "            Beta -> mean of input\n",
    "        \"\"\"\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Batch normalization over the dimension C of (N, C, L)\n",
    "\n",
    "        Args:\n",
    "            input: Pytorch tensor, shape[N, C, L]\n",
    "            N: number of minibatch samples\n",
    "            C: number of channels (input's dimension)\n",
    "            L: shape of the one channel\n",
    "                -RGB image: L=(H, W)\n",
    "        Return:\n",
    "            Pytorch tensor, same shape as input\n",
    "        \"\"\"\n",
    "\n",
    "        eps = 1e-5 #helper for numerical stability\n",
    "\n",
    "        aggregate_dims = [0, 2]\n",
    "        mean = torch.mean(input, dim=aggregate_dims, keepdim=True)\n",
    "        std = torch.std(input, dim=aggregate_dims, keepdim=True)\n",
    "\n",
    "        input_normalized=(input-mean) / (std + eps)\n",
    "        return self.gamma[None, :, None] * input_normalized + self.beta[None, :, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batch normalization\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "test = torch.randn(8, 2, 4)\n",
    "\n",
    "b1 = BatchNorm(2)\n",
    "test_b1 = b1(test)\n",
    "\n",
    "# nn.BatchNorm1d\n",
    "    # affine\n",
    "    #   True >this module has leranable parameter\n",
    "    # track_running_stats\n",
    "    #   True >track running mean and standard deviation\n",
    "\n",
    "b2 = nn.BatchNorm1d(2, affine=False, track_running_stats=False)\n",
    "test_b2 = b2(test)\n",
    "\n",
    "# torch.allclose \n",
    "    # checks if all input and other satisfy the condition\n",
    "    # elementwise, for all elements of input and other\n",
    "    # rtol > relattive tolerance\n",
    "    # atol > absolute tolerance \n",
    "assert torch.allclose(test_b1, test_b2, rtol=2*1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet\n",
    "- Resnet is the first network which is introduced residual connections\n",
    "- Skip connection can solve problems such as vanishing and exploding gradients as the network gets deeper and deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func=func\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the stride is more than 1 or the in_channels is not equal to out_channels, we need to modify the shape of the input to add the second convolution output and before the final relu activation.\n",
    "\n",
    "In this case, we should care the stride size and the subtraction between out_channel and in_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n",
      "torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "stride = 2\n",
    "x = torch.randn(1, 1, 4, 4)\n",
    "print(x.shape)\n",
    "_x=x[:, :, ::stride, ::stride]\n",
    "print(_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtraction 16\n",
      "original channel 1\n",
      "torch.Size([1, 17, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "out_cha=32\n",
    "in_cha=16\n",
    "print(\"subtraction\", out_cha-in_cha)\n",
    "print(\"original channel\", x.shape[1])\n",
    "\n",
    "# F.pad configuration\n",
    "# padding setting = (padding_left, right, top, bottom, front, back)\n",
    "\n",
    "padded_inpput = F.pad(_x, (0, 0, 0, 0, 0, out_cha-in_cha), mode=\"constant\", value=0) \n",
    "print(padded_inpput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The residual block used by ResNet\n",
    "\n",
    "    Args:\n",
    "        in_channels: The number of channels (feature map) of the incoming embedding\n",
    "        out_channels: The number of channels after the first convolution\n",
    "        stride: Stride size of th first convolution, used for downsampling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        if stride > 1 or in_channels != out_channels:\n",
    "            #Add strides in the skip connection and zeros for the new channels\n",
    "            self.skip = Lambda(lambda x: F.pad(x[:, :, ::stride, ::stride],\n",
    "             (0,0,0,0,0,out_channels - in_channels), mode=\"constant\", value=0))\n",
    "        else:\n",
    "            self.skip = nn.Sequential()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # bias term in the convolution layer will be erased by the batch-normalization\n",
    "        self.conv2=nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x1 = F.relu(self.bn1(self.conv1(input)))\n",
    "        x2 = self.bn2(self.conv2(x1))\n",
    "        return F.relu(x2 + self.skip(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual blocks\n",
    "\n",
    "    Args:\n",
    "        in_channels: The number of channels of the incomming embedding\n",
    "        out_channels: The number of channels of the outgoing embedding\n",
    "        stride: Stride size a the first convolution later for the downsampling\n",
    "        num_blocks: Number of residual blocks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, num_blocks):\n",
    "        super().__init__()\n",
    "        blocks = [ResidualBlock(in_channels, out_channels, stride=stride)]\n",
    "        for _ in range(num_blocks-1):\n",
    "            blocks.append(ResidualBlock(out_channels, out_channels))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Structure\n",
    "- a covolition latyer is always containing\n",
    "    - Convolution layer\n",
    "    - Batch Normalization\n",
    "    - ReLU activation\n",
    "- The each residual block has two comvolution layer with 1 skip connection\n",
    "- We should __squeeze__ the final output to feed it to the fully-connected(dense) layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Architectures\n",
    "- Args:\n",
    "    - input: 32x32 images\n",
    "    - Per-pixel mean subtracted\n",
    "1. 3x3 Conv layer (input_cha=3, out cha=16)\n",
    "    - Batch\n",
    "    - ReLU activation\n",
    "2. __SAME__ Residual stack (num_blocks = n, input_cha=16, out_channel=16)\n",
    "    - Residual block (1st, stride=1)\n",
    "        - 3x3 Same Convolution (Spacial reduction: 32->32)\n",
    "        - Batch\n",
    "        - ReLU\n",
    "    - Residual block (2st-n)\n",
    "        - 3x3 Same Convolution\n",
    "        - Batch\n",
    "        - ReLU\n",
    "3. __VALID__ Residual stach (num_blocks=n, input_cha=16, out_cha=32)\n",
    "    - Residual block (1st, strider=2)\n",
    "        - 3x3 Valid Convolution (Spacial reduction: 32->16)\n",
    "        - Batch\n",
    "        - ReLU\n",
    "    - Residual block (2st-n)\n",
    "        - 3x3 Same Convolution\n",
    "        - Batch\n",
    "        - ReLU\n",
    "4. __VALID__ Residual stach (num_blocks=n, input_cha=32, out_cha=64)\n",
    "    - Residual block (1st, strider=2)\n",
    "        - 3x3 Valid Convolution (Spacial reduction: 16->8)\n",
    "        - Batch\n",
    "        - ReLU\n",
    "    - Residual block (2st-n)\n",
    "        - 3x3 Same Convolution\n",
    "        - Batch\n",
    "        - ReLU\n",
    "5. Average Pooling to reduce spacial dimension from 8x8 to 1x1 per channel\n",
    "6. Lambda squeeze the output and flatten\n",
    "7. Fully connected NN (input_dim=64, out_dim=num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "num_classes = 10\n",
    "\n",
    "resnet = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(num_features=16),\n",
    "    nn.ReLU(),\n",
    "    ResidualStack(in_channels=16, out_channels=16, stride=1, num_blocks=n),\n",
    "    ResidualStack(in_channels=16, out_channels=32, stride=2, num_blocks=n),\n",
    "    ResidualStack(in_channels=32, out_channels=64, stride=2, num_blocks=n),\n",
    "    nn.AdaptiveAvgPool2d(output_size=1),\n",
    "    Lambda(lambda x: x.squeeze()),\n",
    "    nn.Linear(in_features=64, out_features=num_classes)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the module(layer) weight and bias\n",
    "- We need to initialize the weights of our model appropriately\n",
    "- module.weight, module.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Lambda()\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (5): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Lambda()\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (6): AdaptiveAvgPool2d(output_size=1)\n",
       "  (7): Lambda()\n",
       "  (8): Linear(in_features=64, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize_weight_Resnet(module): ##intilalize each layer(module)\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        nn.init.constant_(module.weight, 1) #initialize gamma with 1\n",
    "        nn.init.constant_(module.bias, 0) #initialize beta with 0\n",
    "\n",
    "resnet.apply(initialize_weight_Resnet)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of resnet\n",
    "### Data loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define data download class from the dataset server\n",
    "- Define the transformation\n",
    "    - mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]\n",
    "    - Use precomputed mean and standard deviation of respective channel\n",
    "- Down load the data using the transmormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Subset(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"\n",
    "    Ger a subset of the CIFAR10 dataset provided thougth torch.vision\n",
    "    return the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, idx=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        if idx is None:\n",
    "            return\n",
    "        \n",
    "        self.data=self.data[idx]\n",
    "        target_np = np.array(self.targets)\n",
    "        self.targets=target_np[idx].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation in precomputing\n",
    "- We need to make the model be robust to wide variety of the transformation\n",
    "- Usually a real-world image does not contain the full-body image of object\n",
    "- Cropping is good data augmentation for the occulusion\n",
    "- Horizonfilp is good data augmentation foe the robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "ntrain = 45_000\n",
    "## creating dataset for dataloader\n",
    "train_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain), download=True, transform=transform_train)\n",
    "eval_set = CIFAR10Subset(root='./data', train=True, idx = range(ntrain, 50_000), download=True, transform=transform_eval)\n",
    "test_set = CIFAR10Subset(root='./data', train=False, download=True, transform=transform_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size torch.Size([3, 32, 32])\n",
      "target label 6\n"
     ]
    }
   ],
   "source": [
    "print(\"image size\", train_set[0][0].shape)\n",
    "print(\"target label\", train_set[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "def PredandImage_visuallization(pred):\n",
    "    num_images = 10\n",
    "    image = []\n",
    "    fig = plt.figure(figsize=(10., 10.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(2, 5), axes_pad=0.5)\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        pred1 = torch.argmax(pred[i], dim=-1).numpy()\n",
    "        gt = y_val[i]\n",
    "        image.append(x_val[i].reshape(28,28))\n",
    "        preds.append(pred1)\n",
    "\n",
    "    counter = 0\n",
    "    print(preds)\n",
    "\n",
    "    for ax, im in zip(grid, image):\n",
    "        ax.set_title(str(preds[counter]))\n",
    "        ax.imshow(im, cmap=\"gray\")\n",
    "        counter = counter + 1\n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def Image_Grid_visuallization(dataset):\n",
    "    transform_topilimage = transforms.ToPILImage()\n",
    "    num_images = 10\n",
    "    images = []\n",
    "    fig = plt.figure(figsize=(10., 10.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(2, 5), axes_pad=0.5)\n",
    "    targets = []\n",
    "\n",
    "    for i in range(num_images):\n",
    "        ima, target = transform_topilimage(dataset[i][0]), dataset[i][1]\n",
    "        images.append(ima)\n",
    "        targets.append(target)\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for ax, im in zip(grid, images):\n",
    "        ax.set_title(str(targets[counter]))\n",
    "        ax.imshow(im)\n",
    "        counter = counter + 1\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data loader\n",
    "# By enabling the pinned memory transfer, GPU pinns the certain memory partition and it can be accessed by the host device efficiently with low latency\n",
    "dataloaders = {}\n",
    "dataloaders['train']=DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "dataloaders['val']=DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "dataloaders['test']=DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we push the model to our GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Lambda()\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (5): ResidualStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (skip): Lambda()\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (skip): Sequential()\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (6): AdaptiveAvgPool2d(output_size=1)\n",
       "  (7): Lambda()\n",
       "  (8): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "resnet.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to define a helper that does one epoch of training or evaluation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, dataloaders, train):\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation\n",
    "\n",
    "    Args:\n",
    "        model:The model used for prediction\n",
    "        optimizer: Optimization algorithm for the model\n",
    "        dataloader: iterable data loader providing the data to run our model on \n",
    "        train: Flag if we run for training or evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "\n",
    "    epoch_loss=0.0\n",
    "    epoch_acc=0.0\n",
    "\n",
    "    # Iterate over data\n",
    "    for xb, yb in dataloaders:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(train):\n",
    "            pred = model(xb)\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "            top1 = torch.argmax(pred, dim=1)\n",
    "            ncrorrect = torch.sum(top1==yb)\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += ncrorrect.item()\n",
    "\n",
    "    epoch_loss /= len(dataloaders.dataset)\n",
    "    epoch_acc /= len(dataloaders.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement a method for fitting our model\n",
    "For many models early stopping can save a lot of training time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience):\n",
    "    \"\"\"\n",
    "    Fit the given model on the dataset\n",
    "\n",
    "    Args:\n",
    "        model: The model used for prediction\n",
    "        optimizer: OPtimization algorithm for the model\n",
    "        lr_schedular: Learning rate scheduler that improved training in late epochs with learning rate decay\n",
    "        dataloaders: Dataloaders for training and validation\n",
    "        max_epochs: Maximum Number of epochs for training\n",
    "        patience: Number of epochs to wait with early stopping the training if validation loss has decreased\n",
    "\n",
    "    Returns:\n",
    "        Loss and accuracy in this epoch\n",
    "    \"\"\"\n",
    "\n",
    "    best_acc = 0\n",
    "    curr_partience =0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss, train_acc = run_epoch(model, optimizer, dataloaders['train'], train=True)\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\")\n",
    "\n",
    "        val_loss, val_acc = run_epoch(model, None, dataloaders['val'], train=True)\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, val loss: {val_loss:.2e}, accuracy: {val_acc * 100:.2f}%\")\n",
    "\n",
    "        if val_acc >= best_acc:\n",
    "            best_epoch = epoch\n",
    "            best_acc = val_acc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if epoch-best_epoch >= patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb Cell 117\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lr_scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mMultiStepLR(optimizer, milestones\u001b[39m=\u001b[39m[\u001b[39m100\u001b[39m, \u001b[39m150\u001b[39m], gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train(resnet, optimizer, lr_scheduler, dataloaders, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "\u001b[1;32m/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb Cell 117\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m curr_partience \u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m run_epoch(model, optimizer, dataloaders[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m >3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mmax_epochs\u001b[39m}\u001b[39;00m\u001b[39m, train loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.2e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb Cell 117\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, optimizer, dataloaders, train)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(train):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(xb)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(pred, yb)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     top1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(pred, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb Cell 117\u001b[0m in \u001b[0;36mResidualStack.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb Cell 117\u001b[0m in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     x1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(\u001b[39minput\u001b[39;49m)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x1))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryotok/Documents/ML_DL/machine-learning-excercise/machine_learning_sum.ipynb#Y234sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mrelu(x2 \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip(\u001b[39minput\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dlml/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "\n",
    "# train the model\n",
    "train(resnet, optimizer, lr_scheduler, dataloaders, max_epochs=10, patience=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = run_epoch(resnet, None, dataloaders['test'], train=False)\n",
    "print(f\"Test loss: {test_loss:.1e}, accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary `torch`\n",
    "- `torch.tensor`: PyTorch tensors work like numpy arrays, but can remember gradients and be sent to GPUs\n",
    "- `torch.nn`\n",
    "    - `torch.nn.funciotnal` : Provides various useful functions (non stateful) for training neural networks, e.g. activation functions and loss functions\n",
    "    - `nn.Module` : Subclass from this to create a callable that acts like a function, but can remember stat. It knows what parameters and submodules it contains and privides various functionality based on that\n",
    "    - `nn.Parameters` : Wraps a tensor and tells the containing Module that it needs updating dusring backpropagation\n",
    "    - `torch.nn` : Many useful layers are already implemented in this library e.g. nn.Linear, nn.Conv2d\n",
    "    - `nn.Sequential` : Provides an easy way of defining purely stacked modules\n",
    "\n",
    "- `torch.optim` : Optimizers such as SGD or Adam, which let you easily update and train the paramters inside the passed model\n",
    "\n",
    "- `Dataset(Tensordataset)` : interface for data using only the __len__ and __getitem__ functions. Tensors can be converetd into a Dataset by using Tensor Dataset\n",
    "\n",
    "- `DataLoader` : Takes any Dataset and provides an iterator for returning mini-bathces with various advanced functionality\n",
    " \n",
    "- `GPU` : To use your GPU you need to move your model and each mini-batch to your GPU using __.to(devide)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6b7a25b8b2426eb550c8146811605d9bdb61f23d99ada7f1c3a4f71b608de8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
